<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">17118133</article-id><article-id pub-id-type="pmc">1683561</article-id><article-id pub-id-type="publisher-id">1471-2105-7-S2-S12</article-id><article-id pub-id-type="doi">10.1186/1471-2105-7-S2-S12</article-id><article-categories><subj-group subj-group-type="heading"><subject>Proceedings</subject></subj-group></article-categories><title-group><article-title>Improving the Performance of SVM-RFE to Select Genes in Microarray Data</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Ding</surname><given-names>Yuanyuan</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>yding@olemiss.edu</email></contrib><contrib id="A2" corresp="yes" contrib-type="author"><name><surname>Wilkins</surname><given-names>Dawn</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>dwilkins@cs.olemiss.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Computer &#x00026; Information Science Department, The University of Mississippi, University, MS, USA</aff><pub-date pub-type="collection"><year>2006</year></pub-date><pub-date pub-type="epub"><day>26</day><month>9</month><year>2006</year></pub-date><volume>7</volume><issue>Suppl 2</issue><supplement><named-content content-type="supplement-title">Third Annual MCBIOS Conference. Bioinformatics: A Calculated Discovery</named-content><named-content content-type="supplement-editor">Jonathan D Wren (Senior Editor), Stephen Winters-Hilt, Yuriy Gusev, Andrey Ptitsyn</named-content><ext-link ext-link-type="uri" xlink:href="http://www.mcbios.org">http://www.mcbios.org</ext-link></supplement><fpage>S12</fpage><lpage>S12</lpage><permissions><copyright-statement>Copyright &#x000a9; 2006 Ding &#x00026; Wilkins; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2006</copyright-year><copyright-holder>Ding &#x00026; Wilkins; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Ding
               Yuanyuan
               
               yding@olemiss.edu
            </dc:author><dc:title>
            Improving the Performance of SVM-RFE to Select Genes in Microarray Data
         </dc:title><dc:date>2006</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 7(Suppl 2): S12-. (2006)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2006)7:Suppl 2&#x0003c;S12&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Recursive Feature Elimination is a common and well-studied method for reducing the number of attributes used for further analysis or development of prediction models. The effectiveness of the RFE algorithm is generally considered excellent, but the primary obstacle in using it is the amount of computational power required.</p></sec><sec><title>Results</title><p>Here we introduce a variant of RFE which employs ideas from simulated annealing. The goal of the algorithm is to improve the computational performance of recursive feature elimination by eliminating chunks of features at a time with as little effect on the quality of the reduced feature set as possible. The algorithm has been tested on several large gene expression data sets. The RFE algorithm is implemented using a Support Vector Machine to assist in identifying the least useful gene(s) to eliminate.</p></sec><sec><title>Conclusion</title><p>The algorithm is simple and efficient and generates a set of attributes that is very similar to the set produced by RFE.</p></sec></abstract><conference><conf-date>2&#x02013;4 March 2006</conf-date><conf-name>Third Annual MidSouth Computational Biology and Bioinformatics Society (MCBIOS) Conference. Bioinformatics: A Calculated Discovery</conf-name><conf-loc>Baton Rouge, LA, USA</conf-loc></conference></article-meta></front><body><sec><title>Background</title><p>In many machine learning applications, a prediction is to be made from a data set of historical information. Gene expression data sets have been constructed with the goal of predicting whether or not disease is present (e.g. colon cancer), or which type of disease exists in the patient. One of the primary difficulties in working with gene expression data sets is the large number of attributes (genes). A major focus of gene expression analysis is in the area of feature selection or dimension reduction. Most of the algorithms for elucidating models for prediction are less effective when the number of genes is too large. There are many approaches for reducing the size of the feature set, and among them is recursive feature elimination (RFE). The idea of RFE is to start with all features, select the "least useful" feature (using some metric or heuristic), remove that feature, and repeat until some stopping condition is met. There are many variations of RFE based on how the feature to be removed is selected, and when to stop.</p><p>RFE is well-studied for use in gene expression studies [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. Finding an optimal subset of features is combinatorially prohibitive, so RFE reduces the complexity of feature selection by being "greedy". That is, once a feature is selected for removal, it is never reintroduced. Most studies have found RFE to select very good gene sets, but with 12000 or more genes to select from, when the number of samples (patients) is large, RFE takes considerable computation time. Recursive feature elimination is extremely computationally expensive when only <italic>one </italic>least useful feature is removed during each iteration. A modified version of RFE, RFE-Annealing, is proposed here, aimed at greatly reducing the computational time required to perform the RFE ranking process while maintaining comparable performance with respect to prediction accuracy. Instead of removing only one feature at a time, RFE-Annealing removes a set of features each time, with the number of features removed decreasing in each iteration. As its name implies, the process is similar to the well-known method of simulated annealing [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B6">6</xref>].</p><p>Simulated annealing has its roots in metallurgy and thermodynamics. Annealing is used in metallurgy to create materials with fewer defects. In thermodynamics, annealing is used to find an optimal state which has minimum energy. The basic process has two steps: heat to a high temperature and then cool very slowly. The annealing schedule, which is the heart of the process, defines how to reduce the temperature during the cooling phase. Simulated annealing is a metaheuristic (not problem specific) that is used in many combinatorial optimization problems. Combinatorial search techniques have difficulty distinguishing between <italic>local </italic>minima (or maxima) and <italic>global </italic>minima (or maxima). Simulated annealing is used in a search by selecting a random state to start and using the annealing schedule to guide the search. Early in the search there is a higher probability of making a move in the search space to a solution that is worse than the one before. This is appropriate since the initial solution was random and the optimal solution may be far off. As the search proceeds, the probability of making a move to a worse solution is decreased slowly. The temperature decrease corresponds to the decreasing probability of moving to a worse solution. RFE-Annealing uses the annealing schedule idea to remove a large number of genes in the initial iterations (when it is easy to identify unimportant genes). In later iterations, the number of genes removed is reduced so that important genes are not removed. The simple schedule of removing <inline-graphic xlink:href="1471-2105-7-S2-S12-i1.gif"/> of the remaining genes during iteration <italic>i </italic>is used. That is, half are removed in the first iteration, one-third in the second, one-fourth in the third, and so on. Details of the algorithm can be found in the Methods section.</p><p>Vladimir Vapnik invented Support Vector Machines (SVMs) in 1979 [<xref ref-type="bibr" rid="B7">7</xref>]. SVMs often achieve superior classification performance compared to other learning algorithms across most domains and tasks. They are efficient enough to handle very large-scale classification in both number of samples and number of variables [<xref ref-type="bibr" rid="B8">8</xref>]. SVMs are generated in two steps. First, the data vectors are mapped to a high-dimensional space. Second, the SVM tries to find a hyperplane in this new space with maximum margin separating the classes of data. Sometimes it is not possible to find a separating hyperplane even in a very high-dimensional space. In this case, a trade-off is introduced between the size of the separating margin and penalties for every vector that is within the margin [<xref ref-type="bibr" rid="B9">9</xref>]. The margin denotes the distance from the boundary to the closest data point in the feature space.</p><p>In its simplest, linear form, a SVM is a hyperplane that separates two classes of examples (postive and negative) with maximum margin (see Figure <xref ref-type="fig" rid="F1">1</xref>). The SVM creates and outputs a weight vector, where each dimension (feature) is assigned a weight. The weight vector is used to determine the least important feature, which is defined to be the one with the <italic>smallest </italic>weight in the weight vector. The least important feature is selected for removal in each iteration of the recursive feature elimination procedure.</p></sec><sec><title>Results and Discussion</title><sec><title>Results</title><sec><title>Data Sets</title><p>We analyzed three well known data sets: 1) the data of Bhattacharjee <italic>et al</italic>. [<xref ref-type="bibr" rid="B10">10</xref>], which is a set of 12,600 gene expression measurements (Affymetrix oligonucleotide arrays) per patient from 203 patients with normal subjects and four subtypes of lung carcinomas; 2) a colon cancer data set [<xref ref-type="bibr" rid="B11">11</xref>], consisting of expression levels of 2000 genes describing 62 samples (40 tumor and 22 normal colon tissues, Affymetrix oligonucleotide array); 3) a pediatric Acute Lymphoblastic Leukemia (ALL) data set from St. Jude Children's Research Hospital (SJCRH) [<xref ref-type="bibr" rid="B12">12</xref>], which includes 12,625 gene expression measurements (Affymetrix arrays) per patient from 246 patients with six different subtypes of ALL. In the research of SJCRH, 246 cases of pediatric ALL were analyzed on the U133 A and B chips. There are six primary subtypes of ALL involved in the study: BCR-ABL, E2A-PBX1, Hyperdiploid&#x0003e;50, MLL, T-ALL and TEL.</p></sec><sec><title>Model</title><p>The three data sets were used to compare the performance of the RFE and RFE-Annealing algorithms. Each data set was randomly divided into stratified training and hidden test sets. A different number of genes was selected by each of the algorithms. The SVM was trained on the training data that was trimmed to the selected genes from each algorithm respectively. The SVM model produced was evaluated by its performance on the hidden test data to predict the class labels (since cross validation results on the training data tend to be optimistic). More details of the model design are given in the Methods section. Comparisons of the two algorithms in terms of prediction rate and time required are made. A comparison between RFE-Annealing and SQRT-RFE [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>] is also performed. Like RFE-Annealing, the goal of SQRT-RFE is to improve the performance of RFE by removing a set of features during each iteration. In that algorithm, <inline-graphic xlink:href="1471-2105-7-S2-S12-i2.gif"/> features are removed at each step.</p></sec><sec><title>Experimental results on the SJCRH data</title><p>The performance of RFE-Annealing was comparable to both the RFE and SQRT-RFE algorithms in terms of prediction accuracy rate (each achieving around 98%-100% accuracy on the test data), but RFE-Annealing is much less computationally intensive than the RFE algorithm. RFE-Annealing took approximately 26 minutes for gene selection, while the RFE algorithm spent around 58 hours, and SQRT-RFE required 1 hour to complete. All the experiments were run separately on an unloaded machine (with 2.6GHz dual processors and 2GB memory).</p><p>Figure <xref ref-type="fig" rid="F2">2</xref> shows a comparison of RFE and RFE-Annealing. We can see that in most of the cases when the number of genes ranges from one to 200, RFE-Annealing performs at least as well as the original RFE. Also it is more stable than the original RFE. Only in a few points (e.g. when the number of genes selected is around 10), does RFE give a slightly higher prediction rate than RFE-Annealing.</p><p>When compared to SQRT-RFE, RFE-Annealing has comparable performance in terms of prediction rate. See Figure <xref ref-type="fig" rid="F3">3</xref>.</p></sec><sec><title>Experimental results on Bhattacharjee and Alon data</title><p>Experiments on the Bhattacharjee and Alon colon cancer data sets also show that RFE-Annealing has similar performance when compared with both RFE and SQRT-RFE with respect to accuracy. The computational requirement of RFE-Annealing is slightly less than SQRT-RFE, and significantly less than the original RFE. Figures <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F5">5</xref> show that when tested on the Bhattacharjee data, in many cases RFE-Annealing outperforms the other two, but in some cases, it performs slightly worse. RFE-Annealing seems to perform slightly better when fewer than 100 genes are selected and RFE performs slightly better when 100 or more genes are selected.</p><p>When tested on Alon's colon cancer data, Figure <xref ref-type="fig" rid="F6">6</xref> shows little difference between RFE-Annealing and the other two algorithms in terms of prediction rate performance. But when comparing computational time, RFE-Annealing is much faster than the other two. Results are shown in Table <xref ref-type="table" rid="T1">1</xref>. Even with a small data set like Alon, with 62 samples of around 2000 genes, there is a significant difference between the time required by RFE-Annealing and RFE, at 30 seconds and 6 minutes 18 seconds, respectively. SQRT-RFE required one minute on the same data set. For the larger data sets the difference is even more dramatic. In general, the three algorithms selected very similar "best" gene subsets, which also shows that these three algorithms are comparable. In the SJCRH data, 187 out of 200 genes are in common between RFE and RFE-Annealing algorithms, while 189 between RFE-Annealing and SQRT-RFE.</p><p>In the Bhattacharjee data, 182 out of 200 genes are the same from the RFE and RFE-Annealing algorithms. Similarly, 185 are the same for RFE-Annealing and SQRT-RFE.</p><p>In the Alon data, there are 47 out of 50 genes in common between RFE and RFE-Annealing and 46 out of 50 between RFE-Annealing and SQRT-RFE.</p></sec></sec><sec><title>Biological analysis</title><p>In order to investigate the biological meaning of the selected important genes in the SJCRH data, table <xref ref-type="table" rid="T2">2</xref> shows the pathways derived from NetAffx [<xref ref-type="bibr" rid="B13">13</xref>] to which the 200 genes selected by the algorithms belong. RFE and RFE-Annealing are very similar in the pathways represented by the genes selected. The primary differences are between SQRT-RFE and the other two algorithms. The pathways "Calcium regulation in cardiac cells", "G Protein Signaling", "Inflammatory Response Pathway", "Inositol Phosphate metabolism", and "Smooth muscle contraction" have the least consistency.</p></sec></sec><sec><title>Conclusion</title><p>In general, RFE-Annealing allows an enormous increase in the efficiency of the algorithm without a decrease of classification accuracy. Thus the gene selection process is made much more practical in domains with a large number of features, such as gene expression data. This improvement is especially important as the number of samples available increases.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>SVM-RFE-Annealing</title><p>Guyon introduced Recursive Feature Elimination (RFE) for Support vector machines (SVM) in [<xref ref-type="bibr" rid="B1">1</xref>]. SVM-RFE performs feature selection by iteratively training a SVM classifier with the current set of features and removing the least important feature indicated by the SVM [<xref ref-type="bibr" rid="B14">14</xref>]. In the linear case, the separating hyperplane (decision function) is <italic>D </italic>(<inline-graphic xlink:href="1471-2105-7-S2-S12-i3.gif"/>) = (<inline-graphic xlink:href="1471-2105-7-S2-S12-i4.gif"/>) + <italic>b</italic>. The feature with the smallest weight <italic>w</italic><sup>2 </sup>contributes the least to the resulting hyperplane and can be discarded. Due to the heavy computational cost of RFE, several variants have been introduced to speed up the algorithm. Instead of removing only one least important feature at every iteration, removing a big chunk of features in each iteration will speed up the process. The goal is to remove more features during each iteration, but not to eliminate the important features. SQRT-RFE removes <inline-graphic xlink:href="1471-2105-7-S2-S12-i5.gif"/> features at each step, where <italic>S </italic>is the number of remaining features in each iteration. Another variant is Entropy-based RFE (E-RFE) which eliminates features based on the structure of the weight distribution [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>]. An entropy function <italic>H </italic>is introduced as a measure of the weight distribution.</p><p>We introduced RFE-Annealing which eliminates larger sets of features at the beginning steps, but as the algorithm proceeds, the number of features removed is reduced in each step. The basic idea is: in the <italic>n</italic>th iteration, train a SVM on the active features and remove l/(<italic>n </italic>+ 1) of least important features. This process continues until only <italic>m </italic>features are left, where <italic>m </italic>is a user defined variable. This method is simple and easy to implement.</p><p><bold>Algorithm SVM-RFE-Annealing: </bold>(Variation of SVM-RFE [<xref ref-type="bibr" rid="B1">1</xref>])</p><p>Inputs:</p><p>Training examples</p><p><italic>X</italic><sub>0 </sub>= [x<sub>1</sub>, x<sub>2</sub>, ...x<sub><italic>k</italic></sub>, ...x<sub><italic>l</italic></sub>]<italic>T</italic></p><p>Class labels</p><p><bold>y </bold>= [<italic>y</italic><sub>1</sub>, <italic>y</italic><sub>2</sub>, ....<italic>y</italic><sub><italic>k</italic></sub>, ....<italic>y</italic><sub><italic>l</italic></sub>]<italic>T</italic></p><p>Initialize:</p><p>Subset of surviving features</p><p><bold>s </bold>= [l, 2, ...n]</p><p>Feature ranked list</p><p><bold>r </bold>= []</p><p>Repeat until <bold>s </bold>= []</p><p>Restrict training examples to good feature indices</p><p><italic>X </italic>= <italic>X</italic><sub>0</sub>(:, <bold>s</bold>)</p><p>Train the classifier</p><p>&#x003b1; = <italic>SV M </italic>- <italic>train</italic>(<italic>X</italic>, <bold>y</bold>)</p><p>Compute the weight vector of dimension length(s)</p><p><bold>w </bold>= &#x003a3;<sub><italic>k</italic></sub>&#x003b1;<sub><italic>k</italic></sub><italic>y</italic><sub><italic>k</italic></sub><bold>X</bold><sub><italic>k</italic></sub></p><p>Compute the ranking criteria</p><p><italic>c</italic><sub><italic>i </italic></sub>= (<italic>w</italic><sub><italic>i</italic></sub>)<sup>2</sup>, for all i</p><p>Repeat <italic>length</italic>(<italic>s</italic>) &#x000d7; <inline-graphic xlink:href="1471-2105-7-S2-S12-i6.gif"/> times</p><p>Find the feature with smallest ranking criterion</p><p><italic>f </italic>= <italic>argmin</italic>(<bold>c</bold>)</p><p>Update feature ranked list</p><p><bold>r </bold>= [<bold>S</bold>(<italic>f</italic>), <bold>r</bold>]</p><p>Eliminate the feature with smallest ranking criterion</p><p><bold>s </bold>= <bold>s</bold>(l : <italic>f </italic>- 1, <italic>f </italic>+ 1 : <italic>length</italic>(<bold>s</bold>))</p><p>end</p><p>end</p><p>Output:</p><p>Feature ranked list <bold>r</bold>.</p></sec><sec><title>Models</title><p>Data is split into training data and test data with proportion 80% training and 20% test (except the smaller Alon data, which is split with 75%/25%). As shown in Figure <xref ref-type="fig" rid="F7">7</xref>, based on training data, a linear SVM is built. The implementation of SVM in Weka [<xref ref-type="bibr" rid="B15">15</xref>] was used for all testing. Genes with the smallest weight are removed iteratively, according to the gene selection algorithms (RFE, RFE-Annealing and SQRT-RFE). We will have <italic>m </italic>genes left, where <italic>m </italic>is a user defined parameter, and the ranking of genes is based on the order they are removed. If <italic>m </italic>= 0, we will get the ranking of all of the genes. The user can select the <italic>m </italic>most important genes and build a SVM classifier based on the training data. The SVM is then run on the test data and an estimated prediction accuracy rate is obtained. Since running the RFE algorithm on large data sets like the SJCRH data takes more than 50 hours, this train/test process was performed only once on each data set. The algorithms are deterministic, so the accuracy is fixed for the particular train/test split used. The machine was unloaded except for the particular program being tested, so the time estimates are relatively stable.</p><p>In our experiments, we let <italic>m </italic>= 1, 2, ...200 for the SJCRH and Bhattacharjee data sets and <italic>m </italic>= 1, 2, ....50 for the Alon data, since the Alon data only had 2000 genes initially and the other two had more than 12,000 genes.</p><p>In order to test the performance of the RFE-Annealing algorithm, for each <italic>m, m </italic>genes were selected by RFE, SQRT-RFE and RFE-Annealing respectively, and the SVM models were built and the prediction rates on the test data were compared. The overall process is explained in the diagram in Figure <xref ref-type="fig" rid="F7">7</xref>.</p></sec></sec><sec><title>Authors' contributions</title><p>YD contributed to writing the computer code. DW guided the analysis. Both authors contributed to the development of methodology and writing the manuscript. Both authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>This project was funded in part by NIH grant R01-AI049770-03. We thank the participants of MCBIOS 2006, and the anonymous referees for their suggestions.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>Barnhill</surname><given-names>SMD</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Gene Selection for Cancer Classification using Support Vector Machines</article-title><source>Machine Learning</source><year>2002</year><volume>46</volume><fpage>389</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Furlanello</surname><given-names>C</given-names></name><name><surname>Maria</surname><given-names>S</given-names></name><name><surname>Serler</surname><given-names>M</given-names></name><name><surname>Giuseppe</surname><given-names>J</given-names></name></person-group><article-title>An Accelerated Procedure for Recursive Feature Ranking on Microarray Data</article-title><source>Neural Networks</source><year>2003</year><volume>16</volume><fpage>641</fpage><lpage>648</lpage><pub-id pub-id-type="pmid">12850018</pub-id><pub-id pub-id-type="doi">10.1016/S0893-6080(03)00103-5</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Furlanello</surname><given-names>C</given-names></name><name><surname>Serafini</surname><given-names>M</given-names></name><name><surname>Merler</surname><given-names>S</given-names></name><name><surname>Jurman</surname><given-names>G</given-names></name></person-group><article-title>Entropy-based gene ranking without selection bias for the predictive classification of microarray data</article-title><source>BMC Bioinformatics</source><year>2003</year><pub-id pub-id-type="pmid">14604446</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Metropolis</surname><given-names>N</given-names></name><name><surname>Rosenbluth</surname><given-names>A</given-names></name><name><surname>Rosenbluth</surname><given-names>MN</given-names></name><name><surname>Teller</surname><given-names>A</given-names></name><name><surname>Teller</surname><given-names>E</given-names></name></person-group><article-title>Equations of State Calculations by Fast Computing Machines</article-title><source>J Chem Phys</source><year>1958</year><volume>21</volume><fpage>1087</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.1063/1.1699114</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pincus</surname><given-names>M</given-names></name></person-group><article-title>A Monte Carlo Method for the Approximate Solution of Certain Types of Constrained Optimization Probl</article-title><source>Oper Res</source><year>1970</year><volume>18</volume><fpage>1225</fpage><lpage>1228</lpage></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>S</given-names><suffix>Jr</suffix></name><collab>CDG</collab><name><surname>Vecchi</surname><given-names>M</given-names></name></person-group><article-title>Optimization by Simulated Annealing</article-title><source>Science</source><year>1983</year><volume>220</volume><fpage>671</fpage><lpage>680</lpage></citation></ref><ref id="B7"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><source>The Nature of Statistical Learning Theory</source><year>1995</year><publisher-name>Springer-Verlag</publisher-name></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Statnikov</surname><given-names>A</given-names></name><name><surname>Aliferis</surname><given-names>CF</given-names></name><name><surname>Tsamardinos</surname><given-names>I</given-names></name><name><surname>Hardin</surname><given-names>D</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name></person-group><article-title>A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><fpage>631</fpage><lpage>643</lpage><pub-id pub-id-type="pmid">15374862</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bti033</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Byvatov</surname><given-names>E</given-names></name><name><surname>Fechner</surname><given-names>U</given-names></name><name><surname>Sadowski</surname><given-names>J</given-names></name><name><surname>Schneider</surname><given-names>G</given-names></name></person-group><article-title>Comparison of Support Vector Machine and Artificial Neural Network Systems for Drug/Nondrug Classification</article-title><source>J Chem Inf Comput Sci</source><year>2003</year><volume>43</volume><fpage>1882</fpage><lpage>1889</lpage><pub-id pub-id-type="pmid">14632437</pub-id><pub-id pub-id-type="doi">10.1021/ci0341161</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharjee</surname><given-names>A</given-names></name><name><surname>Richards</surname><given-names>WG</given-names></name><name><surname>Staunton</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Monti</surname><given-names>S</given-names></name><name><surname>Vasa</surname><given-names>P</given-names></name><name><surname>Ladd</surname><given-names>C</given-names></name><name><surname>Beheshti</surname><given-names>J</given-names></name><name><surname>Bueno</surname><given-names>R</given-names></name><name><surname>Gillette</surname><given-names>M</given-names></name><name><surname>Loda</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>G</given-names></name><name><surname>Mark</surname><given-names>EJ</given-names></name><name><surname>Lander</surname><given-names>ES</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Johnson</surname><given-names>BE</given-names></name><name><surname>Golub</surname><given-names>TR</given-names></name><name><surname>Sugarbaker</surname><given-names>DJ</given-names></name><name><surname>Meyerson</surname><given-names>M</given-names></name></person-group><article-title>Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses</article-title><source>Proc Natl Acad Sci U S A</source><year>2001</year><volume>98</volume><fpage>13790</fpage><lpage>13795</lpage><pub-id pub-id-type="pmid">11707567</pub-id><pub-id pub-id-type="doi">10.1073/pnas.191502998</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alon</surname><given-names>U</given-names></name><name><surname>Barkai</surname><given-names>N</given-names></name><name><surname>Notterman</surname><given-names>DA</given-names></name><name><surname>Gish</surname><given-names>K</given-names></name><name><surname>Ybarra</surname><given-names>S</given-names></name><name><surname>Mack</surname><given-names>D</given-names></name><name><surname>Levine</surname><given-names>AJ</given-names></name></person-group><article-title>Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</article-title><source>Proc Natl Acad Sci USA</source><year>1999</year><volume>96</volume><fpage>6745</fpage><lpage>6750</lpage><pub-id pub-id-type="pmid">10359783</pub-id><pub-id pub-id-type="doi">10.1073/pnas.96.12.6745</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yeoh</surname><given-names>EJ</given-names></name><name><surname>Ross</surname><given-names>ME</given-names></name><name><surname>Shurtleff</surname><given-names>SA</given-names></name><name><surname>Williams</surname><given-names>WK</given-names></name><name><surname>Patel</surname><given-names>D</given-names></name><name><surname>Mahfouz</surname><given-names>R</given-names></name><name><surname>Behm</surname><given-names>EG</given-names></name><name><surname>Raimondi</surname><given-names>SC</given-names></name><name><surname>Relling</surname><given-names>MV</given-names></name><name><surname>Patel</surname><given-names>A</given-names></name><name><surname>Cheng</surname><given-names>C</given-names></name><name><surname>Campana</surname><given-names>D</given-names></name><name><surname>Wilkins</surname><given-names>D</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Pui</surname><given-names>CH</given-names></name><name><surname>Evans</surname><given-names>WE</given-names></name><name><surname>Naeve</surname><given-names>C</given-names></name><name><surname>Wong</surname><given-names>L</given-names></name><name><surname>Downing</surname><given-names>JR</given-names></name></person-group><article-title>Pediatric Lymphoblastic Leukemia by Gene Expression Profiling</article-title><source>Cancer Cell</source><year>2002</year><volume>1</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">12086872</pub-id><pub-id pub-id-type="doi">10.1016/S1535-6108(02)00032-6</pub-id></citation></ref><ref id="B13"><citation citation-type="other"><article-title>NETAFFX analysis center</article-title><ext-link ext-link-type="uri" xlink:href="http://www.affymetrix.com/analysis/index.affx"/></citation></ref><ref id="B14"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Scholkopf</surname><given-names>B</given-names></name><name><surname>Tsuda</surname><given-names>K</given-names></name><name><surname>Vert</surname><given-names>JP</given-names></name><collab>Eds</collab></person-group><source>Kernel Methods in Computational Biology</source><year>2004</year><publisher-name>MIT Press</publisher-name></citation></ref><ref id="B15"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Witten</surname><given-names>IH</given-names></name><name><surname>Frank</surname><given-names>E</given-names></name></person-group><source>Data Mining: Practical machine learning tools and techniques</source><year>2005</year><publisher-name>San Francisco: Morgan Kaufmann</publisher-name></citation></ref><ref id="B16"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Platt</surname><given-names>JC</given-names></name></person-group><article-title>Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines</article-title><source>Tech rep</source><year>1998</year><publisher-name>Microsoft Research</publisher-name></citation></ref><ref id="B17"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Paul</surname><given-names>TK</given-names></name></person-group><article-title>Gene Expression Based Cancer Classification Using Evolutionary and Non-evolutionary Methods</article-title><source>Tech rep</source><year>2004</year><publisher-name>Department of Frontier Informatics The University of Tokyo</publisher-name></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>A Linear Support Vector Machine [16]</bold>. In its simplest, linear form, a SVM is a hyperplane that separates two classes of examples (postive and negative) with maximum margin. The margin is defined by the distance from the hyperplane to the nearest of the data points.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Comparison of RFE and RFE-Annealing in terms of Prediction Rate on SJCRH data</bold>. This figure shows the results of comparing RFE and RFE-Annealing using the St. Jude Children's Research Hospital ALL study. Accuracy rates on the hidden test set are very high for both gene selection algorithms across all gene set sizes up to 200.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Comparison of SQRT-RFE and RFE-Annealing in terms of Prediction Rate on SJCRH data</bold>. This figure shows the results of comparing SQRT-RFE and RFE-Annealing. They are very comparable with respect to accuracy on the hidden test data.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Comparison of RFE and RFE-Annealing in terms of Prediction Rate based on Bhattacharjee Data</bold>. When comparing RFE and RFE-Annealing on the hidden test data from the Bhattacharjee set, the algorithms both do well. RFE does slightly better when more than 100 genes are selected and RFE-Annealing does slightly better when less than 100 genes are used.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Comparison of SQRT-RFE and RFE-Annealing in terms of Prediction Rate based on Bhattacharjee Data</bold>. The accuracy rates are very similar when comparing SQRT-RFE and RFE-Annealing on the hidden test data from the Bhattacharjee set.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>Comparison of RFE, RFE-Annealing and SQRT-RFE in terms of Prediction Rate based on Alon's Data</bold>. On the smaller Alon data set, all three gene selection methods yielded the same accuracy on the hidden test data when 10 or more genes were selected. The largest gene set selected was 50 due to the fewer number of genes and samples.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-6"/></fig><fig position="float" id="F7"><label>Figure 7</label><caption><p><bold>Model Design</bold>. This figure describes the methodology used in the testing. The methodology employs a similar wrapper technique as described in [17]. The data sets were first divided, in a stratified way, into training and hidden test data (80% training and 20% test, except for Alon which was 75%/25% since it was smaller). The weight vector from the SVM with a linear kernel was used to identify the gene(s) to remove. The training data was projected down to just the specified subset and a classifier was constructed using a linear SVM. Then the test data was projected to the same feature set and tested using the classifier built from the training data. This was repeated for each data set, for each algorithm and for each size feature set from 1 to the maximum features selected.</p></caption><graphic xlink:href="1471-2105-7-S2-S12-7"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Comparison of RFE, RFE-Annealing and SQRT-RFE algorithms in terms of time</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Data</td><td align="center">Number of samples</td><td align="center">RFE</td><td align="center">RFE-Annealing</td><td align="right">SQRT-RFE</td></tr></thead><tbody><tr><td align="left">St.Jude</td><td align="center">246</td><td align="center">58 hours</td><td align="center">26 minutes</td><td align="right">60 minutes</td></tr><tr><td align="left">Bhattacharjee</td><td align="center">203</td><td align="center">27 hours</td><td align="center">20 minutes</td><td align="right">38 minutes</td></tr><tr><td align="left">Alon</td><td align="center">62</td><td align="center">6.3 minutes</td><td align="center">0.5 minute</td><td align="right">1 minute</td></tr></tbody></table><table-wrap-foot><p>This table clearly demonstrates the computational efficiency of RFE-Annealing over SQRT-RFE and especially RFE. When data sets with a large number of samples and a large number of genes (e.g. SJCRH and Bhattacharjee) are used the difference is substantial.</p></table-wrap-foot></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Pathways associated with the selected genes in SJCRH data</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Pathway</td><td align="center">RFE</td><td align="center">RFE- Annealing</td><td align="right">SQRT-RFE</td></tr></thead><tbody><tr><td align="left">Apoptosis</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Apoptosis_GenMAPP</td><td align="center">3</td><td align="center">3</td><td align="right">3</td></tr><tr><td align="left">Apoptosis_KEGG</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Arginine and proline metabolism</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Biosynthesis of steroids</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Calcium signaling pathway</td><td align="center">19</td><td align="center">18</td><td align="right">17</td></tr><tr><td align="left"><bold>Calcium_regulation_in_cardiac_cells</bold></td><td align="center">6</td><td align="center">4</td><td align="right">4</td></tr><tr><td align="left">Cell_cycle_KEGG</td><td align="center">8</td><td align="center">8</td><td align="right">9</td></tr><tr><td align="left">Cholesterol_Biosynthesis</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Circadian_Exercise</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">DNA_replication_Reactome</td><td align="center">0</td><td align="center">0</td><td align="right">1</td></tr><tr><td align="left">Electron_Transport_Chain</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Fructose and mannose metabolism</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Gl_to_S_cell_cycle_Reactome</td><td align="center">6</td><td align="center">6</td><td align="right">7</td></tr><tr><td align="left"><bold>G_Protein_Signaling</bold></td><td align="center">6</td><td align="center">4</td><td align="right">4</td></tr><tr><td align="left">Galactose metabolism</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Glutathione metabolism</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Glycerolipid metabolism</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Glycerophospholipid metabolism</td><td align="center">3</td><td align="center">3</td><td align="right">3</td></tr><tr><td align="left">Glycine, serine and threonine metabolism</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Glycolysis/Gluconeogenesis</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Glycolysis_and_Gluconeogenesis</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">GPCRDB_Class_A_Rhodopsin-like</td><td align="center">0</td><td align="center">1</td><td align="right">0</td></tr><tr><td align="left">Hypertrophy _model</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left"><bold>Inflammatory_Response_Pathway</bold></td><td align="center">1</td><td align="center">1</td><td align="right">4</td></tr><tr><td align="left"><bold>Inositol phosphate metabolism</bold></td><td align="center">4</td><td align="center">4</td><td align="right">0</td></tr><tr><td align="left">Integrin-mediated_cell_adhesion_KEGG</td><td align="center">4</td><td align="center">4</td><td align="right">4</td></tr><tr><td align="left">MAPK_Cascade</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">mRNA_processing_Reactome</td><td align="center">2</td><td align="center">2</td><td align="right">3</td></tr><tr><td align="left">Nicotinate and nicotinamide metabolism</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Ovarian_Infertility_Genes</td><td align="center">3</td><td align="center">3</td><td align="right">3</td></tr><tr><td align="left">Oxidative phosphorylation</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Pentose phosphate pathway</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Phosphatidylinositol signaling system</td><td align="center">4</td><td align="center">4</td><td align="right">3</td></tr><tr><td align="left">Prostaglandin_synthesis_regulation</td><td align="center">1</td><td align="center">0</td><td align="right">1</td></tr><tr><td align="left">Proteasome_Degradation</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Purine metabolism</td><td align="center">3</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Pyrimidine metabolism</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left"><bold>Smooth_muscle_contraction</bold></td><td align="center">12</td><td align="center">10</td><td align="right">10</td></tr><tr><td align="left">Statin_Pathway_PharmGKB</td><td align="center">0</td><td align="center">0</td><td align="right">1</td></tr><tr><td align="left">TGF_Beta_Signaling_Pathway</td><td align="center">4</td><td align="center">4</td><td align="right">4</td></tr><tr><td align="left">Terpenoid biosynthesis</td><td align="center">1</td><td align="center">1</td><td align="right">1</td></tr><tr><td align="left">Type I diabetes mellitus</td><td align="center">4</td><td align="center">5</td><td align="right">5</td></tr><tr><td align="left">Ubiquitin mediated proteolysis</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Urea cycle and metabolism of amino groups</td><td align="center">2</td><td align="center">2</td><td align="right">2</td></tr><tr><td align="left">Wnt_signaling</td><td align="center">3</td><td align="center">3</td><td align="right">2</td></tr></tbody></table><table-wrap-foot><p>This table lists the various known pathway associations with the size 200 gene sets selected by the algorithms using the SJCRH data. The values in the table represent the number of genes associated with that pathway in each of the sets. In general there was considerable overlap in the genes selected (over 90%). The pathways with the least consistency are shown in boldface. Pathway data was derived using NetAffx [13].</p></table-wrap-foot></table-wrap></sec></back></article>


