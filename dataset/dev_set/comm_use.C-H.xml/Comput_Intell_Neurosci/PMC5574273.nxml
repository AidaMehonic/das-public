<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Intell Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Comput Intell Neurosci</journal-id><journal-id journal-id-type="publisher-id">CIN</journal-id><journal-title-group><journal-title>Computational Intelligence and Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1687-5265</issn><issn pub-type="epub">1687-5273</issn><publisher><publisher-name>Hindawi</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28894463</article-id><article-id pub-id-type="pmc">5574273</article-id><article-id pub-id-type="doi">10.1155/2017/7643065</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Joint Extraction of Entities and Relations Using Reinforcement Learning and Deep Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-1882-1005</contrib-id><name><surname>Feng</surname><given-names>Yuntian</given-names></name><xref ref-type="aff" rid="I1"/><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Hongjun</given-names></name><xref ref-type="aff" rid="I1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0880-1227</contrib-id><name><surname>Hao</surname><given-names>Wenning</given-names></name><xref ref-type="aff" rid="I1"/></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Gang</given-names></name><xref ref-type="aff" rid="I1"/></contrib></contrib-group><aff id="I1">Institute of Command Information System, PLA University of Science and Technology, Nanjing, Jiangsu 210007, China</aff><author-notes><corresp id="cor1">*Yuntian Feng: <email>fengyuntian2009@live.cn</email></corresp><fn fn-type="other"><p>Academic Editor: Athanasios Voulodimos</p></fn></author-notes><pub-date pub-type="ppub"><year>2017</year></pub-date><pub-date pub-type="epub"><day>14</day><month>8</month><year>2017</year></pub-date><volume>2017</volume><elocation-id>7643065</elocation-id><history><date date-type="received"><day>23</day><month>1</month><year>2017</year></date><date date-type="rev-recd"><day>10</day><month>4</month><year>2017</year></date><date date-type="accepted"><day>21</day><month>5</month><year>2017</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2017 Yuntian Feng et al.</copyright-statement><copyright-year>2017</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>We use both reinforcement learning and deep learning to simultaneously extract entities and relations from unstructured texts. For reinforcement learning, we model the task as a two-step decision process. Deep learning is used to automatically capture the most important information from unstructured texts, which represent the state in the decision process. By designing the reward function per step, our proposed method can pass the information of entity extraction to relation extraction and obtain feedback in order to extract entities and relations simultaneously. Firstly, we use bidirectional LSTM to model the context information, which realizes preliminary entity extraction. On the basis of the extraction results, attention based method can represent the sentences that include target entity pair to generate the initial state in the decision process. Then we use Tree-LSTM to represent relation mentions to generate the transition state in the decision process. Finally, we employ <italic>Q</italic>-Learning algorithm to get control policy <italic>&#x003c0;</italic> in the two-step decision process. Experiments on ACE2005 demonstrate that our method attains better performance than the state-of-the-art method and gets a 2.4% increase in recall-score.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Information extraction [<xref rid="B1" ref-type="bibr">1</xref>] is the task of automatically extracting entities, relations, and events from unstructured texts. Researchers usually do research on entity extraction, relation extraction, and event extraction as separated tasks, but in fact there are important dependencies among tasks. For instance, entity information can further help relation extraction, so relation extraction takes the results of entity extraction as input. If just using a pipelined approach to tackle the above problem, information from each task cannot interact and get any feedback. Therefore, we make a detailed study of joint extraction of entities and relations from unstructured texts, which can pass the information of entity extraction to relation extraction and obtain feedback in order to improve the performance of entity extraction and relation extraction simultaneously.</p><p>In recent years, more and more researchers have applied deep learning to entity extraction and relation extraction. Huang et al. [<xref rid="B2" ref-type="bibr">2</xref>] proposed a bidirectional LSTM with a CRF layer (BILSTM-CRF) for sequence tagging, which included part-of-speech tagging (POS), chunking, and named entity recognition (NER). Nguyen and Grishman [<xref rid="B3" ref-type="bibr">3</xref>] proposed to combine the traditional feature-based method and the convolutional and recurrent neural networks for relation extraction. Deep learning can automatically extract features of entities and relations between entities to replace the method of designing features manually. It reduces the dependence of external resources and achieves good performance.</p><p>But how to pass entity information to relation extraction and obtain feedback is the research focus to the task of joint extraction of entities and relations, which means that we need an effective combination of different deep learning methods. To tackle the problem, we use reinforcement learning to model the task as a two-step decision process. Because it is difficult to find some measures to directly represent the state from unstructured texts, we use some deep learning methods to extract the state in the process. Firstly, we regard entity extraction as a sequence tagging task and use bidirectional LSTM to capture the context information, which preliminarily realizes the tagging of entity state. On the basis of preliminary results, we use attention based method to represent the sentences that include target entity pair and generate the initial state <italic>s</italic><sub>1</sub> in the decision process, where the first decision is made. Then we use Tree-LSTM to capture the most important information of relation mentions and generate the transition state <italic>s</italic><sub>2</sub>, where the second decision is made. The meaning of the two-step decision is as follows: the first decision is to judge if a sentence that includes target entity pair is a relation mention according to the preliminary results of entity extraction; the second decision is to classify the relation mention into a certain targeted type. By designing the reward function per step, entity information and relation information can interact. Finally, we use <italic>Q</italic>-Learning to get control policy <italic>&#x003c0;</italic> by maximizing cumulative rewards through a sequence of actions, which is essentially the mapping from state to action. In the training process of <italic>Q</italic>-Learning, all the parameters are jointly updated, which helps to realize the joint extraction of entities and relations. We conduct experiments on ACE2005 dataset and achieve better recall-score of both entity mentions and relation mentions than the state-of-the-art method. In the following, we define the task in <xref ref-type="sec" rid="sec2">Section 2</xref> and present our method in <xref ref-type="sec" rid="sec3">Section 3</xref>. Then we detail an extensive evaluation in <xref ref-type="sec" rid="sec4">Section 4</xref> and finally conclude in <xref ref-type="sec" rid="sec5">Section 5</xref>.</p></sec><sec id="sec2"><title>2. Task Definition</title><p>Our task is to extract all the entities and relations from unstructured texts simultaneously. In the section we randomly pick a sentence from ACE2005 dataset to analyze. The entity mentions and relation mention in the sentence are shown in <xref ref-type="table" rid="tab1">Table 1</xref>, where Entity ID, Relation ID, and RefID are the identifications of mentions.</p><p>
<italic>Entity Extraction.</italic> It can be taken as a sequence tagging task, which assigns a tag to each word <italic>s</italic><sub><italic>t</italic></sub> in the input sequence <italic>S</italic> = [<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>,&#x02026;, <italic>s</italic><sub><italic>n</italic></sub>]. The tag of a word means a combination of the entity type it belongs to and the boundary type it locates within. The boundary types are the Beginning, Inside, Last, Outside, and Unit of an entity (BILOU scheme). <xref ref-type="table" rid="tab1">Table 1</xref> shows two entity mentions in the sentence. The first entity mention is &#x0201c;third parties,&#x0201d; and its entity type is &#x0201c;ORG.&#x0201d; The second entity mention is &#x0201c;Entertainment,&#x0201d; and its entity type is &#x0201c;ORG.&#x0201d; ACE2005 dataset defines 7 coarse-grained entity types, which are &#x0201c;PER&#x0201d; (Person), &#x0201c;ORG&#x0201d; (Organization), &#x0201c;LOC&#x0201d; (Location), &#x0201c;GPE&#x0201d; (Geo-Political Entities), &#x0201c;FAC&#x0201d; (Facility), &#x0201c;VEH&#x0201d; (Vehicle), and &#x0201c;WEA&#x0201d; (Weapon). The types all have their own different subtypes.</p><p>
<italic>Relation Extraction.</italic> It is to extract semantic relations of the targeted types between a pair of entities. <xref ref-type="table" rid="tab1">Table 1</xref> shows one relation mention in the sentence, of which the relation type is &#x0201c;ORG-AFF.&#x0201d; The first entity argument is &#x0201c;third parties,&#x0201d; and the second entity argument is &#x0201c;Entertainment.&#x0201d; The order of the arguments cannot be changed, which means the relation type is with direction. ACE2005 dataset defines 7 coarse-grained relation types between entities, which are &#x0201c;PHYS&#x0201d; (Physical), &#x0201c;PART-WHOLE&#x0201d; (Part-Whole), &#x0201c;PER-SOC&#x0201d; (Person-Social), &#x0201c;ORG-AFF&#x0201d; (Org-Affiliation), &#x0201c;ART&#x0201d; (Artifact), &#x0201c;GEN-AFF&#x0201d; (Gen-Affiliation), and &#x0201c;METONYMY&#x0201d; (Metonymy). Similarly, the types all have their own different subtypes.</p><p>
<italic>Joint Extraction.</italic> It is to extract entities and relations in a sentence simultaneously. In the process of extraction, entity information and relation information can interact and get feedback information. Therefore, the joint extraction is more practical and different than separated entity extraction and separated relation extraction. We define and conduct research on the joint extraction task and present to use both reinforcement learning and deep learning for the task in the following section.</p></sec><sec id="sec3"><title>3. Our Method</title><p>The section combines three deep learning methods in the decision process of reinforcement learning for the joint extraction task. Firstly, we describe the two-step decision process; then we expound three deep learning methods used in this paper, that are bidirectional LSTM, attention mechanism, and Tree-LSTM; finally, we introduce <italic>Q</italic>-Learning algorithm that can get control policy <italic>&#x003c0;</italic>.</p><sec id="sec3.1"><title>3.1. Reinforcement Learning</title><p>In general, entity extraction is performed before relation extraction, and its results can also be taken as the input of relation extraction. Relation extraction is fundamentally divided into two stages: judge if a sentence that includes target entity pair is a relation mention; classify the relation mention into a targeted type. According to the thoughts, we model the joint extraction task as a two-step decision process by reinforcement learning. The two steps correspond to entity extraction and relation extraction roughly, and the specific flow is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><p>
<italic>Reinforcement Learning (RL).</italic> It [<xref rid="B4" ref-type="bibr">4</xref>] is a commonly used framework for learning control policies by the agent, through interacting with its environment.</p><p>
<italic>State.</italic> The internal state <italic>S</italic> in the environment consists of the initial state <italic>s</italic><sub>1</sub>, the transition state <italic>s</italic><sub>2</sub>, and the end state <italic>s</italic><sub><italic>e</italic></sub>. Because it is difficult to find some appropriate measures to directly represent the state from unstructured texts, we use some deep learning methods to automatically extract features of texts, which can represent the state in the decision process. To be specific, we use bidirectional LSTM (<xref ref-type="sec" rid="sec3.2">Section 3.2</xref>) to realize preliminary entity extraction and use attention based method (<xref ref-type="sec" rid="sec3.3">Section 3.3</xref>) to generate the initial state <italic>s</italic><sub>1</sub> = Att(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>1</sub>). In addition, we use Tree-LSTM (<xref ref-type="sec" rid="sec3.4">Section 3.4</xref>) to generate the transition state <italic>s</italic><sub>2</sub> = Tree(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>2</sub>). The action taken at <italic>s</italic><sub>2</sub> realizes preliminary relation extraction. <italic>X</italic> is the features of the input sentence; <bold><italic>&#x003b8;</italic></bold><sub>1</sub> and <bold><italic>&#x003b8;</italic></bold><sub>2</sub> are parameters in the above models.</p><p>
<italic>Action.</italic> There are a set of predefined actions <italic>A</italic> in the environment: Action 1 <italic>a</italic><sub>1</sub>, Action 2 <italic>a</italic><sub>2</sub>, Action 3 <italic>a</italic><sub>3</sub>, Action 4 <italic>a</italic><sub>4</sub>, and so forth. The first decision judges to take <italic>a</italic><sub>1</sub> or <italic>a</italic><sub>2</sub>. <italic>a</italic><sub>1</sub> is to judge that a sentence that includes target entity pair is not a relation mention, and <italic>a</italic><sub>2</sub> is to judge that a sentence that includes target entity pair is a relation mention. The second decision judges to take <italic>a</italic><sub>3</sub> or <italic>a</italic><sub>4</sub>&#x02026;. <italic>a</italic><sub>3</sub> is to classify the relation mention into a targeted type, and <italic>a</italic><sub>4</sub> is to classify the relation mention into another targeted type. <italic>R</italic> = <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, <italic>r</italic><sub>3</sub>, <italic>r</italic><sub>4</sub>,&#x02026; denotes the reward obtained for each action. The agent takes an action <italic>a</italic> in state <italic>s</italic> and receives a reward <italic>r</italic> from the environment. (<italic>s</italic><sub>1</sub>, <italic>a</italic><sub>1</sub>, <italic>r</italic><sub>1</sub>, <italic>s</italic><sub><italic>e</italic></sub>), (<italic>s</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, <italic>r</italic><sub>2</sub>, <italic>s</italic><sub>2</sub>), (<italic>s</italic><sub>2</sub>, <italic>a</italic><sub>3</sub>, <italic>r</italic><sub>3</sub>, <italic>s</italic><sub><italic>e</italic></sub>), and (<italic>s</italic><sub>2</sub>, <italic>a</italic><sub>4</sub>, <italic>r</italic><sub>4</sub>, <italic>s</italic><sub><italic>e</italic></sub>) denote the transitions of the decision process.</p><p>
<italic>Transition and Reward Function.</italic> A state transition tuple (<italic>s</italic><sub>1</sub>, <italic>a</italic><sub>1</sub>, <italic>r</italic><sub>1</sub>, <italic>s</italic><sub><italic>e</italic></sub>) means that the agent takes <italic>a</italic><sub>1</sub> at <italic>s</italic><sub>1</sub> and then transits to <italic>s</italic><sub><italic>e</italic></sub>. If the judgement of <italic>a</italic><sub>1</sub> is right, then the agent receives a reward <italic>r</italic><sub>1</sub> = 10; if the judgement of <italic>a</italic><sub>1</sub> is wrong, then set <italic>r</italic><sub>1</sub> = &#x02212;20 to punish the wrong judgement of the first decision. A state transition tuple (<italic>s</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, <italic>r</italic><sub>2</sub>, <italic>s</italic><sub>2</sub>) means that the agent takes <italic>a</italic><sub>2</sub> at <italic>s</italic><sub>1</sub>, then transits to <italic>s</italic><sub>2</sub>, and receives a reward <italic>r</italic><sub>2</sub> = 5. A state transition tuple (<italic>s</italic><sub>2</sub>, <italic>a</italic><sub>3</sub>, <italic>r</italic><sub>3</sub>, <italic>s</italic><sub><italic>e</italic></sub>) means that the agent takes <italic>a</italic><sub>3</sub> at <italic>s</italic><sub>2</sub> and then transits to <italic>s</italic><sub><italic>e</italic></sub>. If the judgement of <italic>a</italic><sub>3</sub> is right, then the agent receives a reward <italic>r</italic><sub>3</sub> = 10; if the judgement on type is wrong, then set <italic>r</italic><sub>3</sub> = &#x02212;10 to punish the wrong judgement of the second decision; if it is not a relation mention, then set <italic>r</italic><sub>3</sub> = &#x02212;20. The meaning of other state transition tuple (<italic>s</italic><sub>2</sub>, <italic>a</italic><sub>4</sub>, <italic>r</italic><sub>4</sub>, <italic>s</italic><sub><italic>e</italic></sub>) and the definition of its reward function are similar to those of (<italic>s</italic><sub>2</sub>, <italic>a</italic><sub>3</sub>, <italic>r</italic><sub>3</sub>, <italic>s</italic><sub><italic>e</italic></sub>).</p></sec><sec id="sec3.2"><title>3.2. BILSTM</title><p>Long Short-Term Memory (LSTM) [<xref rid="B5" ref-type="bibr">5</xref>] is a variant of recurrent neural networks (RNN) designed to cope with the gradient vanishing problem, and LSTM is very useful to find and exploit long range dependencies in the data. Now lots of LSTM variants have been proposed and applied to natural language processing tasks, such as sentiment analysis, relation classification, and question answering system. We use bidirectional LSTM (BILSTM) to model word sequence, which can efficiently make use of past features and future features. BILSTM finds the right representation of each word and assigns a tag of entity state to each word in the input sequence to realize preliminary entity extraction. BILSTM mainly consists of three representation layers: embedding layer, BILSTM layer, and output layer. <xref ref-type="fig" rid="fig2"> Figure 2</xref> gives the basic structure of the BILSTM.</p><sec id="sec3.2.1"><title>3.2.1. Embedding Layer</title><p>The embedding layer converts discrete features of each word into continuous features as input of the BILSTM layer. We do forward and backward for input sentence, so we need a special treatment at the beginning and the end of the sequence.</p><p>Part-of-speech feature can further help entity extraction, so we only use word embedding <italic>e</italic><sub><italic>t</italic></sub> and part-of-speech embedding <italic>d</italic><sub><italic>t</italic></sub> to represent each word <italic>w</italic><sub><italic>t</italic></sub> in the input sentence, which replace the method of designing features manually. After passing through the lookup table, the lowercased word is mapped to its corresponding embedding. For word feature, the lookup table is initialized by the publicly available word embeddings. For part-of-speech feature, the lookup table is randomly initialized with values drawn from a uniform distribution. The word embeddings and the part-of-speech embeddings are allowed to be modified during training.</p><p>We concatenate the word embedding <italic>e</italic><sub><italic>t</italic></sub> and the part-of-speech embedding <italic>d</italic><sub><italic>t</italic></sub> of each word <italic>w</italic><sub><italic>t</italic></sub> to generate input feature vector <italic>x</italic><sub><italic>t</italic></sub> = [<italic>e</italic><sub><italic>t</italic></sub>, <italic>d</italic><sub><italic>t</italic></sub>]. The matrix <italic>X</italic> = [<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,&#x02026;, <italic>x</italic><sub><italic>n</italic></sub>] represents the features of the whole sentence, and is passed to the BILSTM layer, where n is the length of the input sentence.</p></sec><sec id="sec3.2.2"><title>3.2.2. BILSTM Layer</title><p>Basically, each LSTM unit in the BILSTM layer is composed of three multiplicative gates: an input gate <italic>i</italic><sub><italic>t</italic></sub>, a forget gate <italic>f</italic><sub><italic>t</italic></sub>, and an output gate <italic>o</italic><sub><italic>t</italic></sub>. The gates can control the proportions of information to forget and to pass on to the next time step. In addition, there is a memory cell <italic>c</italic><sub><italic>t</italic></sub> in each LSTM unit, which can keep the previous state and memorize the features of the current input word. Therefore, the data sources of each LSTM unit are as follows: the feature vector <italic>x</italic><sub><italic>t</italic></sub> = [<italic>e</italic><sub><italic>t</italic></sub>, <italic>d</italic><sub><italic>t</italic></sub>] at time <italic>t</italic>, the hidden state vector <italic>h</italic><sub><italic>t</italic>&#x02212;1</sub> before time <italic>t</italic> or <italic>h</italic><sub><italic>t</italic>+1</sub> after time <italic>t</italic>, and the cell vector <italic>c</italic><sub><italic>t</italic>&#x02212;1</sub>. The forward passes are implemented as follows:<disp-formula id="eq1"><label>(1)</label><mml:math id="M1"><mml:mtable style="T17"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.568pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.568pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">tanh</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="7.08pt" depth="2.568pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.568pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.568pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.568pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">tanh</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="4.53pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.53pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>W</italic> are weight matrices, <italic>b</italic> are bias vectors, and their subscripts have the meaning as the name suggests. <italic>&#x003c3;</italic> denotes the logistic function.</p><p>The backward passes over time are carried out in a similar way to forward passes. The hidden state vectors of two directions <italic>h</italic><sub><italic>t</italic></sub> and <italic>h</italic><sub><italic>t</italic></sub>&#x02032; are simultaneously computed at time <italic>t</italic> in the BILSTM layer, so we can efficiently make use of past features and future features for a specific time frame.</p></sec><sec id="sec3.2.3"><title>3.2.3. Output Layer</title><p>We treat entity extraction as a sequence labeling task. By assigning an entity tag to each word, we realize preliminary entity extraction on top of the BILSTM layer. At time <italic>t</italic>, we pass the hidden state vectors of two directions <italic>h</italic><sub><italic>t</italic></sub> and <italic>h</italic><sub><italic>t</italic></sub>&#x02032; to a softmax layer.<disp-formula id="eq2"><label>(2)</label><mml:math id="M2"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="9.12598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.12598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>W</italic> are weight matrices and <italic>b</italic> is bias vector.</p></sec><sec id="sec3.2.4"><title>3.2.4. Objective Function</title><p>We employ the Viterbi algorithm to inference the tag sequence <italic>T</italic> = [<italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>,&#x02026;, <italic>t</italic><sub><italic>n</italic></sub>] for a given input sentence <italic>W</italic> = [<italic>w</italic><sub>1</sub>, <italic>w</italic><sub>2</sub>,&#x02026;, <italic>w</italic><sub><italic>n</italic></sub>]. To model the tag dependency, we use the transition score <bold>A</bold><sub><italic>ij</italic></sub> for measuring the probability of the transformation from tag <italic>i</italic> to tag <italic>j</italic>. Thus, the sentence-level score can be formulated as follows:<disp-formula id="eq3"><label>(3)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="4.13496pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.95pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="5.95pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="4.13496pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic><sub><italic>i</italic></sub>) is the score for choosing tag <italic>t</italic><sub><italic>i</italic></sub> for the <italic>i</italic>th word in the input sentence. <bold><italic>&#x003b8;</italic></bold><sub>0</sub> is the parameter set of BILSTM.</p><p>For a given training instance (<italic>W</italic><sub><italic>i</italic></sub>, <italic>T</italic><sub><italic>i</italic></sub>), <italic>W</italic><sub><italic>i</italic></sub> is a given sentence and the correct tag sequence for <italic>W</italic><sub><italic>i</italic></sub> is <italic>T</italic><sub><italic>i</italic></sub>. We search for the tag sequence with the highest score:<disp-formula id="eq4"><label>(4)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02217;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.34999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="9.34999pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is a predicted tag sequence.</p><p>The regularized objective function for <italic>m</italic> training instances is the loss function <italic>J</italic>(<bold><italic>&#x003b8;</italic></bold><sub>0</sub>) including a <italic>l</italic><sub>2</sub>-norm term:<disp-formula id="eq5"><label>(5)</label><mml:math id="M6"><mml:mtable style="T19"><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="6.349853515625pt"/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.52998pt" depth="4.43001pt"/><mml:mn mathvariant="normal">0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.34999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="9.34999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.34999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.34999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.52998pt" depth="4.43001pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="M7"><mml:mi>&#x00394;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is a structured margin loss for predicted tag sequence <inline-formula><mml:math id="M8"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. <italic>&#x003bb;</italic> is an <italic>L</italic>2 regularization hyperparameter.</p><p>To minimize <italic>J</italic>(<bold><italic>&#x003b8;</italic></bold><sub>0</sub>), we use a generalization of gradient descent called subgradient method [<xref rid="B6" ref-type="bibr">6</xref>] which computes a gradient-like direction.</p></sec></sec><sec id="sec3.3"><title>3.3. Attention Mechanism</title><p>Recently, attention mechanisms have successfully been applied to machine translation [<xref rid="B7" ref-type="bibr">7</xref>], text summarization [<xref rid="B8" ref-type="bibr">8</xref>], text comprehension [<xref rid="B9" ref-type="bibr">9</xref>], syntactic constituency parsing [<xref rid="B10" ref-type="bibr">10</xref>], relation classification [<xref rid="B11" ref-type="bibr">11</xref>], and text classification [<xref rid="B12" ref-type="bibr">12</xref>]. Inspired by those studies, we introduce attention based method to compute the hidden state vectors <italic>h</italic><sub><italic>t</italic></sub> and <italic>h</italic><sub><italic>t</italic></sub>&#x02032; in the BILSTM layer and generate the initial state <italic>s</italic><sub>1</sub> in the decision process. The method can obtain the information of entity extraction and represent the sentences that include target entity pair. After the first decision on <italic>s</italic><sub>1</sub>, we realize preliminary entity extraction and get ready to perform relation extraction. In essence, attention based method can pass entity information to relation extraction and obtain feedback information of relation extraction by jointly updating all the parameters. Attention based method better integrates entity extraction and relation extraction.</p><p>After realizing preliminary entity extraction, we choose two entities as target entity pair in the sentence <italic>W</italic> = [<italic>w</italic><sub>1</sub>, <italic>w</italic><sub>2</sub>,&#x02026;, <italic>w</italic><sub><italic>n</italic></sub>]. The attention layer is depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Let <italic>H</italic> be a matrix consisting of the hidden state vectors [<italic>h</italic><sub>1</sub>, <italic>h</italic><sub>1</sub>&#x02032;, <italic>h</italic><sub>2</sub>, <italic>h</italic><sub>2</sub>&#x02032;,&#x02026;, <italic>h</italic><sub><italic>n</italic></sub>, <italic>h</italic><sub><italic>n</italic></sub>&#x02032;] in the BILSTM layer, and <italic>H</italic> is the input of the attention layer. Then attention based method represents the sentence that includes target entity pair as a weighted sum of these hidden state vectors.<disp-formula id="eq6"><label>(6)</label><mml:math id="M9"><mml:mtable style="T17"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi>A</mml:mi><mml:malignmark/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">tanh</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="6.57999pt" depth="0.0pt"/><mml:mi>H</mml:mi><mml:mspace height="6.57999pt" depth="0.0pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi>&#x003b1;</mml:mi><mml:malignmark/><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.49698pt" depth="0.12pt"/><mml:msup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mspace height="9.49698pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">tanh</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="9.49698pt" depth="0.12pt"/><mml:mi>H</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace height="9.49698pt" depth="0.12pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>&#x003b1;</italic> is the normalized weight vector and <italic>&#x003c9;</italic> is a parameter vector. <italic>s</italic><sub>1</sub> is the initial state, in which we denote by <italic>s</italic><sub>1</sub> = Att(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>1</sub>), and <bold><italic>&#x003b8;</italic></bold><sub>1</sub> represents all the parameters in this method.</p><p>After generating the initial state <italic>s</italic><sub>1</sub>, the first decision will be made to judge if a sentence that includes target entity pair is a relation mention. We pass <italic>s</italic><sub>1</sub> to a softmax output layer to get <italic>y</italic><sub><italic>a</italic></sub>, which is the probability of relation mention and nonrelation for a sentence. Finally, we can determine to take <italic>a</italic><sub>1</sub> or <italic>a</italic><sub>2</sub>.<disp-formula id="eq7"><label>(7)</label><mml:math id="M10"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>W</italic> is weight matric and <italic>b</italic> is bias vector.</p><p>The objective function for <italic>m</italic> training instances is the negative log-likelihood:<disp-formula id="eq8"><label>(8)</label><mml:math id="M11"><mml:mtable style="T2"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.4pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.4pt"/></mml:mrow></mml:mfenced><mml:malignmark/><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.35999pt" depth="0.12pt"/><mml:mn mathvariant="normal">0</mml:mn><mml:mspace height="6.35999pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.35999pt" depth="0.0pt"/><mml:mn mathvariant="normal">1</mml:mn><mml:mspace height="6.35999pt" depth="0.0pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.4pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.4pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>t</italic><sub>0</sub><sup>(<italic>i</italic>)</sup> and <italic>t</italic><sub>1</sub><sup>(<italic>i</italic>)</sup> are the one-hot represented ground truth. <italic>y</italic><sub><italic>a</italic></sub><sup>(<italic>i</italic>)</sup>(0) and <italic>y</italic><sub><italic>a</italic></sub><sup>(<italic>i</italic>)</sup>(1) are the estimated probability for relation mention and nonrelation, respectively. <italic>&#x003bb;</italic> is an <italic>L</italic>2 regularization hyperparameter.</p><p>To minimize <italic>J</italic>(<bold><italic>&#x003b8;</italic></bold><sub>1</sub>), we use a simple optimization technique called stochastic gradient descent (SGD).</p></sec><sec id="sec3.4"><title>3.4. Tree-LSTM</title><p>Unlike traditional sequence LSTM, Tree-LSTM [<xref rid="B13" ref-type="bibr">13</xref>] is constructed over a tree structure. As is known to all, the dependency tree is very useful for analyzing the relations between words. Two words may be far apart in the linear structure and separated by many unrelated words or preposition structure, but they are in hyponymy for the dependency tree. Therefore, we construct the Tree-LSTM over the dependency tree to represent relation mentions in a bottom-up way. Tree-LSTM can extract the core dependency relation between target entity pair and generate the transition state <italic>s</italic><sub>2</sub> in the decision process. The second decision on <italic>s</italic><sub>2</sub> performs preliminary relation extraction.</p><p>We take the relation mention &#x0201c;AFP_ENG_20030319.0879-R2&#x0201d; in <xref ref-type="table" rid="tab1">Table 1</xref> as an example to illustrate, and the two entity arguments are &#x0201c;third parties&#x0201d; and &#x0201c;Entertainment.&#x0201d; Firstly, we perform dependency parsing on the relation mention and generate the dependency tree, as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Instead of using the full mention boundary, we use head spans for entities directly. The entity head of &#x0201c;third parties&#x0201d; is &#x0201c;parties,&#x0201d; and the entity head of &#x0201c;Entertainment&#x0201d; is &#x0201c;Entertainment.&#x0201d; The core dependency relation between target entity pair is shown by red lines in <xref ref-type="fig" rid="fig4">Figure 4</xref>. So we use dependency tree as a backbone to construct Tree-LSTM. Moreover, for the convenience of implementation, we prune or pad dependency trees to keep the same depth and width.</p><p>Like BILSTM, each LSTM unit of Tree-LSTM takes continuous feature vector of a word as input. In addition to word embedding <italic>e</italic><sub><italic>t</italic></sub> and part-of-speech embedding <italic>d</italic><sub><italic>t</italic></sub>, we use entity type embedding <italic>t</italic><sub><italic>t</italic></sub> and entity position embedding <italic>l</italic><sub><italic>t</italic></sub>, to which entity type feature and entity position feature are mapped. We can get the entity type features from the preliminary results of entity extraction and get the entity position features by computing the relative distances of the current word to the two entity arguments. Unlike BILSTM, the LSTM unit does not accept hidden state vectors of the adjacent words and accept the hidden state vectors of all children nodes <italic>h</italic><sub><italic>tk</italic></sub> as input. The Tree-LSTM is developed from its leaf node in a recursive way up to the root, which is the common ancestor (&#x0201c;divesting&#x0201d; in <xref ref-type="fig" rid="fig4">Figure 4</xref>) of all the words. Then we carry out nonlinear transformation on the hidden state vector of the ancestor to generate <italic>s</italic><sub>2</sub>, which is the final representation of relation mentions and serves as the transition state in the decision process. We denote <italic>s</italic><sub>2</sub> by <italic>s</italic><sub>2</sub> = Tree(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>2</sub>), and <bold><italic>&#x003b8;</italic></bold><sub>2</sub> represents all the parameters in the Tree-LSTM.</p><p>After generating the transition state <italic>s</italic><sub>2</sub>, the second decision will be made to classify the relation mention into a targeted type. Then <italic>s</italic><sub>2</sub> is passed to a softmax output layer to get <italic>y</italic><sub><italic>r</italic></sub>, which is the probability of different types for a relation mention. Finally, we choose a type with the maximum probability, which determines to take <italic>a</italic><sub>3</sub> or <italic>a</italic><sub>4</sub>&#x02026;.<disp-formula id="eq9"><label>(9)</label><mml:math id="M12"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>W</italic> is weight matric and <italic>b</italic> is bias vector. At each dependency tree, we use a softmax layer to predict the type for the root node given the inputs <italic>X</italic> observed at its children nodes.</p><p>The objective function for <italic>m</italic> training instances is the negative log-likelihood:<disp-formula id="eq10"><label>(10)</label><mml:math id="M13"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.4pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.4pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.4pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.4pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here, <italic>y</italic><sub><italic>r</italic></sub><sup>(<italic>i</italic>)</sup> is the estimated probability for the true type at each root node. The root node of Tree-LSTM is able to selectively incorporate information from each child. <italic>&#x003bb;</italic> is an <italic>L</italic>2 regularization hyperparameter.</p><p>To minimize <italic>J</italic>(<bold><italic>&#x003b8;</italic></bold><sub>2</sub>), we use AdaGrad [<xref rid="B14" ref-type="bibr">14</xref>].</p></sec><sec id="sec3.5"><title>3.5. <italic>Q</italic>-Learning</title><p>
<italic>Q</italic>-Learning algorithm [<xref rid="B15" ref-type="bibr">15</xref>] is a popular form of reinforcement learning and can be used to learn an optimal state-action value function <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>) for the agent. The agent takes an action <italic>a</italic> in state <italic>s</italic> by consulting <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>), which is a measure of the action's expected long-term reward. The aim is to maximize some cumulative rewards through a sequence of actions. As the state space is infinite in the decision process, it is impractical to obtain <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>) for all possible state-action pairs.</p><p>For the above challenge, we approximate <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>) using a neural network, which can represent <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>) as a parameterized function <italic>Q</italic><sub><bold>&#x003b7;</bold></sub>(<italic>s</italic>, <italic>a</italic>) = MLP(<italic>&#x003d5;</italic>(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold>), <italic>a</italic>; <bold>&#x003b7;</bold>). <italic>&#x003d5;</italic>(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold>) refers to <italic>s</italic><sub>1</sub> = Att(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>1</sub>) and <italic>s</italic><sub>2</sub> = Tree(<italic>X</italic>; <bold><italic>&#x003b8;</italic></bold><sub>2</sub>) above, where <bold><italic>&#x003b8;</italic></bold> can be obtained by pretraining the deep learning models above and <bold>&#x003b7;</bold> represents the parameters in the neural network, which are learnt by performing stochastic gradient descent step with RMSprop [<xref rid="B16" ref-type="bibr">16</xref>].</p><p>To approximate the real value function <italic>Q</italic><sup><italic>&#x003c0;</italic></sup> as closely as possible, we measure the degree of approximation with the least squares error:<disp-formula id="eq11"><label>(11)</label><mml:math id="M14"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003b7;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="11.48999pt" depth="4.43001pt"/><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.93999pt" depth="4.07999pt"/><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.53pt" depth="1.16998pt"/><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mspace height="4.53pt" depth="1.16998pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003b7;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.53pt" depth="1.16998pt"/><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mspace height="4.53pt" depth="1.16998pt"/></mml:mrow></mml:mfenced><mml:mspace height="6.93999pt" depth="4.07999pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mspace height="11.48999pt" depth="4.43001pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In <italic>Q</italic>-Learning, we use the estimated value function <italic>Q</italic><sub><bold>&#x003b7;</bold></sub>(<italic>s</italic>, <italic>a</italic>) instead of the real value function <italic>Q</italic><sup><italic>&#x003c0;</italic></sup>(<italic>s</italic>, <italic>a</italic>). During each epoch, the updates of parameters aim to reduce the discrepancy between the estimation <italic>Q</italic><sub><bold>&#x003b7;</bold></sub>(<italic>s</italic>, <italic>a</italic>) and the expectation <italic>Q</italic><sup><italic>&#x003c0;</italic></sup>(<italic>s</italic>, <italic>a</italic>). The agent starts from a random <italic>Q</italic><sub><bold>&#x003b7;</bold></sub>(<italic>s</italic>, <italic>a</italic>) and continuously updates its values by making the decisions and obtaining rewards. Then the agent can maximize its expected future rewards by choosing the action with the highest <italic>Q</italic><sub><bold>&#x003b7;</bold></sub>(<italic>s</italic>, <italic>a</italic>&#x02032;&#x02032;). Finally, <italic>Q</italic>-Learning algorithm gets control policy <italic>&#x003c0;</italic> in the two-step decision process. <xref ref-type="fig" rid="alg1"> Algorithm 1</xref> details the <italic>Q</italic>-Learning training procedure.</p><p>During the training procedure we pretrain BILSTM, the attention layer, and Tree-LSTM, respectively. The training parameters mainly include all the parameters <bold><italic>&#x003b8;</italic></bold><sub>0</sub> in BLSTM, all the parameters <bold><italic>&#x003b8;</italic></bold><sub>1</sub> in the attention layer, and all the parameters <bold><italic>&#x003b8;</italic></bold><sub>2</sub> in Tree-LSTM.</p><p>The functionality of the attention model in our RL method is very similar to that of a separate relation mention classification part in a pipeline. We use deep learning methods to represent words and sentences in the text and use RL to combine three tasks in the decision process, that are entity extraction, relation mention classification, and relation classification. The pipeline architecture just passes the information of entity extraction to relation extraction and does not enable information to flow in the global architecture. However, our RL method not only combines the above tasks sequentially but also globally makes decisions. At the beginning, the decisions have close to a random chance. After several epochs, they will be stabilizing. Meanwhile, the parameters in our architecture are globally updated and eventually converge. Therefore, our RL method can obtain feedback from decision-making and state changes and enable information to flow in the global architecture. The attention model connects entity extraction task with relation extraction task, thus helping us to realize the joint extraction of entities and relations. Experimental results demonstrate that our RL method performs slightly better than the pipeline method for both entity extraction and relation extraction, which shows that we are on the right track.</p></sec></sec><sec id="sec4"><title>4. Experiments</title><sec id="sec4.1"><title>4.1. Data</title><p>Most previous work has reported results on ACE2005 data set, so we evaluate our method on ACE2005 for joint extraction of entities and relations. We use three common metrics to evaluate the performance: microprecision (<italic>P</italic>), recall (<italic>R</italic>), and primary micro <italic>F</italic>1-scores (<italic>F</italic>1). An entity mention is correct when its entity type and the region of its head are correct, and a relation mention is correct when its relation type and both entity arguments are correct.</p><p>Data source for English in ACE2005 is as follows: 20% Newswire (NW), 20% Broadcast News (BN), 15% Broadcast Conversation (BC), 15% Weblog (WL), 15% Usenet Newsgroups/Discussion Forum (UN), and 15% Conversational Telephone Speech (CTS). The two small subsets UN and CTS are informal, so we remove them. In addition, in order to compare with state of the art, we employ the same method as previous work [<xref rid="B17" ref-type="bibr">17</xref>] to split and preprocess the data. Training set contains 351 documents, development set contains 80 documents, and testing set contains 80 documents.</p></sec><sec id="sec4.2"><title>4.2. Hyperparameters</title><p>We set up Python2.7 + Theano + Cuda7.5 environments to implement our method. We use the publicly available word embedding Glove [<xref rid="B18" ref-type="bibr">18</xref>] to initialize the word embedding table, and its dimension <italic>n</italic><sub><italic>e</italic></sub> is 300. We fix the dimension of part-of-speech embedding <italic>n</italic><sub><italic>d</italic></sub> and the dimension of entity type embedding <italic>n</italic><sub><italic>t</italic></sub> to 50 and fix the dimension of entity position embedding <italic>n</italic><sub><italic>l</italic></sub> to 5. Those feature embeddings are randomly initialized and allowed to be modified during training. In addition, we fix the state size of all the LSTM units to 200 and fix the dimensions of other hidden layers to 100. We use tanh for the nonlinear function.</p><p>We tune hyperparameters using development set to achieve high <italic>F</italic>1. The best hyperparameters are as follows. Dropout rate [<xref rid="B19" ref-type="bibr">19</xref>] is 0.5, minibatch size is 30, the constraint of max-norm regularization is equal to 3, and initial learning rate is 0.0005. The reward after each action is described in the <xref ref-type="sec" rid="sec3.1">Section 3.1</xref>. Therefore, for all the experiments below, we will directly employ the best hyperparameters.</p></sec><sec id="sec4.3"><title>4.3. Overall Performance</title><p>We run experiments to analyze the effectiveness of the various components of our joint extraction method.</p><p>Firstly, we compare the performance of BILSTM with a baseline system, LSTM for entity extraction task. We train models using training set and report models' performance on development set in <xref ref-type="table" rid="tab2">Table 2</xref>. The result shows that BILSTM obtains better performance than LSTM on all evaluation metrics. Bidirectional model can actually improve the performance of sequence tagging task. Therefore, throughout the experiment, we will use BILSTM to extract entities.</p><p>Then, to demonstrate the effectiveness of the relation extraction component of our method, we carry out experiments on relation extraction when entities are known. We build a baseline system, CNN. In addition, we parse relation mentions using the Stanford neural dependency parser [<xref rid="B20" ref-type="bibr">20</xref>] and directly use Tree-LSTM extract relations. On the basis of Tree-LSTM, we use reinforcement learning method to control the process of relation extraction. We compare the performance of the above three methods on development set in <xref ref-type="table" rid="tab3">Table 3</xref>. The result demonstrates that Tree-LSTM is better suited to extract relations than CNN, and reinforcement learning method obtains a substantial gain in recall-score over Tree-LSTM with 3.7%. Therefore, in the rest of the experiment, we will use reinforcement learning method based on Tree-LSTM to extract relations.</p><p>Finally, we demonstrate the effectiveness of our joint extraction method. We build a pipelined system, which directly connects the entity extraction component and the relation extraction component above. To be specific, the pipelined system first trains the entity extraction model and then builds a separate relation extraction model using the detected entities. Our joint system is based on the pipelined system. The joint system uses attention based method to pass entity information to relation extraction and updates the parameters in all the components simultaneously during the training procedure for <italic>Q</italic>-Learning, which realizes the joint extraction of entities and relations. We compare the performance of the two systems on development set in <xref ref-type="table" rid="tab4">Table 4</xref>. The result demonstrates that our joint system slightly improves the performance of entity extraction and significantly improves the performance of relation extraction. Therefore, the experiments show that our method is effective and practical.</p><p>We will clearly show the process of the above experiments. <xref ref-type="fig" rid="fig5"> Figure 5</xref> shows the average reward after each training epoch. At the beginning of training, the reward is negative, because the agent takes actions randomly. But with the increase of epoch number, the reward improves gradually. <xref ref-type="fig" rid="fig6"> Figure 6</xref> shows the learning curves of the performance for entity extraction and relation extraction. The <italic>F</italic>1-score in both (a) and (b) increases simultaneously. From the two figures, we can clearly see that all the metrics significantly improve and then stabilize after 13 epochs of training. So we set the number of training epochs as 13.</p></sec><sec id="sec4.4"><title>4.4. Comparison with State of the Art</title><p>Now deep learning methods achieve state-of-the-art performance in end-to-end relation extraction task. Miwa and Bansal [<xref rid="B21" ref-type="bibr">21</xref>] stacked bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs to extract entities and relations between them, which could capture both word sequence and dependency tree substructure information. The method is denoted by SPTree. <xref ref-type="table" rid="tab5"> Table 5</xref> compares our joint extraction method with SPTree on the testing set and shows that our method performs slightly better than SPTree for both entity mentions and relation mentions. Although our method is not comparable with SPTree in precision-score, our method outperforms the best results of SPTree in recall-score. The main reason is that the reward after each action in reinforcement learning may play an important role.</p></sec><sec id="sec4.5"><title>4.5. Analysis</title><p>We pretrain the attention model which is used for relation mention classification. Relation mention classification is always processed in a very unbalanced corpus, where most sentences are not a relation mention. From <xref ref-type="fig" rid="fig7">Figure 7</xref>, we see that the SGD algorithm gets to the minimum objective fast, but the objective function's value is a bit high. That means that during the pretraining of the attention model there would be a huge loss. The parameters in the attention layer are updated to accepted values, which are prepared for <italic>Q</italic>-Learning. When we do <italic>Q</italic>-Learning, we learn a stacked MLP on top of the attention model (without softmax output layer). From <xref ref-type="fig" rid="fig7">Figure 7</xref>, we see that <italic>Q</italic>-Learning takes more epochs to converge but reduces the value of the objective function in the first stage of the MDP. That means that our reinforcement learning method is effective despite the huge loss and poor initialization in the pretraining of the attention model. Moreover, <xref ref-type="fig" rid="fig8">Figure 8</xref> shows the learning curves of the performance for relation mention classification. We can see that our reinforcement learning method gets good performance in the <italic>F</italic>1-score, which is also a proof of our effectiveness.</p></sec></sec><sec id="sec5"><title>5. Related Work</title><p>As for joint extraction of entities and relations, the research has been dominated by four methods. The first one is structured prediction. Li and Ji [<xref rid="B17" ref-type="bibr">17</xref>] presented an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search. The second one is integer linear programming. Dan and Yih [<xref rid="B22" ref-type="bibr">22</xref>] studied global inference for entity and relation identification via a linear programming formulation. The third one is card-pyramid parsing. Kate and Mooney [<xref rid="B23" ref-type="bibr">23</xref>] presented a new method for joint entity and relation extraction using card-pyramid parsing. The last one is global probabilistic graphical models. Yu and Lam [<xref rid="B24" ref-type="bibr">24</xref>] jointly identified entities and extracted relations in encyclopedia text via a graphical model approach.</p><p>Recently, deep learning methods have been widely used in many research areas with the aim of reducing the number of handcrafted features. However, the only work of end-to-end (joint) extraction of relations between entities with deep learning methods is due to Miwa and Bansal [<xref rid="B21" ref-type="bibr">21</xref>], and most researchers simply solve entity extraction, relation classification, or relation extraction separately. Chiu and Nichols [<xref rid="B25" ref-type="bibr">25</xref>] presented a novel neural network architecture for named entity recognition, which automatically detected word- and character-level features using a hybrid bidirectional LSTM and CNN architecture. Zhang et al. [<xref rid="B26" ref-type="bibr">26</xref>] proposed bidirectional long short-term memory networks (BLSTM) to model the sentence with complete, sequential information about all words for relation classification. Nguyen and Grishman [<xref rid="B27" ref-type="bibr">27</xref>] departed from these traditional approaches with complicated feature engineering by introducing a convolutional neural network for relation extraction.</p><p>At present, the research of reinforcement learning has risen. El-Laithy and Bogdan [<xref rid="B28" ref-type="bibr">28</xref>] presented a reinforcement learning framework for spiking networks with dynamic synapses. Mousavi et al. [<xref rid="B29" ref-type="bibr">29</xref>] discussed the notion of context transfer in reinforcement learning tasks. However, few researchers apply reinforcement learning in text processing tasks. We use both reinforcement learning and deep learning to simultaneously extract entities and relations from unstructured texts. To the best of our knowledge, there has been no work on employing reinforcement learning for information extraction so far. This paper is the first attempt to fill in that gap and provides a good thinking way for future research in this area.</p></sec><sec id="sec6"><title>6. Conclusions</title><p>In this paper we define and research the joint extraction of entities and relations. We model the task as a two-step decision process in reinforcement learning. In addition, we use deep learning methods to represent the state in the decision process. Attention based method can pass entity information to relation extraction task. During the training procedure for <italic>Q</italic>-Learning, all the parameters are updated simultaneously to realize the interaction and feedback of entity information and relation information. The reward after each action in reinforcement learning apparently helps to improve the recall-score. Under the same experimental conditions, our method outperforms the state-of-the-art method in <italic>F</italic>1-score of entity mentions and relation mentions. In future work, we plan to perfect the model of the two-step decision process and optimize the <italic>Q</italic>-Learning algorithm.</p></sec></body><back><sec><title>Conflicts of Interest</title><p>The authors declare that they have no conflicts of interest.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="other"><article-title>Information Extraction: a multidisciplinary approach to an emerging information technology</article-title><comment>Springer, Heidelberg, Germany, 1997</comment></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Xu</surname><given-names>W.</given-names></name><name><surname>Yu</surname><given-names>K.</given-names></name></person-group><article-title>Bidirectional LSTM-CRF models for sequence tagging</article-title><comment>2015, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1508.01991v1">https://arxiv.org/abs/1508.01991v1</ext-link></comment></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Nguyen H</surname><given-names>T.</given-names></name><name><surname>Grishman</surname><given-names>R.</given-names></name></person-group><article-title>Combining Neural Networks and Log-linear Models to Improve Relation Extraction</article-title><comment>2015, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.05926">https://arxiv.org/abs/1511.05926</ext-link></comment></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Amato</surname><given-names>C.</given-names></name><name><surname>Shani</surname><given-names>G.</given-names></name></person-group><article-title>High-level reinforcement learning in strategy games</article-title><conf-name>Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems</conf-name><conf-date>2010</conf-date><publisher-name>International Foundation for Autonomous Agents and Multiagent Systems</publisher-name><fpage>75</fpage><lpage>82</lpage></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S.</given-names></name><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group><article-title>Long short-term memory</article-title><source><italic>Neural Computation</italic></source><year>1997</year><volume>9</volume><issue>8</issue><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="other">2-s2.0-0031573117</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratliff</surname><given-names>N. D.</given-names></name><name><surname>Bagnell</surname><given-names>J. A.</given-names></name><name><surname>Zinkevich</surname><given-names>M. A.</given-names></name></person-group><article-title>subgradient methods for structured prediction</article-title><source><italic>Journal of Machine Learning Research</italic></source><year>2007</year><volume>2</volume><fpage>380</fpage><lpage>387</lpage><pub-id pub-id-type="other">2-s2.0-84862297087</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D.</given-names></name><name><surname>Cho</surname><given-names>K.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Neural machine translation by jointly learning to align and translate</article-title><comment>2014, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</ext-link></comment></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Rush</surname><given-names>A. M.</given-names></name><name><surname>Chopra</surname><given-names>S.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name></person-group><article-title>A Neural Attention Model for Abstractive Sentence Summarization</article-title><comment>2015, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1509.00685">https://arxiv.org/abs/1509.00685</ext-link></comment><pub-id pub-id-type="doi">10.18653/v1/D15-1044</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rush</surname><given-names>A. M.</given-names></name><name><surname>Chopra</surname><given-names>S.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name></person-group><article-title>A Neural Attention Model for Abstractive Sentence Summarization</article-title><conf-name>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</conf-name><conf-date>September 2015</conf-date><conf-loc>Lisbon, Portugal</conf-loc><fpage>379</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.18653/v1/D15-1044</pub-id></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinyals</surname><given-names>O.</given-names></name><name><surname>Kaiser</surname><given-names>L.</given-names></name><name><surname>Koo</surname><given-names>T.</given-names></name><name><surname>Petrov</surname><given-names>S.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Grammar as a foreign language</article-title><source><italic>Advances in Neural Information Processing Systems</italic></source><year>2015</year><fpage>2773</fpage><lpage>2781</lpage><pub-id pub-id-type="other">2-s2.0-84965136196</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>de Melo</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name></person-group><article-title>Relation classification via multi-level attention CNNs</article-title><conf-name>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name><conf-date>August 2016</conf-date><conf-loc>Berlin, Germany</conf-loc><fpage>1298</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.18653/v1/P16-1123</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Yang</surname><given-names>D.</given-names></name><name><surname>Dyer</surname><given-names>C.</given-names></name><name><surname>He</surname><given-names>X.</given-names></name><name><surname>Smola</surname><given-names>A.</given-names></name><name><surname>Hovy</surname><given-names>E.</given-names></name></person-group><article-title>Hierarchical attention networks for document classification</article-title><conf-name>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics</conf-name><conf-date>June 2016</conf-date><publisher-name>Human Language Technologies</publisher-name><fpage>1480</fpage><lpage>1489</lpage><pub-id pub-id-type="other">2-s2.0-84994158553</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tai</surname><given-names>K. S.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group><article-title>Improved semantic representations from tree-structured long short-term memory networks</article-title><conf-name>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</conf-name><conf-date>July 2015</conf-date><conf-loc>Beijing, China</conf-loc><fpage>1556</fpage><lpage>1566</lpage><pub-id pub-id-type="doi">10.3115/v1/P15-1150</pub-id></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchi</surname><given-names>J.</given-names></name><name><surname>Hazan</surname><given-names>E.</given-names></name><name><surname>Singer</surname><given-names>Y.</given-names></name></person-group><article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title><source><italic>Journal of Machine Learning Research (JMLR)</italic></source><year>2011</year><volume>12</volume><fpage>2121</fpage><lpage>2159</lpage><pub-id pub-id-type="other">MR2825422</pub-id></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>C. J. C. H.</given-names></name><name><surname>Dayan</surname><given-names>P.</given-names></name></person-group><article-title>Q-learning</article-title><source><italic>Machine Learning</italic></source><year>1992</year><volume>8</volume><issue>3-4</issue><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1007/BF00992698</pub-id><pub-id pub-id-type="other">Zbl0773.68062</pub-id><pub-id pub-id-type="other">2-s2.0-34249833101</pub-id></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tieleman</surname><given-names>T.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude</article-title><source><italic>Coursera: Neural Networks for Machine Learning</italic></source><year>2012</year><volume>4</volume><issue>2</issue></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Ji</surname><given-names>H.</given-names></name></person-group><article-title>Incremental Joint Extraction of Entity Mentions and Relations</article-title><comment>2014</comment><pub-id pub-id-type="other">2-s2.0-84906932622</pub-id></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>J.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group><article-title>GloVe: global vectors for word representation</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014</conf-name><conf-date>October 2014</conf-date><fpage>1532</fpage><lpage>1543</lpage><pub-id pub-id-type="other">2-s2.0-84961289992</pub-id></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G. E.</given-names></name><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><article-title>Improving neural networks by preventing co-adaptation of feature detectors</article-title><comment>2012, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</ext-link></comment></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group><article-title>A fast and accurate dependency parser using neural networks</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014</conf-name><conf-date>October 2014</conf-date><fpage>740</fpage><lpage>750</lpage><pub-id pub-id-type="other">2-s2.0-84951272941</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Miwa</surname><given-names>M.</given-names></name><name><surname>Bansal</surname><given-names>M.</given-names></name></person-group><article-title>End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures</article-title><comment>2016, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.00770">https://arxiv.org/abs/1601.00770</ext-link></comment></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dan</surname><given-names>R.</given-names></name><name><surname>Yih</surname><given-names>W. T.</given-names></name></person-group><comment>Global Inference for Entity and Relation Identification via a Linear Programming Formulation, 2007</comment></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kate</surname><given-names>J. R.</given-names></name><name><surname>Mooney</surname><given-names>R. J.</given-names></name></person-group><article-title>Joint entity and relation extraction using card-pyramid parsing</article-title><conf-name>Proceedings of the Fourteenth Conference on Computational Natural Language Learning</conf-name><conf-date>2010</conf-date><publisher-name>Association for Computational Linguistics</publisher-name><fpage>203</fpage><lpage>212</lpage></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Lam</surname><given-names>W.</given-names></name></person-group><article-title>Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</article-title><volume>23-27</volume><conf-name>Proceedings of the International Conference on Computational Linguistics</conf-name><conf-date>August 2010</conf-date><conf-loc>Beijing , China</conf-loc><fpage>1399</fpage><lpage>1407</lpage></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>P. C. J.</given-names></name><name><surname>Nichols</surname><given-names>E.</given-names></name></person-group><article-title>Named Entity Recognition with Bidirectional LSTM-CNNs</article-title><comment>2015, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.08308">https://arxiv.org/abs/1511.08308</ext-link></comment></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Zheng</surname><given-names>D.</given-names></name><name><surname>Hu</surname><given-names>X.</given-names></name></person-group><comment>Bidirectional Long Short-Term Memory Networks for Relation Classification, 2015</comment></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T. H.</given-names></name><name><surname>Grishman</surname><given-names>R.</given-names></name></person-group><article-title>Relation extraction: perspective from convolutional neural networks</article-title><source><italic>The Workshop on Vector Space Modeling for Natural Language Processing</italic></source><year>2015</year><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.3115/v1/W15-1506</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Laithy</surname><given-names>K.</given-names></name><name><surname>Bogdan</surname><given-names>M.</given-names></name></person-group><article-title>A reinforcement learning framework for spiking networks with dynamic synapses</article-title><source><italic>Computational Intelligence and Neuroscience</italic></source><year>2011</year><volume>2011</volume><fpage>12</fpage><pub-id pub-id-type="publisher-id">869348</pub-id><pub-id pub-id-type="doi">10.1155/2011/869348</pub-id><pub-id pub-id-type="other">2-s2.0-84855176553</pub-id></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mousavi</surname><given-names>A.</given-names></name><name><surname>Araabi</surname><given-names>B. N.</given-names></name><name><surname>Ahmadabadi</surname><given-names>M. N.</given-names></name></person-group><article-title>Context transfer in reinforcement learning using action-value functions</article-title><source><italic>Computational Intelligence and Neuroscience</italic></source><year>2014</year><volume>2014</volume><pub-id pub-id-type="publisher-id">428567</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Two-step decision process.</p></caption><graphic xlink:href="CIN2017-7643065.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Basic structure of BILSTM.</p></caption><graphic xlink:href="CIN2017-7643065.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Attention layer.</p></caption><graphic xlink:href="CIN2017-7643065.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Dependency tree of a relation mention.</p></caption><graphic xlink:href="CIN2017-7643065.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Learning curve of average reward.</p></caption><graphic xlink:href="CIN2017-7643065.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Learning curves of the performance.</p></caption><graphic xlink:href="CIN2017-7643065.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Objective values in the attention model.</p></caption><graphic xlink:href="CIN2017-7643065.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Performance of relation mention classification.</p></caption><graphic xlink:href="CIN2017-7643065.008"/></fig><fig id="alg1" orientation="portrait" position="float"><label>Algorithm 1</label><caption><p>Training procedure for <italic>Q</italic>-Learning.</p></caption><graphic xlink:href="CIN2017-7643065.alg.001"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>A sentence in ACE2005 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Sentence</th><th colspan="2" align="center" rowspan="1"> While either divesting or inviting third parties to take a minority stake in the remaining Entertainment assets.</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Entity ID = &#x0201c;AFP_ENG_20030319.0879-E24&#x0201d;</td><td align="center" rowspan="1" colspan="1">Type = &#x0201c;ORG&#x0201d;<break/>Subtype = &#x0201c;Commercial&#x0201d;</td><td align="center" rowspan="1" colspan="1">third parties</td></tr><tr><td colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Entity ID = &#x0201c;AFP_ENG_20030319.0879-E25&#x0201d;</td><td align="center" rowspan="1" colspan="1">Type = &#x0201c;ORG&#x0201d;<break/>Subtype = &#x0201c;Entertainment&#x0201d;</td><td align="center" rowspan="1" colspan="1">Entertainment</td></tr><tr><td colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td rowspan="2" align="left" colspan="1">Relation ID = &#x0201c;AFP_ENG_20030319.0879-R2&#x0201d; </td><td rowspan="2" align="center" colspan="1">Type = &#x0201c;ORG-AFF&#x0201d;<break/>Subtype = &#x0201c;Investor-Shareholder&#x0201d; </td><td align="center" rowspan="1" colspan="1">RefID = &#x0201c;AFP_ENG_20030319.0879-E24&#x0201d;<break/>Role = &#x0201c;Arg-1&#x0201d;</td></tr><tr><td align="center" rowspan="1" colspan="1">RefID = &#x0201c;AFP_ENG_20030319.0879-E25&#x0201d;<break/>Role = &#x0201c;Arg-2&#x0201d;</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Performance for entity extraction task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" rowspan="1">Entity</th></tr><tr><th align="left" rowspan="1" colspan="1">Score</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">LSTM</td><td align="center" rowspan="1" colspan="1">81.0</td><td align="center" rowspan="1" colspan="1">78.1</td><td align="center" rowspan="1" colspan="1">79.5</td></tr><tr><td align="left" rowspan="1" colspan="1">BILSTM</td><td align="center" rowspan="1" colspan="1">82.5</td><td align="center" rowspan="1" colspan="1">79.8</td><td align="center" rowspan="1" colspan="1">81.1</td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Performance for relation extraction task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" rowspan="1">Relation</th></tr><tr><th align="left" rowspan="1" colspan="1">Score</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="center" rowspan="1" colspan="1">63.1</td><td align="center" rowspan="1" colspan="1">52.9</td><td align="center" rowspan="1" colspan="1">57.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Tree-LSTM</td><td align="center" rowspan="1" colspan="1">63.9</td><td align="center" rowspan="1" colspan="1">54.1</td><td align="center" rowspan="1" colspan="1">58.6</td></tr><tr><td align="left" rowspan="1" colspan="1">RL</td><td align="center" rowspan="1" colspan="1">63.6</td><td align="center" rowspan="1" colspan="1">59.4</td><td align="center" rowspan="1" colspan="1">61.4</td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Performance of two extraction systems.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" rowspan="1">Entity</th><th colspan="3" align="center" rowspan="1">Relation</th></tr><tr><th align="left" rowspan="1" colspan="1">Score</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Pipeline</td><td align="center" rowspan="1" colspan="1">82.5</td><td align="center" rowspan="1" colspan="1">79.8</td><td align="center" rowspan="1" colspan="1">81.1</td><td align="center" rowspan="1" colspan="1">60.2</td><td align="center" rowspan="1" colspan="1">43.9</td><td align="center" rowspan="1" colspan="1">50.8</td></tr><tr><td align="left" rowspan="1" colspan="1">Joint</td><td align="center" rowspan="1" colspan="1">83.6</td><td align="center" rowspan="1" colspan="1">80.4</td><td align="center" rowspan="1" colspan="1">82.0</td><td align="center" rowspan="1" colspan="1">60.6</td><td align="center" rowspan="1" colspan="1">45.9</td><td align="center" rowspan="1" colspan="1">52.2</td></tr></tbody></table></table-wrap><table-wrap id="tab5" orientation="portrait" position="float"><label>Table 5</label><caption><p>Comparison with state of the art.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" rowspan="1">Entity</th><th colspan="3" align="center" rowspan="1">Relation</th></tr><tr><th align="center" rowspan="1" colspan="1">Score</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>R</italic> (%)</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>1 (%)</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">SPTree</td><td align="center" rowspan="1" colspan="1">85.5</td><td align="center" rowspan="1" colspan="1">81.2</td><td align="center" rowspan="1" colspan="1">83.3</td><td align="center" rowspan="1" colspan="1">65.8</td><td align="center" rowspan="1" colspan="1">42.9</td><td align="center" rowspan="1" colspan="1">51.9</td></tr><tr><td align="center" rowspan="1" colspan="1">Joint</td><td align="center" rowspan="1" colspan="1">85.0</td><td align="center" rowspan="1" colspan="1">
<bold>82.4</bold>
</td><td align="center" rowspan="1" colspan="1">83.7</td><td align="center" rowspan="1" colspan="1">65.9</td><td align="center" rowspan="1" colspan="1">
<bold>45.3</bold>
</td><td align="center" rowspan="1" colspan="1">53.7</td></tr></tbody></table></table-wrap></floats-group></article>