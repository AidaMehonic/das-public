<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">ScientificWorldJournal</journal-id><journal-id journal-id-type="iso-abbrev">ScientificWorldJournal</journal-id><journal-id journal-id-type="publisher-id">TSWJ</journal-id><journal-title-group><journal-title>The Scientific World Journal</journal-title></journal-title-group><issn pub-type="ppub">2356-6140</issn><issn pub-type="epub">1537-744X</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26221625</article-id><article-id pub-id-type="pmc">4491731</article-id><article-id pub-id-type="doi">10.1155/2015/574589</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A Hybrid Swarm Intelligence Algorithm for Intrusion Detection Using Significant Features</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Amudha</surname><given-names>P.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Karthik</surname><given-names>S.</given-names></name><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Sivakumari</surname><given-names>S.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="I1">
<sup>1</sup>Department of CSE, Avinashilingam Institute for Home Science and Higher Education for Women, Coimbatore 641 108, India</aff><aff id="I2">
<sup>2</sup>Department of CSE, SNS College of Technology, Coimbatore 641 035, India</aff><author-notes><corresp id="cor1">*P. Amudha: <email>amudharul@gmail.com</email></corresp><fn fn-type="other"><p>Academic Editor: Giuseppe A. Trunfio</p></fn></author-notes><pub-date pub-type="ppub"><year>2015</year></pub-date><pub-date pub-type="epub"><day>22</day><month>6</month><year>2015</year></pub-date><volume>2015</volume><elocation-id>574589</elocation-id><history><date date-type="received"><day>20</day><month>1</month><year>2015</year></date><date date-type="rev-recd"><day>19</day><month>5</month><year>2015</year></date><date date-type="accepted"><day>31</day><month>5</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2015 P. Amudha et al.</copyright-statement><copyright-year>2015</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/3.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>Intrusion detection has become a main part of network security due to the huge number of attacks which affects the computers. This is due to the extensive growth of internet connectivity and accessibility to information systems worldwide. To deal with this problem, in this paper a hybrid algorithm is proposed to integrate Modified Artificial Bee Colony (MABC) with Enhanced Particle Swarm Optimization (EPSO) to predict the intrusion detection problem. The algorithms are combined together to find out better optimization results and the classification accuracies are obtained by 10-fold cross-validation method. The purpose of this paper is to select the most relevant features that can represent the pattern of the network traffic and test its effect on the success of the proposed hybrid classification algorithm. To investigate the performance of the proposed method, intrusion detection KDDCup'99 benchmark dataset from the UCI Machine Learning repository is used. The performance of the proposed method is compared with the other machine learning algorithms and found to be significantly different.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Due to the tremendous growth in the field of information technology, one of the significant challenging issues is network security. Hence, intrusion detection system (IDS) which is an indispensable component of the network needs to be secured. The traditional IDS is unable to handle newly arising attacks. The main goal of IDSs is to identify and distinguish the normal and abnormal network connections in an accurate and quick manner which is considered as one of the main issues in intrusion detection system because of the large amount of attributes or features. To study about this aspect, data mining based network intrusion detection is widely used to identify how and where the intrusions occur. Related to achieving real-time intrusion detection, researchers have investigated several methods of performing feature selection. Reducing the number of features by selecting the important features is critical to improve the accuracy and speed of classification algorithms. Hence, selecting the differentiating features and developing the best classifier model in terms of high accuracy and detection rates are the main focus of this work.</p><p>The research on machine learning or data mining considers the intrusion detection as a classification problem, implementing algorithms such as Na&#x000ef;ve Bayes, genetic algorithm, neural networks, Support Vector Machine, decision trees. In order to improve the accuracy of an individual classifier, popular approach is to combine the classifiers. Recently, application of swarm intelligence technique for intrusion detection has gained prominence among the research community [<xref rid="B1" ref-type="bibr">1</xref>]. Swarm intelligence can be a measure presenting the communal behaviour of social insect colonies or other animal societies to implement algorithms [<xref rid="B2" ref-type="bibr">2</xref>]. The potential of swarm intelligence makes it a perfect candidate for IDS, which needs to distinguish normal and abnormal behaviors from large amount of data.</p><p>The main objective of this work is (1) to select important features using two feature selection methods, namely, single feature selection method and random feature selection method and (2) to propose a hybrid optimization algorithm based on Artificial Bee Colony (ABC) and Particle Swarm Optimization (PSO) algorithms for classifying intrusion detection dataset. The studies on ABC and PSO indicate that ABC has powerful global search ability but poor local search ability [<xref rid="B3" ref-type="bibr">3</xref>], while the PSO has powerful local search ability but poor global search ability [<xref rid="B4" ref-type="bibr">4</xref>]. In order to provide a powerful global search capability and local search capability, in this paper a hybridized model called MABC-EPSO is proposed which brings the two algorithms together so that the computation process may benefit from both advantages. In this hybrid algorithm, the local search and global search abilities are balanced to obtain more quality results. KDDCUP'99 intrusion detection dataset developed by MIT Lincoln Laboratory is used for experiments to find the accuracy of the proposed hybrid approach.</p><p>The rest of this paper is organized as follows. <xref ref-type="sec" rid="sec2"> Section 2</xref> provides an overview of related work. <xref ref-type="sec" rid="sec3"> Section 3</xref> presents the principles of PSO and ABC. <xref ref-type="sec" rid="sec4"> Section 4</xref> describes the methodology, dataset description and preprocessing, proposed feature selection methods, and hybrid approach. <xref ref-type="sec" rid="sec5"> Section 5</xref> gives performance metrics, experimental results, and discussions. Finally, conclusion is given in <xref ref-type="sec" rid="sec6">Section 6</xref>.</p></sec><sec id="sec2"><title>2. Related Work</title><p>Being related to achieving real-time intrusion detection, researchers have investigated several methods of performing feature selection. Kohavi and John [<xref rid="B4" ref-type="bibr">4</xref>] described the feature subset selection problem in supervised learning, which involves identifying the relevant or useful features in a dataset and giving only that subset to the learning algorithm. The real-life intrusion detection dataset contains redundant features or insignificant features. The redundant features make it harder to detect possible intrusion patterns [<xref rid="B5" ref-type="bibr">5</xref>]. With the increasing applications of classification algorithms and feature selection methods for intrusion detection dataset, a comprehensive list of a few such literatures is given in [<xref rid="B6" ref-type="bibr">6</xref>&#x02013;<xref rid="B23" ref-type="bibr">23</xref>].</p><p>Machine learning algorithms such as neural networks [<xref rid="B9" ref-type="bibr">9</xref>], fuzzy clustering [<xref rid="B14" ref-type="bibr">14</xref>] have been applied to IDS to construct good detection model. Support vector machine (SVM) [<xref rid="B24" ref-type="bibr">24</xref>] has become a popular research method in intrusion detection due to its good generalization performance and the sparse representation of solution. Satpute et al. [<xref rid="B25" ref-type="bibr">25</xref>] enhanced the performance of intrusion detection system by combining PSO and its variants with machine learning techniques for the detection of anomaly in network intrusion detection system. Chung and Wahid [<xref rid="B26" ref-type="bibr">26</xref>] proposed a novel simplified swarm optimization (SSO) algorithm as a rule based classifier and for feature selection for classifying audio data. The algorithm is more flexible and cost-effective to solve complex computing environments. Revathi and Malathi [<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B11" ref-type="bibr">11</xref>] proposed hybrid simplified swarm optimization to preprocess the data and compared the proposed approach with a new hybridized approach, PSO with Random Forest, and found that the proposed method provided high detection rate and optimal solution.</p><p>Karaboga and Basturk [<xref rid="B27" ref-type="bibr">27</xref>] proposed Artificial Bee Colony (ABC) algorithm based on a particular intelligent behaviour of honeybee swarms. By understanding the basic behaviour characteristics of foragers, ABC algorithm was developed and was compared with that of differential evolution, Particle Swarm Optimization, and evolutionary algorithm for multidimensional and multimodal numeric problems. Karaboga and Akay [<xref rid="B28" ref-type="bibr">28</xref>] proposed ABC algorithm for anomaly-based network intrusion detection system to optimize the solution. The proposed method was classified into four stages such as parameterization, training stage, testing stage, and detection stage. D. D. Kumar and B. Kumar [<xref rid="B29" ref-type="bibr">29</xref>] applied ABC algorithm for anomaly-based IDS and used feature selection techniques to reduce the number of features used for detection and classification. Mustafa ServetKiran and MesutGunduz [<xref rid="B30" ref-type="bibr">30</xref>] proposed a hybridization of PSO and ABC for different continuous optimization problems in which the information between particle swarm and bee colony helps in increasing global and local search abilities of the hybrid approach.</p></sec><sec id="sec3"><title>3. Theoretical Background</title><p>The following subsections provide the necessary background to understand the problem.</p><sec id="sec3.1"><title>3.1. Particle Swarm Optimization</title><p>Particle Swarm Optimization (PSO) is one of the popular heuristic technique which has been successfully applied in many different application areas, but, however, it suffers from premature convergence especially in high dimension, multimodal problems.</p><p>The algorithm of the standard PSO is as follows.<list list-type="order"><list-item><p>Initialize a population of particles with randomly chosen positions and velocities.</p></list-item><list-item><p>Calculate the fitness value of each particle in the population.</p></list-item><list-item><p>If the fitness value of the particle <italic>i</italic> is better than its<italic> pbest</italic> value, then set the fitness value as a new<italic> pbest</italic> of particle <italic>i</italic>.</p></list-item><list-item><p>If<italic> pbest</italic> is updated and it is better than the current<italic> gbest</italic>, then set<italic> gbest</italic> to the current<italic> pbest</italic> value of particle <italic>i</italic>.</p></list-item><list-item><p>Update particle's velocity and position according to (<xref ref-type="disp-formula" rid="EEq1">1</xref>) and (<xref ref-type="disp-formula" rid="EEq2">2</xref>).</p></list-item><list-item><p>If the best fitness value or the maximum generation is met, then stop the process; otherwise, repeat the process from step&#x02009;&#x02009;2.</p></list-item></list>In PSO, a swarm consists of <italic>N</italic> particles in a<italic> D</italic>-dimensional searching space. The <italic>i</italic>th particle is represented as <italic>X</italic>
<sub><italic>i</italic></sub> = (<italic>x</italic>
<sub><italic>i</italic>1</sub>, <italic>x</italic>
<sub><italic>i</italic>2</sub>,&#x02026;, <italic>x</italic>
<sub><italic>id</italic></sub>). The best previous position<italic> pbest</italic> of any particle is <italic>P</italic>
<sub><italic>i</italic></sub> = (<italic>p</italic>
<sub><italic>i</italic>1</sub>, <italic>p</italic>
<sub><italic>i</italic>2</sub>,&#x02026;, <italic>p</italic>
<sub><italic>id</italic></sub>) and the velocity for particle <italic>i</italic> is <italic>V</italic>
<sub><italic>i</italic></sub> = (<italic>v</italic>
<sub><italic>i</italic>1</sub>, <italic>v</italic>
<sub><italic>i</italic>2</sub>,&#x02026;, <italic>v</italic>
<sub><italic>id</italic></sub>). The global best particle in the whole swarm is denoted by <italic>P</italic>
<sub><italic>g</italic></sub> and it represents the fittest particle [<xref rid="B31" ref-type="bibr">31</xref>]. During each iteration, each particle updates its velocity according to the following equation:<disp-formula id="EEq1"><label>(1)</label><mml:math id="M1"><mml:mtable style="T2"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:malignmark/><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="2.984pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>c</italic>
<sub>1</sub> and <italic>c</italic>
<sub>2</sub> denote the acceleration coefficients, <italic>d</italic> = 1,2,&#x02026;, <italic>D</italic>, and rand<sub>1</sub> and rand<sub>2</sub> are random numbers uniformly distributed within [0,1].</p><p>Each particle then moves to a new potential position as in the following equation:<disp-formula id="EEq2"><label>(2)</label><mml:math id="M2"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p></sec><sec id="sec3.2"><title>3.2. Artificial Bee Colony</title><p>The Artificial Bee Colony (ABC) algorithm is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Karaboga and Basturk [<xref rid="B27" ref-type="bibr">27</xref>]. The Artificial Bee Colony comprises of three groups: scout bees, onlooker bees, and employed bees. The bee, which carries out random search, is known as scout bee. The bee which visits the food source is an employed bee. The bee, which waits on the dance region is an onlooker bee and the onlooker bee with scout is also called unemployed bee. The employed and unemployed bees search for the good food sources around the hive. The employed bees share the stored food source information with onlooker bees. The amount of food sources is equal to the amount of employed bees and also is equal to the number of onlooker bees. The solutions of the employed bees which cannot be enhanced by a fixed number of bounds become scouts and their solutions are abandoned [<xref rid="B28" ref-type="bibr">28</xref>]. In the context of optimization, the amount of food sources in ABC algorithm represents the number of solutions in the population. The point of a good food source indicates the location of a promising solution to the optimization problem [<xref rid="B27" ref-type="bibr">27</xref>].</p><p>The four main phases of ABC algorithm are as follows.</p><p>
<italic>Initialization Phase</italic>. The scout bees randomly generate the population size (SN) of food sources. The input vector <italic>x</italic>
<sub><italic>m</italic></sub> which contains <italic>D</italic> variables represents food source where <italic>D</italic> represents the searching space dimension of the objective function to be optimized. Using (<xref ref-type="disp-formula" rid="EEq3">3</xref>), initial sources of food are produced randomly<disp-formula id="EEq3"><label>(3)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.39pt" depth="1.16998pt"/><mml:mn mathvariant="normal">0,1</mml:mn><mml:mspace height="6.39pt" depth="1.16998pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi>&#x02217;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>u</italic>
<sub><italic>i</italic></sub> and <italic>l</italic>
<sub><italic>i</italic></sub> are the upper and lower bounds of the solution space of objective function and rand(0,1) is a random number within the range [0, 1].</p><p>
<italic>Employed Bee Phase</italic>. The employed bee finds a new food source within the region of the food source. The employed bees reminisce higher quantity of food source and share it with onlooker bees. Equation (<xref ref-type="disp-formula" rid="EEq4">4</xref>) determines the neighbour food source <italic>v</italic>
<sub><italic>mi</italic></sub> and is calculated by<disp-formula id="EEq4"><label>(4)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="5.32999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>i</italic> is a randomly selected parameter index, <italic>x</italic>
<sub><italic>k</italic></sub> is a randomly selected food source, and <italic>&#x003d5;</italic>
<sub><italic>mi</italic></sub> is a random number within the range [&#x02212;1,1]. Suitable tuning on specific problems can be made using this parameter range. The fitness of food sources, which is needed to find the global, optimal solution, is calculated by (<xref ref-type="disp-formula" rid="EEq5">5</xref>). And a greedy selection method is used between <italic>x</italic>
<sub><italic>m</italic></sub> and <italic>v</italic>
<sub><italic>m</italic></sub>
<disp-formula id="EEq5"><label>(5)</label><mml:math id="M5"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn mathvariant="normal">1</mml:mn><mml:mo>+</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mn mathvariant="normal">0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>f</italic>
<sub><italic>i</italic></sub> represents the objective value of <italic>i</italic>th solution.</p><p>
<italic>Onlooker Bee Phase</italic>. Onlooker bees examine the effectiveness of food sources by observing the waggle dance in the dance region and then randomly select a rich food source. Then, the bees perform a random search in the neighbourhood area of food source using (<xref ref-type="disp-formula" rid="EEq4">4</xref>). The quantity of a food source is evaluated by its profitability <italic>P</italic>
<sub><italic>i</italic></sub> using the following equation:<disp-formula id="EEq6"><label>(6)</label><mml:math id="M6"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>fit</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mtext>fit</mml:mtext></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where fit<sub><italic>i</italic></sub> denotes the fitness of the solution represented by food source <italic>i</italic> and SN denotes the total number of food sources which is equal to number of employed bees.</p><p>
<italic>Scout Phase</italic>. If the effectiveness of food source cannot be improved by the fixed number of trials, then the scout bees remove the solutions and randomly search for new solutions by using (<xref ref-type="disp-formula" rid="EEq3">3</xref>) [<xref rid="B29" ref-type="bibr">29</xref>].</p><p>The pseudocode of the ABC algorithm is given in <xref ref-type="fig" rid="alg1">Algorithm 1</xref>.</p></sec></sec><sec id="sec4"><title>4. Methodology</title><sec id="sec4.1"><title>4.1. Research Framework</title><p>In this study, the framework of the proposed work is given as follows.<list list-type="roman-lower"><list-item><p>Data preprocessing: it prepared the data for classification and removed unused features and duplicate instances.</p></list-item><list-item><p>Feature selection: it determined the feature subset using SFSM and RFSM methods that contribute to the classification.</p></list-item><list-item><p>Hybrid classification: it performed classification using MABC-EPSO algorithm to enhance the classification accuracy for the KDDCUP'99 dataset.</p></list-item></list>The objective of this study is to help the network administrator in preprocessing the network data using feature selection methods and to perform classification using hybrid algorithm which aims to fit a classifier model to the prescribed data.</p></sec><sec id="sec4.2"><title>4.2. Data Source and Dataset Description</title><p>In this section, we provide brief description of KDDCup'99 dataset [<xref rid="B30" ref-type="bibr">30</xref>] which is derived from UCI Machine Learning Repository [<xref rid="B31" ref-type="bibr">31</xref>]. In 1998, DARPA intrusion detection evaluation program, to perform a comparison of various intrusion detection methods, a simulated environment, was set up by the MIT Lincoln Lab to obtain raw TCP/IP dump data for a local-area network (LAN). The functioning of the environment was like a real one, which included both background network traffic and wide variety of attacks. A version of 1998 DARPA dataset, KDDCup'99, is now widely accepted as a standard benchmark dataset and received much attention in the research community of intrusion detection. The main motivation of using KDDCup'99 Dataset is to show that the proposed method has the advantage of becoming an efficient classification algorithm when applied to the intrusion detection system. In this paper, 10% KDD Cup'99 dataset is used for experimentation. The distribution of connection types and sample size in 10% KDDCUP dataset is shown in Tables <xref ref-type="table" rid="tab1">1</xref> and <xref ref-type="table" rid="tab2">2</xref>. The feature information of 10% KDDCUP dataset is shown in <xref ref-type="table" rid="tab3">Table 3</xref>. The dataset consists of one type of normal data and 22 different attack types categorized into 4 classes, namely, denial of service (DoS), Probe, user-to-root (U2R), and remote-to-login (R2L).</p></sec><sec id="sec4.3"><title>4.3. Data Preprocessing</title><p>Data preprocessing is the time-consuming task which prepares the data for subsequent analysis as per the requirement for intrusion detection system model. The main aim of data preprocessing is to transform the raw network data into suitable form for further analysis. <xref ref-type="fig" rid="fig1"> Figure 1</xref> illustrates the steps involved in data processing and how raw input data are processed for further statistical measures.</p><p>Various statistical analyses such as feature selection, dimensionality reduction, and normalization are essential to select significant features from the dataset. If the dataset contains duplicate instances, then the classification algorithms consume more time and also provide inefficient results. To achieve more accurate and efficient model, duplication elimination is needed. The main deficiency in this dataset is the large number of redundant instances. This large amount of duplicate instances will make learning algorithms be partial towards the frequently occurring instances and will inhibit it from learning infrequent instances which are generally more unsafe to networks. Also, the existence of these duplicate instances will cause the evaluation results to be biased by the methods which have better detection rates on the frequently occurring instances [<xref rid="B32" ref-type="bibr">32</xref>]. Eliminating duplicate instances helps in reducing false-positive rate for intrusion detection. Hence, duplicate instances are removed, so the classifiers will not be partial towards more frequently occurring instances. The details of instances in the dataset are shown in <xref ref-type="table" rid="tab4">Table 4</xref>. After preprocessing, selected random sample of 10% normal data and 10% Neptune attack in DoS class and four new sets of data are generated with the normal class and four categories of attack [<xref rid="B33" ref-type="bibr">33</xref>]. Moreover, irrelevant and redundant attributes of intrusion detection dataset may lead to complex intrusion detection model and reduce detection accuracy.</p></sec><sec id="sec4.4"><title>4.4. Feature Selection</title><p>Feature selection is an important data processing process. As the dataset is large, it is essential to remove the insignificant features, in order to distinguish normal traffic or intrusions in a well-timed manner. In this paper, feature subsets are formed based on single feature method (SFSM), random feature selection method (RFSM) and compared the two techniques. The proposed methods reduce the features in the datasets which aim to improve accuracy rate, reduce processing time, and improve efficiency for intrusion detection.</p><sec id="sec4.4.1"><title>4.4.1. Single Feature Selection Method</title><p>Single feature method (SFSM) uses the one-dimensional feature vector. In the first iteration, it considers only the first attribute and is evaluated for calculating the accuracy using the Support Vector Machine classifier. In the second iteration, it considers only the corresponding attribute for evaluation. The process is repeated until all 41 features are evaluated. After calculating the entire feature's efficiency, it is sorted and vital features are selected, whose accuracy and detection rate are acc_threshold and dr_threshold values, respectively. The pseudocode of single feature selection algorithm is given in <xref ref-type="fig" rid="alg2">Algorithm 2</xref>.</p></sec><sec id="sec4.4.2"><title>4.4.2. Random Feature Selection Method</title><p>In this method, the features are removed randomly and evaluated using the classifier. In the first iteration, all the features are evaluated using SVM classifier, and then by deleting one feature, update the dataset, using the classifier efficiency. The importance of the provided feature is calculated. In the second iteration, another feature is removed randomly from the dataset and updated. The process is repeated until only one feature is left. After calculating the entire feature's efficiency, it is sorted in descending order of its accuracy. If the accuracy and detection rate are greater than the threshold value (accuracy and detection rate obtained using all features), then select those features as vital features. The pseudocode of the random feature selection algorithm is given in <xref ref-type="fig" rid="alg3">Algorithm 3</xref>.</p><p>Tables <xref ref-type="table" rid="tab5">5</xref> and <xref ref-type="table" rid="tab6">6</xref> show the feature subsets identified using the two feature selection methods and size of the subsets identified as a percentage of the full feature set.</p></sec></sec><sec id="sec4.5"><title>4.5. Hybrid Classification Approach</title><p>Artificial intelligence and machine learning techniques were used to build different IDSs, but they have shown limitations in achieving high detection accuracy and fast processing time. Computational intelligence techniques, known for their ability to adapt and to exhibit fault tolerance, high computational speed, and resilience against noisy information, compensate for the limitations of these approaches [<xref rid="B1" ref-type="bibr">1</xref>]. Our aim is to increase the level of performance of intrusion detection of the most used classification techniques nowadays by using optimization methods like PSO and ABC. This work develops an algorithm that combines the logic of both ABC and PSO to produce a high performance IDS and their combination has the advantage of providing a more reliable solution to today's data intensive computing processes.</p><p>Artificial Bee Colony algorithm is a newly proposed optimization algorithm and is becoming a hot topic in computational intelligence nowadays. Because its high probability of avoiding the local optima, it can make up the disadvantage of Particle Swarm Optimization algorithm. Moreover, Particle Swarm Optimization Algorithm can help us to find out the optimal solution more easily. In such circumstances, we bring the two algorithms together so that the computation process may benefit from both of the advantages. The flowchart of the proposed hybrid MABC-EPSO is given in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p>In this hybrid model, the colony is divided into two parts: one possesses the swarm intelligence of Artificial Bee Colony and the other one is the particle swarm intelligence. Assuming that there is cooperation between the two parts, in each iteration, one part which finds out the better solution will share its achievement with the other part. The inferior solution will be replaced by the better solution and will be substituted in the next iteration. The process of MABC-EPSO is as follows.</p><p>
<statement id="step1"><title>Step 1 (initialization of parameters). </title><p>Set the number of individuals of the swarm; set the maximum circle index of the algorithm; set the search range of the solution; set the other constants needed in both ABC and PSO.</p></statement>
</p><p>
<statement id="step2"><title>Step 2 (initialization of the colony). </title><p>Generate a colony with a specific number of individuals. Bee colony is divided into two categories, employed foragers and unemployed foragers, according to each individual's fitness value; on the other hand, as a particle swarm, calculate the fitness value of each particle and take the best location as the global best location.</p></statement>
</p><p>
<statement id="step3"><title>Step 3 . </title><p>In bee colony, to evaluate the fitness value of each solution, an employee bee is assigned using (<xref ref-type="disp-formula" rid="EEq5">5</xref>). The employee bee selects a new candidate solution from the nearby food sources and then uses greedy selection method by calculating the Rastrigin function as follows: <disp-formula id="EEq7"><label>(7)</label><mml:math id="M7"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Min</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="4.53pt" depth="0.12pt"/><mml:mi>x</mml:mi><mml:mspace height="4.53pt" depth="0.12pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="9.24498pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">10</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="6.35999pt" depth="2.484pt"/><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="6.35999pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mspace height="9.24498pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>A multimodal function is said to contain more than one local optimum. A function of variables is separable if it can be modified as a sum of functions of just one variable [<xref rid="B34" ref-type="bibr">34</xref>]. The dimensionality of the search space is another significant factor in the complexity of the problem. The challenge involved in finding optimal solutions to this function is that, on the way towards the global optimum, an optimization problem can be easily confined in a local optimum. Hence, the classical benchmark function Rastrigin [<xref rid="B34" ref-type="bibr">34</xref>] is implemented using Artificial Bee Colony algorithm and named as Modified Artificial Bee Colony (MABC) algorithm. In (<xref ref-type="disp-formula" rid="EEq1">1</xref>) <italic>f</italic>
<sub><italic>i</italic></sub> is Rastrigin function whose value is 0 at its global minimum (0,0,&#x02026;, 0). This function is chosen, because it is considered to be one of the best test functions for finding the global minimum. Initialization range for the function is [&#x02212;15, 15]. This function is with cosine modulation to produce many local minima. Thus, the function is multimodal.</p></statement>
</p><p>
<statement id="step4"><title>Step 4 . </title><p>If the fitness value is larger than the earlier one, the bee remembers the new point and forgets the previous one; otherwise, it keeps the previous solution. Based on the shared information by employee bees, an onlooker bee calculates the shared fitness value and selects a food source with a probability value computed as in (<xref ref-type="disp-formula" rid="EEq6">6</xref>).</p></statement>
</p><p>
<statement id="step5"><title>Step 5 . </title><p>An onlooker bee constructs a new solution selected among the neighbors of a previous solution. It also checks the fitness value and if this value is better than the previous one, it will substitute the old one with the new position; otherwise, it evokes the old position. The objective of scout bees is to determine new random food sources to substitute the solutions that cannot be enhanced after reaching the &#x0201c;limit&#x0201d; value. In order to obtain the best optimized solution, the algorithm goes through a predefined number of cycles (MCN). After all the choices have been made, the best solution generated in that iteration is called MABC<sub>best</sub>.</p></statement>
</p><p>
<statement id="step6"><title>Step 6 . </title><p>As there is a large effect of initial velocity in the balancing of exploration and exploitation process of swarm, in this proposed Enhanced Particle Swarm Optimization (EPSO) algorithm, inertia weight (<italic>&#x003c9;</italic>) [<xref rid="B35" ref-type="bibr">35</xref>] is used to control the velocity and hence the velocity update equation (<xref ref-type="disp-formula" rid="EEq8">8</xref>) becomes as follows:<disp-formula id="EEq8"><label>(8)</label><mml:math id="M8"><mml:mtable style="T2"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="2.984pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="2.984pt"/></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>A small inertia weight facilitates a local search, whereas a large inertia weight facilitates a global search. In the EPSO algorithm, linear decreasing inertia weight [<xref rid="B36" ref-type="bibr">36</xref>] as in (<xref ref-type="disp-formula" rid="EEq9">9</xref>) is used to enhance the efficiency and performance of PSO. It is found experimentally that inertia weight from 0.9 to 0.4 provides the optimal results<disp-formula id="EEq9"><label>(9)</label><mml:math id="M9"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>iter</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>In particle swarm, after the comparison among the solutions that each particle has experienced and the comparison among the solutions that all the particles have ever experienced, the best location in that iteration is called EPSO<sub>best</sub>.</p></statement>
</p><p>
<statement id="step7"><title>Step 7 . </title><p>The minimum of the value MABC<sub>best</sub> and EPSO<sub>best</sub> is called Best and is defined as<disp-formula id="EEq10"><label>(10)</label><mml:math id="M10"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>Best</mml:mtext><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mtext>EPSO</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if&#x02009;&#x02009;</mml:mtext><mml:msub><mml:mrow><mml:mtext>EPSO</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mtext>MABC</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mtext>MABC</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if&#x02009;&#x02009;</mml:mtext><mml:msub><mml:mrow><mml:mtext>MABC</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mtext>EPSO</mml:mtext></mml:mrow><mml:mrow><mml:mtext>best</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p></statement>
</p><p>
<statement id="step8"><title>Step 8 . </title><p>If the termination condition is satisfied, then end the process and report the best solution. Otherwise, return to <xref ref-type="statement" rid="step2">Step 2</xref>.</p><p>
<italic>Parameter Settings</italic>. The algorithms are evaluated using the two feature sets selected by SFSM and RFSM. In ABC algorithm, the parameters set are bee colony size: 40, MCN: 500, and limit: 5. In EPSO algorithm, the inertia weight <italic>&#x003c9;</italic> in (<xref ref-type="disp-formula" rid="EEq11">11</xref>) varies from 0.9 to 0.7 linearly with the iterations. Also, the acceleration coefficients <italic>c</italic>
<sub>1</sub> and <italic>c</italic>
<sub>2</sub> are set as 2. The upper and lower bounds for <italic>v</italic>(<italic>v</italic>
<sub>min</sub>, <italic>v</italic>
<sub>max</sub>) are set as the maximum upper and lower bounds of <italic>x</italic>
<disp-formula id="EEq11"><label>(11)</label><mml:math id="M11"><mml:mtable style="T2"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="6.39pt" depth="1.16998pt"/><mml:mn mathvariant="normal">0,1</mml:mn><mml:mspace height="6.39pt" depth="1.16998pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="2.984pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="6.39pt" depth="1.16998pt"/><mml:mn mathvariant="normal">0,1</mml:mn><mml:mspace height="6.39pt" depth="1.16998pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.19598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.19598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p></statement>
</p></sec></sec><sec id="sec5"><title>5. Experimental Work</title><p>This section provides the performance metrics that are used to assess the efficiency of the proposed approach. It also presents and analyzes the experimental results of hybrid approach and compares it with the other classifiers.</p><sec id="sec5.1"><title>5.1. Performance Metrics</title><p>The performance metrics like accuracy, sensitivity, specificity, false alarm rate, and training time are recorded for the intrusion detection dataset on applying the proposed MABC-PSO classification algorithm. Generally, sensitivity and specificity are the statistical measures used to carry out the performance of classification algorithms. Hence, sensitivity and specificity are chosen to be the parametric indices for carrying out the classification task. In intrusion detection problem, sensitivity can also be called detection rate. The number of instances predicted correctly or incorrectly by a classification model is summarized in a confusion matrix and is shown in <xref ref-type="table" rid="tab7">Table 7</xref>.</p><p>The classification accuracy is the percentage of the overall number of connections correctly classified<disp-formula id="EEq12"><label>(12)</label><mml:math id="M12"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>Classification&#x02009;&#x02009;accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Sensitivity (True Positive Fraction) is the percentage of the number of attack connections correctly classified in the testing dataset <disp-formula id="EEq13"><label>(13)</label><mml:math id="M13"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Specificity (True Negative Fraction) is the percentage of the number of normal connections correctly classified in the testing dataset <disp-formula id="EEq14"><label>(14)</label><mml:math id="M14"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>Specificity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>False alarm rate (FAR) is the percentage of the number of normal connections incorrectly classified in the testing and training dataset<disp-formula id="EEq15"><label>(15)</label><mml:math id="M15"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>False&#x02009;&#x02009;Alarm&#x02009;&#x02009;Rate (FAR)</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. It is the standard way of measuring the accuracy of a learning scheme and it is used to estimate how accurately a predictive model will perform in practice. In this work, 10-fold cross-validation method is used for improving the classifier reliability. In 10-fold cross-validation, the original data is divided randomly into 10 parts. During each run, one of the partitions is chosen for testing, while the remaining nine-tenths are used for training. This process is repeated 10 times so that each partition is used for training exactly once. The average of the results from the 10-fold gives the test accuracy of the algorithm [<xref rid="B37" ref-type="bibr">37</xref>].</p></sec><sec id="sec5.2"><title>5.2. Results and Discussions</title><p>The main motivation is to show that the proposed hybrid method has the advantage of becoming an efficient classification algorithm based on ABC and PSO. To further prove the robustness of the proposed method, other popular machine learning algorithms [<xref rid="B38" ref-type="bibr">38</xref>] such as Naives Bayes (NB) which is a statistical classifier; decision tree (j4.8); radial basis function (RBF) network; Support Vector Machine (SVM) that is based on the statistical learning theory and basic ABC are tested on KDDCup'99 dataset. For each classification algorithm, their default control parameters are used. In <xref ref-type="table" rid="tab8">Table 8</xref>, the results are reported for accuracy rate obtained by various classification algorithms using different feature selection methods.</p><p>The performance comparison of the classifiers on accuracy rate is given in Figures <xref ref-type="fig" rid="fig3">3</xref>&#x02013;<xref ref-type="fig" rid="fig4"/>
<xref ref-type="fig" rid="fig5"/>
<xref ref-type="fig" rid="fig6">6</xref>. The results show that, on classifiying the dataset with all features, the average accuracy rate of 85.5%, 84.5%, and 88.59% is obtained for SVM, ABC, and proposed hybrid approaches. When SFSM is applied, accuracy rate of ABC and proposed MABC-EPSO is increased significantly to 94.36% and 99.32%. The highest accuracy (99.82%) is reported when the proposed MABC-EPSO with random feature selection method is employed. It is also observed that on applying random feature selection method, the accuracy of SVM and ABC is increased to 95.71% and 97.92%. The accuracy rate of NB, j4.8, and RBF classifiers is comparatively high with RFSM method compared to SFSM and full feature set.</p><p>In order to test the significance of the differences among classifiers, six classification algorithms previously mentioned over four datasets are considered and performed experiments using Friedman test and ANOVA. Tables <xref ref-type="table" rid="tab9">9</xref> and <xref ref-type="table" rid="tab10">10</xref> depict the classification accuracy using two feature selection methods and their ranks computed through Friedman test (ranking is given in parenthesis). The null hypothesis states that all the classifiers perform in the same way and hence their ranks should be equal. The Friedman test ranked the algorithms for each dataset, with the best performing algorithm getting the rank of 1, the second best algorithm getting the rank 2. As seen in <xref ref-type="table" rid="tab9">Table 9</xref>, MABC-EPSO is the best performing algorithm, whereas Na&#x000ef;ve Bayes is the least performing algorithm and <xref ref-type="table" rid="tab10">Table 10</xref> shows that MABC-EPSO is the best performing algorithm, whereas Na&#x000ef;ve Bayes and j4.8 are the least performing algorithms. Friedman statistic <italic>&#x003c7;</italic>
<sup>2</sup> = 15.716 and <italic>F</italic>
<sub><italic>F</italic></sub> = 11.005 for SFSM and <italic>&#x003c7;</italic>
<sup>2</sup> = 15.712 and <italic>F</italic>
<sub><italic>F</italic></sub> = 10.992 for RFSM are computed. Having four datasets and six classification algorithms, distribution of <italic>F</italic>
<sub><italic>F</italic></sub> is based on <italic>F</italic> distribution with 6 &#x02212; 1 = 5 and (6 &#x02212; 1)<italic>&#x02217;</italic>(4 &#x02212; 1) = 15 degrees of freedom. The critical value of <italic>F</italic>(5,15) for <italic>&#x003b1;</italic> = 0.05 is 2.9013 and <italic>P</italic> value &#x0003c; 0.05. So, we reject the null hypothesis, and the differences among classifiers are significant.</p><p>The means of several groups by estimating the variances among groups and within a group are compared using the ANOVA test. Here, the null hypothesis which is set as all population means are equal is tested. Also <italic>P</italic> value and the value of <italic>F</italic> are computed. If the null hypothesis is rejected, Tukey's post hoc analysis method is applied to perform a multiple comparison which tests all means pairwise, to determine which ones are significantly different. <xref ref-type="table" rid="tab11"> Table 11</xref> shows the results determined by ANOVA. In SFSM method, the ANOVA test rejected the null hypothesis, as calculated <italic>F</italic>(5,18) = 31.895 is greater than<italic> F</italic>-critical (2.773) for the significance level of 5%. Tukey's post hoc test is performed which states that significantly there are differences among MABC-EPSO and ABC with other classifiers but not among NB, j4.8, RBF, and SVM. Also, there are significant differences between ABC and MABC-EPSO; so ABC and MABC-EPSO are the best classifiers in this case. In RFSM method, there were statistically significant differences between algorithms and hence null hypothesis was rejected, as the calculated <italic>F</italic>(5,18) = 48.547 is greater than<italic> F</italic>-critical (2.773) for the significance level of 5%. Tukey's posthoc test is performed and it reveals that there is a statistically significant difference among SVM, ABC, and MABC-EPSO with other classifiers but not among NB, j4.8, and RBF. However, there is no statistically significant difference between the ABC and MABC-EPSO algorithms.</p><p>In <xref ref-type="table" rid="tab12">Table 12</xref>, the results are reported for detection rate obtained by various classification algorithms using different feature selection methods. The comparison results of sensitivity and specificity obtained by proposed method using the two feature selection methods are given in Figures <xref ref-type="fig" rid="fig7">7</xref>
<xref ref-type="fig" rid="fig8"/>
<xref ref-type="fig" rid="fig9"/>&#x02013;<xref ref-type="fig" rid="fig10">10</xref>. The results show that on classifying the dataset with all features, detection rate of 87.5%, 83.64%, and 87.16% is obtained for SVM, ABC, and proposed MABC-EPSO approaches. On applying the single feature selection method, detection rate of SVM, ABC, and proposed MABC-EPSO is increased significantly to 88.97%, 89.90%, and 98.09%, respectively. The highest detection rate (98.67%) is reported when the proposed MABC-EPSO with random feature selection method is employed. MABC-EPSO with SFSM also shows a comparable performance than other classifier combinations. The performance of NB, j4.8, and RBF is better in terms of specificity and sensitivity using RFSM method compared to SFSM method.</p><p>
<xref ref-type="table" rid="tab13">Table 13</xref> shows the ANOVA results of analyzing the performance of the classifiers based on specificity. In both SFSM and RFSM methods, ANOVA test determined that there are significant differences among the classification algorithms and rejected null hypothesis as calculated <italic>F</italic>(5,18 = 52.535) and <italic>F</italic>(5,18 = 23.539) are greater than<italic> F</italic>-critical (2.773). Finally, multiple comaprison test concluded that MABC-EPSO has significant differences with all the classification algorithms with 0.05 (<italic>P</italic> = 0.05) as significance level. However, there is no statistically significant difference between the SVM and ABC algorithms.</p><p>Experiment was conducted to analyze the false alarm rate and training time of each classifier using SFSM and RFSM methods. <xref ref-type="fig" rid="fig11"> Figure 11</xref> indicates that MABC-EPSO produces lowest FAR (ranging from 0.004 to 0.005) using RFSM for all datasets. Also, the proposed hybrid approach using SFSM shows a comparable performance with SVM and ABC classifiers using RFSM method. <xref ref-type="table" rid="tab14"> Table 14</xref> shows that the training time of proposed approach has been significantly reduced for both feature selection methods when compared to other classification algorithms. Training time of the proposed hybrid classifier considering all features is also recorded in <xref ref-type="fig" rid="fig12">Figure 12</xref>. The results indicate that the time taken by proposed approach is considerably more when all features are employed. It is also observed that the time consumed by the proposed classifier using the features of RFSM method is comparatively lesser than SFSM method. According to the performance of MABC-EPSO with random feature selection method, the proposed method can be used to solve intrusion detection as classification problem.</p></sec></sec><sec id="sec6"><title>6. Conclusion</title><p>In this work, a hybrid algorithm based on ABC and PSO was proposed to classify the benchmark intrusion detection dataset using the two feature selection methods, SFSM and RFSM. A study of different machine learning algorithms was also presented. Performance comparisons amongst different classifiers were made to understand the effectiveness of the proposed method in terms of various performance metrics. The main goal of this paper was to show that the classifiers were significantly different and the proposed hybrid method outperforms other classifiers. Friedman test and ANOVA test was applied to check whether the classification algorithms were significantly different. Based on the conclusion of ANOVA test, the null hypotheses were rejected, if they were significant. Post hoc analysis using Tukey's test was applied to select which classification algorithm was significantly different from the others. The experiments also showed that the effectiveness of ABC is comparable to the proposed hybrid algorithm. In general, the proposed hybrid classifier produced best results using the features of both SFSM and RFSM methods and is also significantly different from other classification algorithms. Hence, MABC-EPSO can be considered as a preferable method for intrusion detection that outperforms its counterpart methods. In the future, we will further improve feature selection algorithm and investigate the use of bioinspired approaches as classification algorithm in the area of intrusion detection.</p></sec></body><back><sec sec-type="conflict"><title>Conflict of Interests</title><p>The authors declare that there is no conflict of interests regarding the publication of this paper.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>S. X.</given-names></name><name><surname>Banzhaf</surname><given-names>W.</given-names></name></person-group><article-title>The use of computational intelligence in intrusion detection systems: a review</article-title><source><italic>Applied Soft Computing Journal</italic></source><year>2010</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2009.06.019</pub-id><pub-id pub-id-type="other">2-s2.0-70350134739</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bonabeau</surname><given-names>E.</given-names></name><name><surname>Dorigo</surname><given-names>M.</given-names></name><name><surname>Theraulaz</surname><given-names>G.</given-names></name></person-group><source><italic>Swarm Intelligence: From Natural to Artificial Intelligence</italic></source><year>1999</year><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>G.</given-names></name><name><surname>Kwong</surname><given-names>S.</given-names></name></person-group><article-title>Gbest-guided artificial bee colony algorithm for numerical function optimization</article-title><source><italic>Applied Mathematics and Computation</italic></source><year>2010</year><volume>217</volume><issue>7</issue><fpage>3166</fpage><lpage>3173</lpage><pub-id pub-id-type="doi">10.1016/j.amc.2010.08.049</pub-id><pub-id pub-id-type="other">MR2733759</pub-id><pub-id pub-id-type="other">2-s2.0-78049297395</pub-id></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohavi</surname><given-names>R.</given-names></name><name><surname>John</surname><given-names>G. H.</given-names></name></person-group><article-title>Wrappers for feature subset selection</article-title><source><italic>Artificial Intelligence</italic></source><year>1997</year><volume>97</volume><issue>1-2</issue><fpage>273</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/s0004-3702(97)00043-x</pub-id><pub-id pub-id-type="other">2-s2.0-0031381525</pub-id></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>W.</given-names></name><name><surname>Stolfo</surname><given-names>S. J.</given-names></name></person-group><article-title>A framework for constructing features and models for intrusion detection systems</article-title><source><italic>ACM Transactions on Information and System Security</italic></source><volume>3</volume><issue>4</issue><fpage>227</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1145/382912.382914</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>H.</given-names></name><name><surname>Franke</surname><given-names>K.</given-names></name><name><surname>Petrovi&#x00107;</surname><given-names>S.</given-names></name></person-group><article-title>Improving effectiveness of intrusion detection by correlation feature selection</article-title><conf-name>Proceedings of the 5th International Conference on Availability, Reliability, and Security (ARES '10)</conf-name><conf-date>February 2010</conf-date><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1109/ares.2010.70</pub-id><pub-id pub-id-type="other">2-s2.0-77952390959</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>T.</given-names></name><name><surname>Ren</surname><given-names>R.</given-names></name></person-group><article-title>A real time IDSs based on artificial bee colony-support vector machine algorithm</article-title><conf-name>Proceedings of the 3rd International Workshop on Advanced Computational Intelligence (IWACI '10)</conf-name><conf-date>August 2010</conf-date><conf-loc>Suzhou, China</conf-loc><publisher-name>IEEE</publisher-name><fpage>91</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1109/iwaci.2010.5585107</pub-id><pub-id pub-id-type="other">2-s2.0-78149418908</pub-id></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parsazad</surname><given-names>S.</given-names></name><name><surname>Saboori</surname><given-names>E.</given-names></name><name><surname>Allahyar</surname><given-names>A.</given-names></name></person-group><article-title>Fast feature reduction in intrusion detection datasets</article-title><conf-name>Proceedings of the 35th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO '12)</conf-name><conf-date>May 2012</conf-date><fpage>1023</fpage><lpage>1029</lpage><pub-id pub-id-type="other">2-s2.0-84865067246</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sung</surname><given-names>A. H.</given-names></name><name><surname>Mukkamala</surname><given-names>S.</given-names></name></person-group><article-title>Identifying important features for intrusion detection using support vector machines and neural networks</article-title><conf-name>Proceedings of the International Symposium on Applications and the Internet</conf-name><conf-date>January 2003</conf-date><conf-loc>Orlando, Fla, USA</conf-loc><publisher-name>IEEE</publisher-name><fpage>209</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1109/SAINT.2003.1183050</pub-id></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Revathi</surname><given-names>S.</given-names></name><name><surname>Malathi</surname><given-names>A.</given-names></name></person-group><article-title>Optimization of KDD Cup 99 dataset for intrusion detection using hybrid swarm intelligence with random forest classifier</article-title><source><italic>International Journal of Advanced Research in Computer Science and Software Engineering</italic></source><year>2013</year><volume>3</volume><issue>7</issue><fpage>1382</fpage><lpage>1387</lpage></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Revathi</surname><given-names>S.</given-names></name><name><surname>Malathi</surname><given-names>A.</given-names></name></person-group><article-title>Data preprocessing for intrusion detection system using swarm intelligence techniques</article-title><source><italic>International Journal of Computer Applications</italic></source><year>2013</year><volume>75</volume><issue>6</issue><fpage>22</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.5120/13116-0458</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Y. Y.</given-names></name><name><surname>Wahid</surname><given-names>N.</given-names></name></person-group><article-title>A hybrid network intrusion detection system using simplified swarm optimization (SSO)</article-title><source><italic>Applied Soft Computing</italic></source><year>2012</year><volume>12</volume><issue>9</issue><fpage>3014</fpage><lpage>3022</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2012.04.020</pub-id><pub-id pub-id-type="other">2-s2.0-84863478979</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L.</given-names></name><name><surname>Jiang</surname><given-names>F.</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kuznetsov</surname><given-names>S. O.</given-names></name><name><surname>Mandal</surname><given-names>D. P.</given-names></name><name><surname>Kundu</surname><given-names>M. K.</given-names></name><name><surname>Pal</surname><given-names>S. K.</given-names></name></person-group><article-title>A rough set based decision tree algorithm and its application in intrusion detection</article-title><source><italic>Pattern Recognition and Machine Intelligence</italic></source><year>2011</year><volume>6744</volume><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>333</fpage><lpage>338</lpage><series>Lecture Notes in Computer Science</series><pub-id pub-id-type="doi">10.1007/978-3-642-21786-9_54</pub-id></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>G.</given-names></name><name><surname>Hao</surname><given-names>J.</given-names></name><name><surname>Mab</surname><given-names>J.</given-names></name><name><surname>Huang</surname><given-names>L.</given-names></name></person-group><article-title>A new approach to intrusion detection using Artificial Neural Networks and fuzzy clustering</article-title><source><italic>Expert Systems with Applications</italic></source><year>2010</year><volume>37</volume><issue>9</issue><fpage>6225</fpage><lpage>6232</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2010.02.102</pub-id><pub-id pub-id-type="other">2-s2.0-78651352372</pub-id></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sivatha Sindhu</surname><given-names>S. S.</given-names></name><name><surname>Geetha</surname><given-names>S.</given-names></name><name><surname>Kannan</surname><given-names>A.</given-names></name></person-group><article-title>Decision tree based light weight intrusion detection using a wrapper approach</article-title><source><italic>Expert Systems with Applications</italic></source><year>2012</year><volume>39</volume><issue>1</issue><fpage>129</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2011.06.013</pub-id><pub-id pub-id-type="other">2-s2.0-81855221688</pub-id></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baig</surname><given-names>Z. A.</given-names></name><name><surname>Sait</surname><given-names>S. M.</given-names></name><name><surname>Shaheen</surname><given-names>A.</given-names></name></person-group><article-title>GMDH-based networks for intelligent intrusion detection</article-title><source><italic>Engineering Applications of Artificial Intelligence</italic></source><year>2013</year><volume>26</volume><issue>7</issue><fpage>1731</fpage><lpage>1740</lpage><pub-id pub-id-type="doi">10.1016/j.engappai.2013.03.008</pub-id><pub-id pub-id-type="other">2-s2.0-84878107136</pub-id></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mukkamala</surname><given-names>S.</given-names></name><name><surname>Janoski</surname><given-names>G.</given-names></name><name><surname>Sung</surname><given-names>A.</given-names></name></person-group><article-title>Intrusion detection using neural networks and support vector machines</article-title><conf-name>Proceedings of the International Joint Conference on Neural Networks (IJCNN '02)</conf-name><conf-date>May 2002</conf-date><fpage>1702</fpage><lpage>1707</lpage><pub-id pub-id-type="other">2-s2.0-0036085392</pub-id></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F.</given-names></name></person-group><article-title>Hybrid neural network intrusion detection system using genetic algorithm</article-title><conf-name>Proceedings of the International Conference on Multimedia Technology</conf-name><conf-date>October 2010</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/icmult.2010.5631462</pub-id><pub-id pub-id-type="other">2-s2.0-79952226834</pub-id></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>G.</given-names></name><name><surname>Mingjie</surname><given-names>E.</given-names></name><name><surname>Sun</surname><given-names>N.</given-names></name></person-group><article-title>A novel intrusion detection method based on improved SVM by combining PCA and PSO</article-title><source><italic>Wuhan University Journal of Natural Sciences</italic></source><year>2011</year><volume>16</volume><issue>5</issue><fpage>409</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1007/s11859-011-0771-6</pub-id><pub-id pub-id-type="other">2-s2.0-80052289855</pub-id></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>T.-S.</given-names></name><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Fan</surname><given-names>S.</given-names></name><name><surname>Makki</surname><given-names>K.</given-names></name></person-group><article-title>Ensemble of machine learning algorithms for intrusion detection</article-title><conf-name>Proceedings of the IEEE International Conference on Systems, Man and Cybernetics (SMC '09)</conf-name><conf-date>October 2009</conf-date><conf-loc>San Antonio, TX, USA</conf-loc><publisher-name>IEEE</publisher-name><fpage>3976</fpage><lpage>3980</lpage><pub-id pub-id-type="doi">10.1109/icsmc.2009.5346669</pub-id><pub-id pub-id-type="other">2-s2.0-74849089286</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panda</surname><given-names>M.</given-names></name><name><surname>Ranjan Patra</surname><given-names>M.</given-names></name></person-group><article-title>Ensemble voting system for anomaly based network intrusion detection</article-title><source><italic>International Journal of Recent Trends in Engineering</italic></source><year>2009</year><volume>2</volume><issue>5</issue><fpage>8</fpage><lpage>13</lpage></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghali</surname><given-names>N. I.</given-names></name></person-group><article-title>Feature selection for effective anomaly-based intrusion detection</article-title><source><italic>International Journal of Computer Science and Network Security</italic></source><year>2009</year><volume>9</volume><issue>3</issue><fpage>285</fpage><lpage>289</lpage></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Einipour</surname><given-names>A.</given-names></name></person-group><article-title>Intelligent intrusion detection in computer networks using fuzzy systems</article-title><source><italic>Global Journal of Computer Science and Technology</italic></source><year>2012</year><volume>12</volume><issue>11</issue><fpage>19</fpage><lpage>29</lpage></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V. N.</given-names></name></person-group><source><italic>The Nature of Statistical Learning Theory</italic></source><year>1995</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4757-2440-0</pub-id><pub-id pub-id-type="other">MR1367965</pub-id></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Satpute</surname><given-names>K.</given-names></name><name><surname>Agrawal</surname><given-names>S.</given-names></name><name><surname>Agrawal</surname><given-names>J.</given-names></name><name><surname>Sharma</surname><given-names>S.</given-names></name></person-group><article-title>A survey on anomaly detection in network intrusion detection system using particle swarm optimization based machine learning techniques</article-title><source><italic>Proceedings of the International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA)</italic></source><year>2013</year><volume>199</volume><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>441</fpage><lpage>452</lpage><series>Advances in Intelligent Systems and Computing</series><pub-id pub-id-type="doi">10.1007/978-3-642-35314-7_50</pub-id></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Y. Y.</given-names></name><name><surname>Wahid</surname><given-names>N.</given-names></name></person-group><article-title>A hybrid network intrusion detection system using simplified swarm optimization (SSO)</article-title><source><italic>Applied Soft Computing Journal</italic></source><year>2012</year><volume>12</volume><issue>9</issue><fpage>3014</fpage><lpage>3022</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2012.04.020</pub-id><pub-id pub-id-type="other">2-s2.0-84863478979</pub-id></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karaboga</surname><given-names>D.</given-names></name><name><surname>Basturk</surname><given-names>B.</given-names></name></person-group><article-title>On the performance of artificial bee colony (ABC) algorithm</article-title><source><italic>Applied Soft Computing Journal</italic></source><year>2008</year><volume>8</volume><issue>1</issue><fpage>687</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2007.05.007</pub-id><pub-id pub-id-type="other">2-s2.0-34548479029</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karaboga</surname><given-names>D.</given-names></name><name><surname>Akay</surname><given-names>B.</given-names></name></person-group><article-title>A comparative study of artificial Bee colony algorithm</article-title><source><italic>Applied Mathematics and Computation</italic></source><year>2009</year><volume>214</volume><issue>1</issue><fpage>108</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.amc.2009.03.090</pub-id><pub-id pub-id-type="other">MR2541051</pub-id><pub-id pub-id-type="other">2-s2.0-67349273050</pub-id></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>D. D.</given-names></name><name><surname>Kumar</surname><given-names>B.</given-names></name></person-group><article-title>Optimization of benchmark functions using artificial bee colony (ABC) algorithm</article-title><source><italic>IOSR Journal of Engineering</italic></source><year>2013</year><volume>3</volume><issue>10</issue><fpage>9</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.9790/3021-031040914</pub-id></element-citation></ref><ref id="B30"><label>30</label><element-citation publication-type="other"><comment><ext-link ext-link-type="uri" xlink:href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz">http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz</ext-link></comment></element-citation></ref><ref id="B31"><label>31</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Newman</surname><given-names>C. B. D.</given-names></name><name><surname>Merz</surname><given-names>C.</given-names></name></person-group><article-title>UCI repository of machine learning databases</article-title><year>1998</year><publisher-loc>Irvine, Calif, USA</publisher-loc><publisher-name>Department of Information and Computer Science, University of California</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="http://www.ics.uci.edu/~mlearn/MLRepository">http://www.ics.uci.edu/~mlearn/MLRepository</ext-link></comment></element-citation></ref><ref id="B32"><label>32</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tavallaee</surname><given-names>M.</given-names></name><name><surname>Bagheri</surname><given-names>E.</given-names></name><name><surname>Lu</surname><given-names>W.</given-names></name><name><surname>Ghorbani</surname><given-names>A. A.</given-names></name></person-group><article-title>A detailed analysis of the KDD CUP 99 data set</article-title><conf-name>IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA '09)</conf-name><conf-date>July 2009</conf-date><pub-id pub-id-type="doi">10.1109/cisda.2009.5356528</pub-id><pub-id pub-id-type="other">2-s2.0-77950575061</pub-id></element-citation></ref><ref id="B33"><label>33</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Amudha</surname><given-names>P.</given-names></name><name><surname>Abdul Rauf</surname><given-names>H.</given-names></name></person-group><article-title>Performance analysis of data mining approaches in intrusion detection</article-title><conf-name>Proceedings of the International Conference on Process Automation, Control and Computing (PACC '11)</conf-name><conf-date>July 2011</conf-date><fpage>9</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/pacc.2011.5978878</pub-id><pub-id pub-id-type="other">2-s2.0-80052217423</pub-id></element-citation></ref><ref id="B34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thakker</surname><given-names>R. A.</given-names></name><name><surname>Baghini</surname><given-names>M. S.</given-names></name><name><surname>Patil</surname><given-names>M. B.</given-names></name></person-group><article-title>Automatic design of low-power low-voltage analog circuits using particle swarm optimization with re-initialization</article-title><source><italic>Journal of Low Power Electronics</italic></source><year>2009</year><volume>5</volume><issue>3</issue><fpage>291</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1166/jolpe.2009.1030</pub-id><pub-id pub-id-type="other">2-s2.0-72749085609</pub-id></element-citation></ref><ref id="B35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karaboga</surname><given-names>D.</given-names></name><name><surname>Basturk</surname><given-names>B.</given-names></name></person-group><article-title>A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm</article-title><source><italic>Journal of Global Optimization</italic></source><year>2007</year><volume>39</volume><issue>3</issue><fpage>459</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1007/s10898-007-9149-x</pub-id><pub-id pub-id-type="other">MR2346178</pub-id><pub-id pub-id-type="other">2-s2.0-35148821762</pub-id></element-citation></ref><ref id="B36"><label>36</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Eberhart</surname><given-names>R. C.</given-names></name></person-group><article-title>A modified particle swarm optimizer</article-title><conf-name>Proceedings of the IEEE World Congress on Computational Intelligence</conf-name><conf-date>May 1998</conf-date><conf-loc>Anchorage, Alaska, USA</conf-loc><publisher-name>IEEE</publisher-name><fpage>69</fpage><lpage>73</lpage></element-citation></ref><ref id="B37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diamantidis</surname><given-names>N. A.</given-names></name><name><surname>Karlis</surname><given-names>D.</given-names></name><name><surname>Giakoumakis</surname><given-names>E. A.</given-names></name></person-group><article-title>Unsupervised stratification of cross-validation for accuracy estimation</article-title><source><italic>Artificial Intelligence</italic></source><year>2000</year><volume>116</volume><issue>1-2</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/s0004-3702(99)00094-6</pub-id><pub-id pub-id-type="other">MR1746938</pub-id><pub-id pub-id-type="other">2-s2.0-0033903664</pub-id></element-citation></ref><ref id="B38"><label>38</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Larose</surname><given-names>D. T.</given-names></name></person-group><source><italic>Discovering Knowledge in Data&#x02014;An Introduction to Data Mining</italic></source><year>2005</year><publisher-name>John Wiley &#x00026; Sons</publisher-name><pub-id pub-id-type="other">MR2100732</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Data preprocessing.</p></caption><graphic xlink:href="TSWJ2015-574589.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Flowchart of the proposed hybrid MABC-EPSO model.</p></caption><graphic xlink:href="TSWJ2015-574589.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Accuracy comparison of classifiers for DoS dataset.</p></caption><graphic xlink:href="TSWJ2015-574589.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Accuracy comparison of classifiers for probe dataset.</p></caption><graphic xlink:href="TSWJ2015-574589.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Accuracy comparison of classifiers for R2L dataset.</p></caption><graphic xlink:href="TSWJ2015-574589.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Accuracy comparison of classifiers for U2R dataset.</p></caption><graphic xlink:href="TSWJ2015-574589.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Comparison on sensitivity using SFSM method.</p></caption><graphic xlink:href="TSWJ2015-574589.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Comparison on sensitivity using RFSM method.</p></caption><graphic xlink:href="TSWJ2015-574589.008"/></fig><fig id="fig9" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Comparison on specificity using SFSM method.</p></caption><graphic xlink:href="TSWJ2015-574589.009"/></fig><fig id="fig10" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Comparison on specificity using RFSM method.</p></caption><graphic xlink:href="TSWJ2015-574589.010"/></fig><fig id="fig11" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Performance comparison on false alarm rate of classifiers.</p></caption><graphic xlink:href="TSWJ2015-574589.011"/></fig><fig id="fig12" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Training time of MABC-EPSO.</p></caption><graphic xlink:href="TSWJ2015-574589.012"/></fig><fig id="alg1" orientation="portrait" position="float"><label>Algorithm 1</label><caption><p>
Artificial Bee Colony.</p></caption><graphic xlink:href="TSWJ2015-574589.alg.001"/></fig><fig id="alg2" orientation="portrait" position="float"><label>Algorithm 2</label><caption><p>Single feature selection method.</p></caption><graphic xlink:href="TSWJ2015-574589.alg.002"/></fig><fig id="alg3" orientation="portrait" position="float"><label>Algorithm 3</label><caption><p>Random feature selection method.</p></caption><graphic xlink:href="TSWJ2015-574589.alg.003"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Distribution of connection types in 10% KDDCup'99 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">
Label</th><th align="center" colspan="6" rowspan="1">% of occurrence</th></tr><tr><th align="center" rowspan="1" colspan="1">DoS</th><th align="center" rowspan="1" colspan="1">Probe</th><th align="center" rowspan="1" colspan="1">U2R</th><th align="center" rowspan="1" colspan="1">R2L</th><th align="center" rowspan="1" colspan="1">Total attack</th><th align="center" rowspan="1" colspan="1">Total normal</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Training data</td><td align="center" rowspan="1" colspan="1">79.24%</td><td align="center" rowspan="1" colspan="1">0.83%</td><td align="center" rowspan="1" colspan="1">0.01%</td><td align="center" rowspan="1" colspan="1">0.23%</td><td align="center" rowspan="1" colspan="1">80.31%</td><td align="center" rowspan="1" colspan="1">19.69%</td></tr><tr><td align="left" rowspan="1" colspan="1">Testing data</td><td align="center" rowspan="1" colspan="1">73.90%</td><td align="center" rowspan="1" colspan="1">1.34%</td><td align="center" rowspan="1" colspan="1">0.07%</td><td align="center" rowspan="1" colspan="1">5.20%</td><td align="center" rowspan="1" colspan="1">81.51%</td><td align="center" rowspan="1" colspan="1">19.49%</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Sample size in 10% KDDCUP dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Category of attack</th><th align="left" rowspan="1" colspan="1">Attack name</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Normal</td><td align="left" rowspan="1" colspan="1">Normal (97277)</td></tr><tr><td align="center" colspan="2" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">DoS</td><td align="left" rowspan="1" colspan="1">Neptune (107201), Smurf (280790), Pod (264), Teardrop (979), Land (21), Back (2203)</td></tr><tr><td align="center" colspan="2" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe</td><td align="left" rowspan="1" colspan="1">Portsweep (1040), IPsweep (1247), Nmap (231), Satan (1589)</td></tr><tr><td align="center" colspan="2" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R</td><td align="left" rowspan="1" colspan="1">Bufferoverflow (30), LoadModule (9), Perl (3), Rootkit (10)</td></tr><tr><td align="center" colspan="2" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L</td><td align="left" rowspan="1" colspan="1">Guesspassword (53), Ftpwrite (8), Imap (12), Phf (4), Multihop (7), Warezmaster (20), Warezclient (1020)</td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Feature information of 10% KDDCUP dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Dataset characteristics</th><th align="center" rowspan="1" colspan="1">Multivariate</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Attribute characteristics</td><td align="center" rowspan="1" colspan="1">Categorical, integer</td></tr><tr><td align="left" rowspan="1" colspan="1">Associated task</td><td align="center" rowspan="1" colspan="1">Classification</td></tr><tr><td align="left" rowspan="1" colspan="1">Area</td><td align="center" rowspan="1" colspan="1">Computer</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of instances</td><td align="center" rowspan="1" colspan="1">494,020</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of attributes</td><td align="center" rowspan="1" colspan="1">42</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of classes</td><td align="center" rowspan="1" colspan="1">1 normal class, 4 attack classes</td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Details of instances in the dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">Before removing duplicates </th><th align="center" rowspan="1" colspan="1">After removing duplicates</th><th align="center" rowspan="1" colspan="1">Selected instances</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Normal</td><td align="center" rowspan="1" colspan="1">97,277</td><td align="center" rowspan="1" colspan="1">87832</td><td align="center" rowspan="1" colspan="1">8783</td></tr><tr><td align="left" rowspan="1" colspan="1">DoS</td><td align="center" rowspan="1" colspan="1">391,458</td><td align="center" rowspan="1" colspan="1">54572</td><td align="center" rowspan="1" colspan="1">7935</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe</td><td align="center" rowspan="1" colspan="1">4,107</td><td align="center" rowspan="1" colspan="1">2131</td><td align="center" rowspan="1" colspan="1">2131</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R</td><td align="center" rowspan="1" colspan="1">52</td><td align="center" rowspan="1" colspan="1">52</td><td align="center" rowspan="1" colspan="1">52</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L</td><td align="center" rowspan="1" colspan="1">1,126</td><td align="center" rowspan="1" colspan="1">999</td><td align="center" rowspan="1" colspan="1">999</td></tr><tr><td align="center" colspan="4" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="center" rowspan="1" colspan="1">494,020</td><td align="center" rowspan="1" colspan="1">145,586</td><td align="center" rowspan="1" colspan="1">19,900</td></tr></tbody></table></table-wrap><table-wrap id="tab5" orientation="portrait" position="float"><label>Table 5</label><caption><p>List of features selected using SFSM methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Selected features</th><th align="center" rowspan="1" colspan="1">Number of features</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DoS + 10% normal</td><td align="left" rowspan="1" colspan="1">24, 32, 41, 28, 40, 27, 34, 35, 5, 17, 21, 4, 39, 11, 9, 7, 14, 1, 30, 6</td><td align="center" rowspan="1" colspan="1">20</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe + 10% normal</td><td align="left" rowspan="1" colspan="1">11, 1, 15, 26, 10, 4, 21, 18, 19, 25, 39, 31, 7, 35, 28</td><td align="center" rowspan="1" colspan="1">15</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L + 10% normal</td><td align="left" rowspan="1" colspan="1">16, 26, 30, 3, 7, 21, 6, 14, 12, 35, 32, 18, 38, 17, 41, 10, 31</td><td align="center" rowspan="1" colspan="1">17</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R + 10% normal</td><td align="left" rowspan="1" colspan="1">27, 40, 26, 1, 34, 41, 7, 18, 28, 3, 20, 37, 11</td><td align="center" rowspan="1" colspan="1">13</td></tr></tbody></table></table-wrap><table-wrap id="tab6" orientation="portrait" position="float"><label>Table 6</label><caption><p>List of features selected using RFSM methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Selected features</th><th align="center" rowspan="1" colspan="1">Number of features</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DoS + 10% normal</td><td align="left" rowspan="1" colspan="1">4, 9, 21, 39, 14, 28, 3, 8, 29, 33, 17, 12, 38, 31</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe + 10% normal</td><td align="left" rowspan="1" colspan="1">27, 2, 3, 30, 11, 33, 23, 9, 39, 20, 21, 37</td><td align="center" rowspan="1" colspan="1">12</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L + 10% normal</td><td align="left" rowspan="1" colspan="1">24, 15, 23, 7, 25, 16, 8, 33, 29, 38, 21, 30, 32</td><td align="center" rowspan="1" colspan="1">13</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R + 10% normal</td><td align="left" rowspan="1" colspan="1">6, 19, 22, 30, 21, 28, 36, 27, 11, 17, 20</td><td align="center" rowspan="1" colspan="1">11</td></tr></tbody></table></table-wrap><table-wrap id="tab7" orientation="portrait" position="float"><label>Table 7</label><caption><p>Confusion matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Actual </th><th align="center" colspan="2" rowspan="1">Predicted </th></tr><tr><th align="center" rowspan="1" colspan="1">Normal</th><th align="center" rowspan="1" colspan="1">Attack</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Normal</td><td align="center" rowspan="1" colspan="1">True Negative (TN)</td><td align="center" rowspan="1" colspan="1">False Positive (FP)</td></tr><tr><td align="left" rowspan="1" colspan="1">Attack</td><td align="center" rowspan="1" colspan="1">False Negative (FN)</td><td align="center" rowspan="1" colspan="1">True Positive (TP)</td></tr></tbody></table><table-wrap-foot><fn><p>
True Positive (TP): the number of of attacks that are correctly identified.</p></fn><fn><p>
True Negative (TN): the number of normal records that are correctly classified.</p></fn><fn><p>
False Positive (FP): the number of normal records incorrectly classified.</p></fn><fn><p>
False Negative (FN): the number of attacks incorrectly classified. </p></fn></table-wrap-foot></table-wrap><table-wrap id="tab8" orientation="portrait" position="float"><label>Table 8</label><caption><p>Performance comparison of classification algorithms on accuracy rate.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Classification Algorithms</th><th align="center" rowspan="1" colspan="1">Average accuracy (%)</th><th align="center" rowspan="1" colspan="1">Feature selection method</th></tr></thead><tbody><tr><td align="left" rowspan="4" colspan="1">
C4.5 [<xref rid="B6" ref-type="bibr">6</xref>]</td><td align="center" rowspan="1" colspan="1">99.11</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">98.69</td><td align="center" rowspan="1" colspan="1">Genetic algorithm</td></tr><tr><td align="center" rowspan="1" colspan="1">98.84</td><td align="center" rowspan="1" colspan="1">Best-first</td></tr><tr><td align="center" rowspan="1" colspan="1">99.41</td><td align="center" rowspan="1" colspan="1">Correlation feature selection</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="4" colspan="1">
BayesNet [<xref rid="B6" ref-type="bibr">6</xref>]</td><td align="center" rowspan="1" colspan="1">99.53</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">99.52</td><td align="center" rowspan="1" colspan="1">Genetic algorithm</td></tr><tr><td align="center" rowspan="1" colspan="1">98.91</td><td align="center" rowspan="1" colspan="1">Best-first</td></tr><tr><td align="center" rowspan="1" colspan="1">98.92</td><td align="center" rowspan="1" colspan="1">Correlation feature selection</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ABC-SVM [<xref rid="B7" ref-type="bibr">7</xref>]</td><td align="center" rowspan="1" colspan="1">92.768</td><td align="center" rowspan="3" colspan="1">Binary ABC</td></tr><tr><td align="left" rowspan="1" colspan="1">PSO-SVM [<xref rid="B7" ref-type="bibr">7</xref>]</td><td align="center" rowspan="1" colspan="1">83.88</td></tr><tr><td align="left" rowspan="1" colspan="1">GA-SVM [<xref rid="B7" ref-type="bibr">7</xref>]</td><td align="center" rowspan="1" colspan="1">80.73</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="2" colspan="1">
KNN [<xref rid="B8" ref-type="bibr">8</xref>]</td><td align="center" rowspan="1" colspan="1">98.24</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">98.11</td><td align="center" rowspan="1" colspan="1">Fast feature selection</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="2" colspan="1">
Bayes Classifier [<xref rid="B8" ref-type="bibr">8</xref>]</td><td align="center" rowspan="1" colspan="1">76.09</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">71.94</td><td align="center" rowspan="1" colspan="1">Fast feature selection</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ANN [<xref rid="B9" ref-type="bibr">9</xref>]</td><td align="center" rowspan="1" colspan="1">81.57</td><td align="center" rowspan="1" colspan="1">Feature reduction</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SSO-RF [<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B11" ref-type="bibr">11</xref>]</td><td align="center" rowspan="1" colspan="1">92.7</td><td align="center" rowspan="1" colspan="1">SSO</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Hybrid SSO [<xref rid="B12" ref-type="bibr">12</xref>]</td><td align="center" rowspan="1" colspan="1">97.67</td><td align="center" rowspan="1" colspan="1">SSO</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RSDT [<xref rid="B13" ref-type="bibr">13</xref>]</td><td align="center" rowspan="1" colspan="1">97.88</td><td align="center" rowspan="1" colspan="1">Rough set</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ID3 [<xref rid="B13" ref-type="bibr">13</xref>]</td><td align="center" rowspan="1" colspan="1">97.665</td><td align="center" rowspan="2" colspan="1">All features</td></tr><tr><td align="left" rowspan="1" colspan="1">C4.5 [<xref rid="B13" ref-type="bibr">13</xref>]</td><td align="center" rowspan="1" colspan="1">97.582</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">FC-ANN [<xref rid="B14" ref-type="bibr">14</xref>]</td><td align="center" rowspan="1" colspan="1">96.71</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="3" colspan="1">
<bold>Proposed MABC-EPSO</bold>
</td><td align="center" rowspan="1" colspan="1">88.59</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">99.32</td><td align="center" rowspan="1" colspan="1">Single feature selection method</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>99.82</bold>
</td><td align="center" rowspan="1" colspan="1">Random feature selection method</td></tr></tbody></table></table-wrap><table-wrap id="tab9" orientation="portrait" position="float"><label>Table 9</label><caption><p>Accuracy rates of classifiers using SFSM feature selection method and Friedman ranks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="center" rowspan="1" colspan="1">NB</th><th align="center" rowspan="1" colspan="1">J4.8</th><th align="center" rowspan="1" colspan="1">RBF</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">ABC</th><th align="center" rowspan="1" colspan="1">MABC-EPSO</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DoS + 10% normal</td><td align="center" rowspan="1" colspan="1">82.57 (6)</td><td align="center" rowspan="1" colspan="1">87.11 (4)</td><td align="center" rowspan="1" colspan="1">87.96 (3)</td><td align="center" rowspan="1" colspan="1">84.7 (5)</td><td align="center" rowspan="1" colspan="1">90.82 (2)</td><td align="center" rowspan="1" colspan="1">99.50 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe + 10% normal</td><td align="center" rowspan="1" colspan="1">82.68 (5)</td><td align="center" rowspan="1" colspan="1">82.6 (6)</td><td align="center" rowspan="1" colspan="1">83.72 (4)</td><td align="center" rowspan="1" colspan="1">85.67 (3)</td><td align="center" rowspan="1" colspan="1">96.58 (2)</td><td align="center" rowspan="1" colspan="1">99.27 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L + 10% normal</td><td align="center" rowspan="1" colspan="1">86.15 (4)</td><td align="center" rowspan="1" colspan="1">82.55 (6)</td><td align="center" rowspan="1" colspan="1">85.16 (5)</td><td align="center" rowspan="1" colspan="1">90.61 (3)</td><td align="center" rowspan="1" colspan="1">92.72 (2)</td><td align="center" rowspan="1" colspan="1">99.24 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R + 10% normal</td><td align="center" rowspan="1" colspan="1">84.06 (6)</td><td align="center" rowspan="1" colspan="1">87.16 (3)</td><td align="center" rowspan="1" colspan="1">85.54 (5)</td><td align="center" rowspan="1" colspan="1">85.97 (4)</td><td align="center" rowspan="1" colspan="1">97.31 (2)</td><td align="center" rowspan="1" colspan="1">99.8 (1)</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Average rank</td><td align="center" rowspan="1" colspan="1">5.25</td><td align="center" rowspan="1" colspan="1">4.75</td><td align="center" rowspan="1" colspan="1">4.25</td><td align="center" rowspan="1" colspan="1">3.75</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">1</td></tr></tbody></table></table-wrap><table-wrap id="tab10" orientation="portrait" position="float"><label>Table 10</label><caption><p>Accuracy rates using RFSM feature selection method and Friedman ranks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="center" rowspan="1" colspan="1">NB</th><th align="center" rowspan="1" colspan="1">J4.8</th><th align="center" rowspan="1" colspan="1">RBF</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">ABC</th><th align="center" rowspan="1" colspan="1">MABC-EPSO</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DoS + 10% normal</td><td align="center" rowspan="1" colspan="1">83.04 (6)</td><td align="center" rowspan="1" colspan="1">90.05 (4)</td><td align="center" rowspan="1" colspan="1">88.83 (5)</td><td align="center" rowspan="1" colspan="1">94.02 (3)</td><td align="center" rowspan="1" colspan="1">96.43 (2)</td><td align="center" rowspan="1" colspan="1">99.81 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe + 10% normal</td><td align="center" rowspan="1" colspan="1">84.01 (5)</td><td align="center" rowspan="1" colspan="1">82.72 (6)</td><td align="center" rowspan="1" colspan="1">85.94 (4)</td><td align="center" rowspan="1" colspan="1">95.87 (3)</td><td align="center" rowspan="1" colspan="1">97.31 (2)</td><td align="center" rowspan="1" colspan="1">99.86 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L + 10% normal</td><td align="center" rowspan="1" colspan="1">86.32 (4)</td><td align="center" rowspan="1" colspan="1">83.10 (6)</td><td align="center" rowspan="1" colspan="1">86.11 (5)</td><td align="center" rowspan="1" colspan="1">97.04 (3)</td><td align="center" rowspan="1" colspan="1">98.96 (2)</td><td align="center" rowspan="1" colspan="1">99.80 (1)</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R + 10% normal</td><td align="center" rowspan="1" colspan="1">85.15 (6)</td><td align="center" rowspan="1" colspan="1">88.42 (5)</td><td align="center" rowspan="1" colspan="1">88.98 (4)</td><td align="center" rowspan="1" colspan="1">95.91 (3)</td><td align="center" rowspan="1" colspan="1">98.96 (2)</td><td align="center" rowspan="1" colspan="1">99.80 (1)</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Average rank</td><td align="center" rowspan="1" colspan="1">5.25</td><td align="center" rowspan="1" colspan="1">5.25</td><td align="center" rowspan="1" colspan="1">4.5</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">1</td></tr></tbody></table></table-wrap><table-wrap id="tab11" orientation="portrait" position="float"><label>Table 11</label><caption><p>ANOVA results for accuracy rate of classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Source of variation</th><th align="center" rowspan="1" colspan="1">SS</th><th align="center" rowspan="1" colspan="1">df</th><th align="center" rowspan="1" colspan="1">MS</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>
</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> value</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>-crit.</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" colspan="6" rowspan="1">SFSM method</td></tr><tr><td align="left" rowspan="1" colspan="1">Between groups</td><td align="center" rowspan="1" colspan="1">781.5143</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">156.3029</td><td align="center" rowspan="1" colspan="1">31.89498</td><td align="center" rowspan="1" colspan="1">&#x0003c;0.05</td><td align="center" rowspan="1" colspan="1">2.772853</td></tr><tr><td align="left" rowspan="1" colspan="1">Within groups</td><td align="center" rowspan="1" colspan="1">88.20985</td><td align="center" rowspan="1" colspan="1">18</td><td align="center" rowspan="1" colspan="1">4.900547</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="center" rowspan="1" colspan="1">869.7241</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" colspan="6" rowspan="1">RFSM method</td></tr><tr><td align="left" rowspan="1" colspan="1">Between groups</td><td align="center" rowspan="1" colspan="1">879.4307</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">175.8861</td><td align="center" rowspan="1" colspan="1">48.54728</td><td align="center" rowspan="1" colspan="1">&#x0003c;0.05</td><td align="center" rowspan="1" colspan="1">2.772853</td></tr><tr><td align="left" rowspan="1" colspan="1">Within groups</td><td align="center" rowspan="1" colspan="1">65.21375</td><td align="center" rowspan="1" colspan="1">18</td><td align="center" rowspan="1" colspan="1">3.622986</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="center" rowspan="1" colspan="1">944.6444</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup><italic>&#x02217;</italic></sup>SS: sum of squared deviations about mean; df: degrees of freedom; MS: variance.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tab12" orientation="portrait" position="float"><label>Table 12</label><caption><p>Performance comparison of classification algorithms on detection rate.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Classification Algorithm</th><th align="center" rowspan="1" colspan="1">Average detection rate (%)</th><th align="center" rowspan="1" colspan="1">Feature selection method</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Na&#x000ef;ve Bayes [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">92.27</td><td align="center" rowspan="6" colspan="1">Genetic algorithm</td></tr><tr><td align="left" rowspan="1" colspan="1">C4.5 [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">92.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Random forest [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">89.21</td></tr><tr><td align="left" rowspan="1" colspan="1">Random tree [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">88.98</td></tr><tr><td align="left" rowspan="1" colspan="1">REP tree [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">89.11</td></tr><tr><td align="left" rowspan="1" colspan="1">Neurotree [<xref rid="B15" ref-type="bibr">15</xref>]</td><td align="center" rowspan="1" colspan="1">98.38</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="3" colspan="1">
GMDH Based neural network [<xref rid="B16" ref-type="bibr">16</xref>]</td><td align="center" rowspan="1" colspan="1">93.7</td><td align="center" rowspan="1" colspan="1">Information gain</td></tr><tr><td align="center" rowspan="1" colspan="1">97.5</td><td align="center" rowspan="1" colspan="1">Gain ratio</td></tr><tr><td align="center" rowspan="1" colspan="1">95.3</td><td align="center" rowspan="1" colspan="1">GMDH</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Neural network [<xref rid="B17" ref-type="bibr">17</xref>]</td><td align="center" rowspan="1" colspan="1">81.57</td><td align="center" rowspan="1" colspan="1">Feature reduction</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Hybrid evolutionary neural network [<xref rid="B18" ref-type="bibr">18</xref>]</td><td align="center" rowspan="1" colspan="1">91.51</td><td align="center" rowspan="1" colspan="1">Genetic algorithm</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Improved SVM (PSO + SVM + PCA) [<xref rid="B19" ref-type="bibr">19</xref>]</td><td align="center" rowspan="1" colspan="1">97.75</td><td align="center" rowspan="1" colspan="1">PCA</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ensemble Bayesian combination [<xref rid="B20" ref-type="bibr">20</xref>]</td><td align="center" rowspan="1" colspan="1">93.35</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Voting + j48 + Rule [<xref rid="B21" ref-type="bibr">21</xref>]</td><td align="center" rowspan="1" colspan="1">97.47</td><td align="center" rowspan="2" colspan="1">All features</td></tr><tr><td align="left" rowspan="1" colspan="1">Voting + AdaBoost + j48 [<xref rid="B21" ref-type="bibr">21</xref>]</td><td align="center" rowspan="1" colspan="1">97.38</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Rough set neural network algorithm [<xref rid="B22" ref-type="bibr">22</xref>]</td><td align="center" rowspan="1" colspan="1">90</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">PSO based fuzzy system [<xref rid="B23" ref-type="bibr">23</xref>]</td><td align="center" rowspan="1" colspan="1">93.7</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="3" colspan="1">
<bold>Proposed MABC-EPSO</bold>
</td><td align="center" rowspan="1" colspan="1">87.16</td><td align="center" rowspan="1" colspan="1">All features</td></tr><tr><td align="center" rowspan="1" colspan="1">98.09</td><td align="center" rowspan="1" colspan="1">
<bold>Single feature selection method</bold>
</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>98.67</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>Random feature selection method</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="tab13" orientation="portrait" position="float"><label>Table 13</label><caption><p>ANOVA results for specificity of classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Source of variation</th><th align="center" rowspan="1" colspan="1">SS</th><th align="center" rowspan="1" colspan="1">df</th><th align="center" rowspan="1" colspan="1">MS</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>
</th><th align="center" rowspan="1" colspan="1">
<italic>P</italic> value</th><th align="center" rowspan="1" colspan="1">
<italic>F</italic>-crit.</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" colspan="6" rowspan="1">SFSM</td></tr><tr><td align="left" rowspan="1" colspan="1">Between groups</td><td align="center" rowspan="1" colspan="1">659.6518</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">131.9304</td><td align="center" rowspan="1" colspan="1">52.5347</td><td align="center" rowspan="1" colspan="1">&#x0003c;0.05</td><td align="center" rowspan="1" colspan="1">2.772853</td></tr><tr><td align="left" rowspan="1" colspan="1">Within groups</td><td align="center" rowspan="1" colspan="1">45.20339</td><td align="center" rowspan="1" colspan="1">18</td><td align="center" rowspan="1" colspan="1">2.511299</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="center" rowspan="1" colspan="1">704.8551</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" colspan="6" rowspan="1">RFSM</td></tr><tr><td align="left" rowspan="1" colspan="1">Between groups</td><td align="center" rowspan="1" colspan="1">617.818</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">123.5636</td><td align="center" rowspan="1" colspan="1">23.53957</td><td align="center" rowspan="1" colspan="1">&#x0003c;0.05</td><td align="center" rowspan="1" colspan="1">2.772853</td></tr><tr><td align="left" rowspan="1" colspan="1">Within groups</td><td align="center" rowspan="1" colspan="1">94.48535</td><td align="center" rowspan="1" colspan="1">18</td><td align="center" rowspan="1" colspan="1">5.249186</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr><tr><td align="center" colspan="7" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Total</td><td align="center" rowspan="1" colspan="1">712.3033</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1">&#x02009;</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup><italic>&#x02217;</italic></sup>SS: sum of squared deviations about mean; df: degrees of freedom; MS: variance.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tab14" orientation="portrait" position="float"><label>Table 14</label><caption><p>Training time of classification algorithms using SFSM and RFSM feature selection methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Dataset</th><th align="center" colspan="6" rowspan="1">SFSM</th><th align="center" colspan="6" rowspan="1">RFSM</th></tr><tr><th align="center" rowspan="1" colspan="1">Na&#x000ef;ve Bayes</th><th align="center" rowspan="1" colspan="1">J4.8</th><th align="center" rowspan="1" colspan="1">RBF</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">ABC</th><th align="center" rowspan="1" colspan="1">MABC-EPSO</th><th align="center" rowspan="1" colspan="1">Na&#x000ef;ve Bayes</th><th align="center" rowspan="1" colspan="1">J4.8</th><th align="center" rowspan="1" colspan="1">RBF</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">ABC</th><th align="center" rowspan="1" colspan="1">MABC-EPSO</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DoS + 10% normal</td><td align="center" rowspan="1" colspan="1">10.20</td><td align="center" rowspan="1" colspan="1">4.7</td><td align="center" rowspan="1" colspan="1">3.8</td><td align="center" rowspan="1" colspan="1">2.86</td><td align="center" rowspan="1" colspan="1">2.78</td><td align="center" rowspan="1" colspan="1">2.22</td><td align="center" rowspan="1" colspan="1">9.95</td><td align="center" rowspan="1" colspan="1">3.95</td><td align="center" rowspan="1" colspan="1">3.28</td><td align="center" rowspan="1" colspan="1">2.59</td><td align="center" rowspan="1" colspan="1">2.07</td><td align="center" rowspan="1" colspan="1">1.5</td></tr><tr><td align="left" rowspan="1" colspan="1">Probe + 10% normal</td><td align="center" rowspan="1" colspan="1">5.33</td><td align="center" rowspan="1" colspan="1">3.12</td><td align="center" rowspan="1" colspan="1">3.05</td><td align="center" rowspan="1" colspan="1">2.36</td><td align="center" rowspan="1" colspan="1">2.24</td><td align="center" rowspan="1" colspan="1">1.87</td><td align="center" rowspan="1" colspan="1">4.15</td><td align="center" rowspan="1" colspan="1">3.01</td><td align="center" rowspan="1" colspan="1">3.19</td><td align="center" rowspan="1" colspan="1">2.11</td><td align="center" rowspan="1" colspan="1">1.97</td><td align="center" rowspan="1" colspan="1">1.69</td></tr><tr><td align="left" rowspan="1" colspan="1">U2R + 10% normal</td><td align="center" rowspan="1" colspan="1">4.75</td><td align="center" rowspan="1" colspan="1">3.81</td><td align="center" rowspan="1" colspan="1">3.08</td><td align="center" rowspan="1" colspan="1">2.21</td><td align="center" rowspan="1" colspan="1">2.16</td><td align="center" rowspan="1" colspan="1">1.98</td><td align="center" rowspan="1" colspan="1">4.01</td><td align="center" rowspan="1" colspan="1">3.46</td><td align="center" rowspan="1" colspan="1">2.79</td><td align="center" rowspan="1" colspan="1">1.80</td><td align="center" rowspan="1" colspan="1">1.78</td><td align="center" rowspan="1" colspan="1">0.65</td></tr><tr><td align="left" rowspan="1" colspan="1">R2L + 10% normal</td><td align="center" rowspan="1" colspan="1">3.98</td><td align="center" rowspan="1" colspan="1">4.97</td><td align="center" rowspan="1" colspan="1">3.01</td><td align="center" rowspan="1" colspan="1">2.46</td><td align="center" rowspan="1" colspan="1">2.23</td><td align="center" rowspan="1" colspan="1">2.0</td><td align="center" rowspan="1" colspan="1">3.12</td><td align="center" rowspan="1" colspan="1">3.23</td><td align="center" rowspan="1" colspan="1">2.55</td><td align="center" rowspan="1" colspan="1">1.42</td><td align="center" rowspan="1" colspan="1">1.37</td><td align="center" rowspan="1" colspan="1">1.46</td></tr></tbody></table></table-wrap></floats-group></article>