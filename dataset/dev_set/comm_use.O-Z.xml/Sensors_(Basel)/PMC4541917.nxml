<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26184219</article-id><article-id pub-id-type="pmc">4541917</article-id><article-id pub-id-type="doi">10.3390/s150716981</article-id><article-id pub-id-type="publisher-id">sensors-15-16981</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visual Contrast Enhancement Algorithm Based on Histogram Equalization</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ting</surname><given-names>Chih-Chung</given-names></name><xref ref-type="aff" rid="af1-sensors-15-16981">1</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Bing-Fei</given-names></name><xref ref-type="aff" rid="af2-sensors-15-16981">2</xref></contrib><contrib contrib-type="author"><name><surname>Chung</surname><given-names>Meng-Liang</given-names></name><xref ref-type="aff" rid="af2-sensors-15-16981">2</xref><xref rid="c1-sensors-15-16981" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Chiu</surname><given-names>Chung-Cheng</given-names></name><xref ref-type="aff" rid="af3-sensors-15-16981">3</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Ya-Ching</given-names></name><xref ref-type="aff" rid="af3-sensors-15-16981">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Passaro</surname><given-names>Vittorio M.N.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-15-16981"><label>1</label>School of Defense Science, Chung Cheng Institute of Technology, National Defense University, Taoyuan 33551, Taiwan; E-Mail: <email>chihchungting@gmail.com</email></aff><aff id="af2-sensors-15-16981"><label>2</label>Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu 30010, Taiwan; E-Mails: <email>bwu@cssp.cn.nctu.edu.tw</email></aff><aff id="af3-sensors-15-16981"><label>3</label>Department of Electrical and Electronic Engineering, Chung Cheng Institute of Technology, National Defense University, Taoyuan 33551, Taiwan; E-Mails: <email>davidchiu@ndu.edu.tw</email> (C-C.C.); <email>m0919048774@yahoo.com.tw</email> (Y-C.W.)</aff><author-notes><corresp id="c1-sensors-15-16981"><label>*</label>Author to whom correspondence should be addressed; E-Mail: <email>mlchung.ece96g@g2.nctu.edu.tw</email>; Tel.: +886-3-390-9962.</corresp></author-notes><pub-date pub-type="epub"><day>13</day><month>7</month><year>2015</year></pub-date><pub-date pub-type="collection"><month>7</month><year>2015</year></pub-date><volume>15</volume><issue>7</issue><fpage>16981</fpage><lpage>16999</lpage><history><date date-type="received"><day>14</day><month>5</month><year>2015</year></date><date date-type="accepted"><day>08</day><month>7</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; 2015 by the authors; licensee MDPI, Basel, Switzerland.</copyright-statement><copyright-year>2015</copyright-year><license><license-p><!--CREATIVE COMMONS-->This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Image enhancement techniques primarily improve the contrast of an image to lend it a better appearance. One of the popular enhancement methods is histogram equalization (HE) because of its simplicity and effectiveness. However, it is rarely applied to consumer electronics products because it can cause excessive contrast enhancement and feature loss problems. These problems make the images processed by HE look unnatural and introduce unwanted artifacts in them. In this study, a visual contrast enhancement algorithm (VCEA) based on HE is proposed. VCEA considers the requirements of the human visual perception in order to address the drawbacks of HE. It effectively solves the excessive contrast enhancement problem by adjusting the spaces between two adjacent gray values of the HE histogram. In addition, VCEA reduces the effects of the feature loss problem by using the obtained spaces. Furthermore, VCEA enhances the detailed textures of an image to generate an enhanced image with better visual quality. Experimental results show that images obtained by applying VCEA have higher contrast and are more suited to human visual perception than those processed by HE and other HE-based methods.</p></abstract><kwd-group><kwd>contrast enhancement</kwd><kwd>dynamic range</kwd><kwd>histogram equalization (HE)</kwd><kwd>just-noticeable difference (JND)</kwd></kwd-group></article-meta></front><body><sec><title>1. Introduction</title><p>Light plays a crucial role in generating images of satisfactory quality in photography. Strong light causes an image to have a washed out appearance; on the contrary, weak light leads to an image that is too dark to be visible. In these two cases, the contrasts of the images are low and their detailed textures are difficult to discern. Furthermore, the poor sensitivity of charge-coupled device/complementary&#x02013;metal&#x02013;oxide&#x02013;semiconductor (CCD/CMOS) sensors leads to images with excessively narrow dynamic ranges and renders their details unclear. Consequently, image enhancement techniques are widely used to solve such problems and improve image quality.</p><p>Histogram equalization (HE) [<xref rid="B1-sensors-15-16981" ref-type="bibr">1</xref>] is a popular image contrast enhancement technique because of its simplicity and effectiveness. The image processed by HE usually has a higher contrast and better visual effects. Although HE can effectively enhance a low-contrast image, it can overstretch the distances between two neighboring gray values of the image and cause the excessive contrast enhancement problem. Furthermore, it can cause the feature loss problem by merging many gray values with small probabilities into a single gray value.</p><p>Many researchers have proposed methods to solve the above-mentioned drawbacks of HE. A few have attempted to solve the excessive contrast enhancement problem. Kim [<xref rid="B2-sensors-15-16981" ref-type="bibr">2</xref>] proposed brightness preserving bi-histogram equalization (BBHE), which divides the histogram of an image into two parts, based on its mean, and equalizes them using HE. Abdullah-Al-Wadud <italic>et al</italic>. [<xref rid="B3-sensors-15-16981" ref-type="bibr">3</xref>,<xref rid="B4-sensors-15-16981" ref-type="bibr">4</xref>] proposed dynamic histogram equalization (DHE), which uses local minima to divide the histogram into several subhistograms. If a subhistogram is not normally distributed, DHE divides it into three parts according to the values of <inline-formula><mml:math id="mm1"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm3"><mml:mi>&#x003bc;</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="mm4"><mml:mi>&#x003c3;</mml:mi></mml:math></inline-formula> are the mean and standard deviation of the subhistogram, respectively. Each subhistogram is then assigned a new dynamic range, and HE is applied to each. Park <italic>et al</italic>. [<xref rid="B5-sensors-15-16981" ref-type="bibr">5</xref>] proposed dynamic range separate histogram equalization (DRSHE), which uses the weighted average of absolute color difference (WAAD) to render the original image more uniformly distributed. DRSHE divides the dynamic range of the histogram into four equal subhistograms and resizes each grayscale range according to its area ratio. Following this, DRSHE uniformly redistributes the intensities of the histogram in the resized grayscale range. Lin <italic>et al</italic>. [<xref rid="B6-sensors-15-16981" ref-type="bibr">6</xref>] proposed statistic-separate tri-histogram equalization (SSTHE), which divides the histogram of an image into three subhistograms based on the mean and standard deviation of the image. The span of each subhistogram is then stretched, and HE is applied to each. Ooi <italic>et al</italic>. [<xref rid="B7-sensors-15-16981" ref-type="bibr">7</xref>] proposed bi-histogram equalization with a plateau level (BHEPL), which is an extension of BBHE. Like BBHE, BHEPL separates the input histogram into two subhistograms based on the mean of the relevant image. It then determines two plateau limits and accordingly clips the two subhistograms in order to avoid over-amplification of noise. Following this, the two subhistograms are separately equalized by utilizing two transform functions. Wu <italic>et al</italic>. [<xref rid="B8-sensors-15-16981" ref-type="bibr">8</xref>] proposed weighting mean-separated sub-histogram equalization (WMSHE) method that divides a histogram of an image into six subhistograms according to the proposed weighting mean function, and performs HE within each subhistogram. All the above methods involve using different methods to segment the histogram into several subhistograms, and then using HE or other equalization methods to enhance the images. They are able to solve the excessive contrast enhancement problem because each subhistogram is restricted to a new range. However, they cannot solve the feature loss problem caused by HE or HE-based methods.</p><p>Furthermore, a growing number of studies have proposed methods to preserve the brightness of images and maintain image quality. Kim proposed BBHE [<xref rid="B2-sensors-15-16981" ref-type="bibr">2</xref>] to maintain a mean value of the enhanced image that is close to that of the input image. Wongsritong <italic>et al</italic>. [<xref rid="B9-sensors-15-16981" ref-type="bibr">9</xref>] proposed multi-peak histogram equalization with brightness preserving (MPHEBP), which uses the peaks of the histogram to divide it into several regions, and performs HE within each region. It can preserve the mean brightness of an input image. Wang <italic>et al</italic>. [<xref rid="B10-sensors-15-16981" ref-type="bibr">10</xref>] proposed equal area dualistic sub-image histogram equalization (DSIHE), which divides an image into two equal area subimages based on its median value, and performs HE within each subimage. The contrast of an image enhanced by the DSIHE method is the average of the segmentation gray level and the middle-gray level of the gray scale of the image. Therefore, DSIHE preserves brightness. Chen <italic>et al.</italic> [<xref rid="B11-sensors-15-16981" ref-type="bibr">11</xref>] proposed a method called recursive mean-separate histogram equalization (RMSHE), which is an extension of BBHE, to preserve the brightness of images. Like BBHE, RMSHE separates the given histogram into two subhistograms using its mean. It performs the division r times. The enhanced image generated by RMSHE can satisfactorily preserve brightness. Chen <italic>et al</italic>. proposed a minimum mean brightness error bi-histogram equalization (MMBEBHE) [<xref rid="B12-sensors-15-16981" ref-type="bibr">12</xref>], which calculates all absolute mean brightness error (AMBE) values for intensity levels 0 to <italic>L</italic> &#x02212; 1, and determines the threshold value that produces the minimum absolute difference between the input and output means. MMBEBHE then separates the entered histogram into two subhistograms based on the threshold value and equalizes them. It can provide maximum brightness preservation of the original image. Wang and Ye [<xref rid="B13-sensors-15-16981" ref-type="bibr">13</xref>] proposed the brightness-preserving histogram equalization with the maximum entropy (BPHEME), which determines a specified histogram that preserves the mean brightness of the original image and has maximum entropy. Therefore, BPHEME can preserve image brightness. Like DSIHE, recursive sub-image histogram equalization (RSIHE) proposed by Sim <italic>et al</italic>. [<xref rid="B14-sensors-15-16981" ref-type="bibr">14</xref>] uses the median value to recursively divide the image r times, and performs HE on each subimage. As in DSIHE, the average brightness of the processed image is the average of the segmentation gray level and the middle-gray level of the grayscale of the image. Thus, RSIHE can preserve brightness. Ibrahim <italic>et al</italic>. [<xref rid="B15-sensors-15-16981" ref-type="bibr">15</xref>] proposed brightness-preserving dynamic histogram equalization (BPDHE), which is an extension of MPHEBP [<xref rid="B9-sensors-15-16981" ref-type="bibr">9</xref>] and DHE [<xref rid="B3-sensors-15-16981" ref-type="bibr">3</xref>,<xref rid="B4-sensors-15-16981" ref-type="bibr">4</xref>]. Like MPHEBP, BPDHE segments a histogram based on the local maxima of the smoothed histogram. Before equalizing each segment, it maps it to a new dynamic range. This process is similar to that used in DHE. The average intensity of the resultant image of BPDHE is nearly the same as the one of the input image. Wang <italic>et al</italic>. [<xref rid="B16-sensors-15-16981" ref-type="bibr">16</xref>] proposed flattest histogram specification with accurate brightness preservation (FHSABP), which tries to determine the optimal histogram, the flattest one with the mean brightness constraint. FHSABP then uses an exact histogram specification to obtain better brightness preservation. Ooi <italic>et al</italic>. [<xref rid="B17-sensors-15-16981" ref-type="bibr">17</xref>] proposed dynamic quadrants histogram equalization plateau limit (DQHEPL), which divides a histogram based on its median and iteratively produces four subhistograms. DQHEPL then calculates each plateau limit, and clips each subhistogram by its plateau limit. Following this, each subhistogram is assigned a new dynamic range and HE is applied to each. The images processed by DQHEPL can maintain mean brightness. Thomas <italic>et al</italic>. [<xref rid="B18-sensors-15-16981" ref-type="bibr">18</xref>] adopted the concepts of BPHEME [<xref rid="B13-sensors-15-16981" ref-type="bibr">13</xref>] and piecewise linear transformation (PLT) [<xref rid="B19-sensors-15-16981" ref-type="bibr">19</xref>] to propose a piecewise maximum entropy (PME) method. PME uses the piecewise transformation function to avoid a mean value too far from the original mean and maximizes entropy. The resulting image processed by PME preserves the original brightness quite well. All the above methods attempt to overcome the drawback of significant changes in brightness caused by HE by maintaining the brightness of the input image as far as possible in order to enhance it. They can generate images that retain almost the same brightness as that of the original. However, when the input image is underexposed or overexposed, maintaining its brightness is not reasonable because it is unsuitable for human visual perception.</p><p>Therefore, in this paper, a visual contrast enhancement algorithm (VCEA) considering the characteristics of human visual perception is proposed. This algorithm mitigates the excessive contrast enhancement and the feature loss problems of HE. Furthermore, VCEA enhances the detailed textures of an image. Images processed by VCEA have better visual quality and are better suited to human visual perception than those processed by HE and other HE-based methods.</p><p>This paper is organized as follows. The proposed VCEA algorithm is introduced in <xref ref-type="sec" rid="sec2-sensors-15-16981">Section 2</xref>. <xref ref-type="sec" rid="sec3-sensors-15-16981">Section 3</xref> is devoted to experimental results to compare the performance of VCEA with HE and other HE-based methods. Finally, conclusions are provided in <xref ref-type="sec" rid="sec4-sensors-15-16981">Section 4</xref>.</p></sec><sec id="sec2-sensors-15-16981"><title>2. Visual Contrast Enhancement Algorithm (VCEA)</title><p>Histogram equalization (HE) is a well-known technique to enhance the contrast of images because of its simplicity and effectiveness. However, HE is rarely applied directly to consumer electronics products because it can cause the excessive contrast enhancement and feature loss problems. Although many research studies have proposed methods to overcome the excessive contrast enhancement problem in HE, they have not considered the problem of the compression of gray values, which results in the loss of a few features in the enhanced image. Starting from the strategy adopted by past studies in the area, a visual contrast enhancement algorithm (VCEA) based on HE is proposed. It considers the requirements of the human visual perception in order to solve the excessive contrast enhancement problem and the feature loss problem caused by HE. Furthermore, VCEA enhances the detailed textures of images and improves the quality of enhanced images.</p><fig id="sensors-15-16981-f001" position="float"><label>Figure 1</label><caption><p>Functional block diagram of visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g001"/></fig><p>VCEA has three major processes: just-noticeable difference contrast adjustment (JNDCA), compressed pixel recovery (CPR), and detailed texture enhancement (DTE). The functional block diagram of VCEA is shown in <xref ref-type="fig" rid="sensors-15-16981-f001">Figure 1</xref>. The details of the three processes of VCEA are as follows:</p><sec><title>2.1. Just-Noticeable Difference Contrast Adjustment (JNDCA)</title><p>The main purpose of the JNDCA process is to address the excessive enhancement problem of overstretched space between two adjacent gray values caused by HE and enable the enhanced image to satisfy the requirements of human visual perception. Before introducing the JNDCA process, just-noticeable difference (JND) needs to be clarified.</p><p>JND is a quantitative measure used to distinguish the luminance change perceived by the human visual system. It is defined as the amount of light <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>&#x02206;</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> necessary to add to a visual field of intensity <italic>B</italic> such that it can be discriminated from the background [<xref rid="B20-sensors-15-16981" ref-type="bibr">20</xref>,<xref rid="B21-sensors-15-16981" ref-type="bibr">21</xref>]. It has been widely used in different applications, such as watermarking, image enhancement, data hiding, <italic>etc</italic>., in recent years. Lie and Chang [<xref rid="B21-sensors-15-16981" ref-type="bibr">21</xref>] proposed the least-significant bit (LSB) mapping function, which provides the number of LSBs embedded for each gray value according to the sensitivity of human visual perception to changes in image contrast. In this paper, a space adjustment function <italic>S</italic>(<italic>x</italic>) is referred to the function proposed by Lie and Chang and devised, which shortens the spaces between two adjacent gray values of the HE histogram to satisfy the minimum discernment requirements of human visual perception for the contrast change in each gray value. The space adjustment function is as follows:
<disp-formula id="FD1"><label>(1)</label><mml:math id="mm6"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>85</mml:mn></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>86</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>182</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>x</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>183</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>255</mml:mn></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The JNDCA process uses the space adjustment function to adjust the spaces between adjacent gray values in order to improve the excessive contrast enhancement problem of HE. It is assumed that <italic>HEhist</italic>(<italic>x</italic>) is the total number of pixels of the HE histogram at gray value <italic>x</italic>, where <italic>x</italic> ranges from 0 to 255, and the space between gray value <italic>x</italic> &#x02212; 1 and <italic>x</italic> is <italic>l</italic>. When <italic>l</italic> is greater than the space adjustment function <italic>S</italic>(<italic>x</italic>), <italic>HEhist</italic>(<italic>x</italic>) is shifted back by <italic>l</italic> &#x02212; <italic>S</italic>(<italic>x</italic>) gray levels. On the contrary, when <italic>l</italic> is equal to or less than <italic>S</italic>(<italic>x</italic>), it implies that no space to be adjusted; thus, <italic>HEhist</italic>(<italic>x</italic>) remains at its original location. Once all <italic>HEhist</italic>(<italic>x</italic>) have sequentially been shifted back, the JNDCA image and available spaces are obtained.</p><p><xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>a shows an underexposed image containing 119 gray values. The image in <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>c is processed by HE and suffers from an excessive enhancement problem: the door, floor, rain shelter, <italic>etc</italic>., are over-enhanced. Moreover, this image has only 54 gray values because multiple gray values are compressed because of HE. As a result, <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>c suffers from the feature loss problem, which causes the textures of the rain shelter to disappear. <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>e shows the JNDCA image obtained by applying the JNDCA process. It contains 54 gray values, which is the same as that in <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>c. Thus, this image satisfies the minimum discrimination requirement of human visual perception. Through space adjustment between two neighboring gray values, <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>e shows improvement in the excessive contrast enhancement problem caused by HE. <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>b,d, and f are the histograms of the luminance (Y) component of <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>a,c, and e, respectively. The obtained available spaces, &#x0201c;free spaces,&#x0201d; are used in the following processes for further enhancement of image quality.</p><fig id="sensors-15-16981-f002" position="float"><label>Figure 2</label><caption><p>&#x0201c;Indoor View&#x0201d; [<xref rid="B22-sensors-15-16981" ref-type="bibr">22</xref>] (image size: 640 &#x000d7; 428 pixels) processed by just-noticeable difference contrast adjustment (JNDCA). (<bold>a</bold>) Original image; (<bold>b</bold>) Original histogram; (<bold>c</bold>) Histogram Equalization (HE) image; (<bold>d</bold>) HE histogram; (<bold>e</bold>) JNDCA image; (<bold>f</bold>) JNDCA histogram.</p></caption><graphic xlink:href="sensors-15-16981-g002"/></fig></sec><sec><title>2.2. Compressed Pixel Recovery (CPR)</title><p>The CPR process mainly addresses the feature loss problem caused by HE or HE-based methods. The principle underlying HE is the enhancement of the contrast of an image by stretching its dynamic range from gray level 0 to 255 based on the cumulative distribution function (CDF). When the cumulative probability of a certain gray value is less than 1/255, the gray value is not allocated a gray level space and is merged into other gray values. In this condition, many gray values are merged into a specific gray value, which leads to the feature loss problem. To address the problem, the CPR process uses free spaces to recover as many compressed gray values as possible in order to regain the lost features in the enhanced image.</p><p>The CPR process is as follows. It is assumed that <italic>JNDCAhist</italic>(<italic>x</italic>), <italic>CPRhist</italic>(<italic>x</italic>), and <italic>histogram</italic>(<italic>x</italic>) are the total number of pixels in the JNDCA, CPR, and the original histograms at gray value <italic>x</italic>, respectively, where <italic>x</italic> ranges from 0 to 255. The CPR process first compares the JNDCA histogram with the original histogram. When <italic>JNDCAhist</italic>(<italic>x</italic>) is not zero, the CPR process determines the range of gray levels of the original histogram containing the sum of pixels, which is equal to <italic>JNDCAhist</italic>(<italic>x</italic>)<italic>.</italic> Following this, the CPR process recovers the pixels in the particular range of gray levels from the original histogram, and repeats the same task until all the free spaces are used up. For example, when the value of <italic>JNDCAhist</italic>(<italic>x</italic>) at gray value <italic>x</italic> is equal to the cumulative pixels from gray level <italic>y</italic> to <italic>z</italic> of the original histogram,
<disp-formula id="FD2"><label>(2)</label><mml:math id="mm7"><mml:mrow><mml:mi>J</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:munderover><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Having recovered the lost features, gray value <italic>x</italic> assumes the range <inline-formula><mml:math id="mm8"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;&#x000a0;</mml:mtext><mml:mi>x</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>+</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>x</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>+</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the CPR image. The pixels of each recovered gray value <italic>x</italic> can be expressed as:
<disp-formula id="FD3"><label>(3)</label><mml:math id="mm9"><mml:mrow><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>.....</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>a is obtained by applying the CPR process. Through this process, many compressed gray values such as the textures of the rain shelter are recovered. <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>a contains 119 gray values, which is the same number as that in the original image. The CPR process effectively mitigates the feature loss problem caused by HE. It also makes <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>a appear better than the JNDCA image because the lost features are recovered in the CPR image. <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>b is the histogram of the luminance (Y) component of <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>a. Because of the recovery of the compressed gray values, the number of gray values in <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>b is more than that in <xref ref-type="fig" rid="sensors-15-16981-f002">Figure 2</xref>f.</p><p>Most images obtained by applying the JNDCA and CPR processes recover most of their lost features, and have better visual enhancement effects. The enhanced images usually have no remaining free spaces. If free spaces remain in an image, the DTE process is applied to enhance the detailed textures of the image.</p><fig id="sensors-15-16981-f003" position="float"><label>Figure 3</label><caption><p>&#x0201c;Indoor View&#x0201d; processed by compressed pixel recovery (CPR). (<bold>a</bold>) CPR image; (<bold>b</bold>) CPR histogram.</p></caption><graphic xlink:href="sensors-15-16981-g003"/></fig></sec><sec><title>2.3. Detailed Texture Enhancement (DTE)</title><p>The third process of VCEA is DTE. The main purpose of DTE is to enhance the detailed textures of an image and make them look clearer. It is usually not easy for people to discern the detailed textures with a few pixels of an image. Thus, the DTE process focuses on those textures for further enhancement.</p><p>The DTE process first calculates the gradient value of each pixel, which is the sum of horizontal and vertical gradients. For example, <inline-formula><mml:math id="mm10"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the pixel value of a pixel located at <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in an image with <inline-formula><mml:math id="mm12"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> pixels. The gradient value <inline-formula><mml:math id="mm13"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm14"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is as follows:
<disp-formula id="FD4"><label>(4)</label><mml:math id="mm15"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#x02009;</mml:mtext><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>The DTE process then accumulates the total gradient value and the count of each gray value. It is assumed that <italic>G</italic>(<italic>x</italic>) and <italic>count</italic>(<italic>x</italic>) denote the total gradient value and the count at gray value <italic>x</italic>, respectively. The average gradient value at gray value <italic>x</italic>, <italic>avg</italic>(<italic>x</italic>), is equal to <italic>G</italic>(<italic>x</italic>) divided by <italic>count</italic>(<italic>x</italic>), and is expressed:
<disp-formula id="FD5"><label>(5)</label><mml:math id="mm16"><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mtext>(</mml:mtext><mml:mi>x</mml:mi><mml:mtext>)</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mtext>(</mml:mtext><mml:mi>x</mml:mi><mml:mtext>)</mml:mtext></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext>(</mml:mtext><mml:mi>x</mml:mi><mml:mtext>)</mml:mtext><mml:mo>,</mml:mo><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the DTE process calculates the mean and the standard deviation gradient of the image. It is assumed that <inline-formula><mml:math id="mm17"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm18"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the mean and the standard deviation gradient of the image, respectively, and are expressed as follows:
<disp-formula id="FD6"><label>(6)</label><mml:math id="mm19"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7"><label>(7)</label><mml:math id="mm20"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>Following this, the DTE process determines the candidate gray values to be further enhanced. Here, DTE uses the gradient value as the basis to determine the candidate gray values in order to enhance the detailed textures. This is because a larger gradient value indicates that the relevant pixel is significantly different from adjacent pixels and is much easier to discriminate from them. On the contrary, a small gradient value indicates that the relevant pixel is similar to adjacent pixels and thus is hard to discriminate from them. To render the enhanced effect more obvious, the values of the total number of pixels of the candidate gray values cannot be small. They must be greater than the threshold value, <inline-formula><mml:math id="mm21"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, where <italic>M</italic> and <italic>N</italic> denote the height and width of an image, respectively. At the same time, the average gradient value of the candidate gray value has to be less than the specific value, which is the absolute value of the difference between <inline-formula><mml:math id="mm22"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Having obtained the qualified candidate gray values, the DTE process sorts them by their average gradient values, and sequentially enhances the candidate gray value with the greater average gradient until all the remaining free spaces are used up. For example, it is assumed that <italic>y</italic> is the first candidate gray value of the CPR histogram to be enhanced. The space between <italic>y</italic> &#x02212; 1 and <italic>y</italic> is <italic>d</italic> gray levels, and <italic>CPRhist</italic>(<italic>y</italic>) denotes the histogram of the CPR image at gray value <italic>y.</italic> When <italic>d</italic> is greater than <italic>S</italic>(<italic>y</italic>), which is the space adjustment function introduced earlier, <italic>CPRhist</italic>(<italic>y</italic>) is shifted back by <italic>d</italic> &#x02212; <italic>S</italic>(<italic>y</italic>) gray levels; conversely, when <italic>d</italic> is equal to or less than <italic>S</italic>(<italic>y</italic>)<italic>,</italic>
<italic>CPRhist</italic>(<italic>y</italic>) is shifted forward by <italic>S</italic>(<italic>y</italic>) &#x02212; <italic>d</italic> gray levels. Once all <italic>CPRhist</italic>(<italic>y</italic>) have been sequentially shifted back or forward, the DTE image is obtained. <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>a is the image processed by using DTE, and it contains 119 gray values. Through the DTE process, detailed textures such as grass, trees on the left and right side, and the view behind the door in <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>a are enhanced. This process also makes the image appear much clearer than the CPR image, indicating that the DTE process can effectively enhance the detailed textures of images. <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>b is the histogram of the luminance (Y) component of <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>a. It is clear that the dynamic range observed in <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>b is wider than that observed in <xref ref-type="fig" rid="sensors-15-16981-f003">Figure 3</xref>b after the DTE process. Therefore, <xref ref-type="fig" rid="sensors-15-16981-f004">Figure 4</xref>a has better image quality. In addition, in this process, all relevant variables are automatically calculated according to the input images, and no parameters need to be tuned manually.</p><fig id="sensors-15-16981-f004" position="float"><label>Figure 4</label><caption><p>&#x0201c;Indoor View&#x0201d; processed by detailed texture enhancement (DTE). (<bold>a</bold>) DTE image; (<bold>b</bold>) DTE histogram.</p></caption><graphic xlink:href="sensors-15-16981-g004"/></fig></sec></sec><sec id="sec3-sensors-15-16981"><title>3. Experimental Results</title><p><xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>, <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>, <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>, <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref> and <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref> show experimental results for VCEA in comparison with those for HE [<xref rid="B1-sensors-15-16981" ref-type="bibr">1</xref>] and other HE-based methods: brightness-preserving bi-histogram equalization (BBHE) [<xref rid="B2-sensors-15-16981" ref-type="bibr">2</xref>], recursive mean-separate histogram equalization (RMSHE) [<xref rid="B11-sensors-15-16981" ref-type="bibr">11</xref>], equal area dualistic sub-image histogram equalization (DSIHE) [<xref rid="B10-sensors-15-16981" ref-type="bibr">10</xref>], recursive sub-image histogram equalization (RSIHE) [<xref rid="B14-sensors-15-16981" ref-type="bibr">14</xref>], bi-histogram equalization with a plateau level (BHEPL) [<xref rid="B7-sensors-15-16981" ref-type="bibr">7</xref>], and dynamic quadrants histogram equalization plateau limit (DQHEPL) [<xref rid="B17-sensors-15-16981" ref-type="bibr">17</xref>].</p><p><xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>a shows an original image that was underexposed. It contains 119 gray values. <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>b shows the image following the processing by using HE. Due to the feature loss problem caused by HE, <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>b only contains 54 gray values. This results in the disappearance of the textures of the rain shelter. In addition, the door and rain shelter in the image are over-enhanced, making the colors in the image appear unnatural, particularly the color of the door. <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>c,e show the results following the application of BBHE and DSIHE, respectively. They exhibited the same problem of the excessively dark appearance of dark regions and the extremely bright appearance of bright ones. Because of this, many details in the dark and bright regions were not visible. <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>d,f were obtained by applying RMSHE and RSIHE, respectively. These had the color distortion problem that made the color of the floor appear very unnatural. <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>g,h are the results of processing through BHEPL and DQHEPL, respectively. These appeared too dark, and this rendered invisible some details in the dark regions of the images. However, <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>i, the image obtained by applying VCEA, has the same number of gray values as the original image. VCEA not only solves the over enhancement problem caused by HE but also recovers the compressed gray values to make the textures of the rain shelter reappear. It makes <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>i show the details in the dark regions most clearly. The image appears more natural, and has higher contrast. In addition, it is suitable for human visual perception.</p><fig id="sensors-15-16981-f005" position="float"><label>Figure 5</label><caption><p>Comparison results for the image &#x0201c;Indoor View&#x0201d; (image size: 640 &#x000d7; 428 pixels). (<bold>a</bold>) Original image; (<bold>b</bold>) Histogram Equalization (HE); (<bold>c</bold>) Bi-histogram equalization (BBHE); (<bold>d</bold>) Recursive mean-separate histogram equalization (RMSHE) (<italic>r</italic> = 2); (<bold>e</bold>) Dualistic sub-image histogram equalization (DSIHE); (<bold>f</bold>) Recursive sub-image histogram equalization (RSIHE) (<italic>r</italic> = 2); (<bold>g</bold>) Bi-histogram equalization with a plateau level (BHEPL); (<bold>h</bold>) Dynamic quadrants histogram equalization plateau limit (DQHEPL); (<bold>i</bold>) Visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g005"/></fig><p><xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>a shows an underexposed image, which contains 205 gray values. <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>b shows the image obtained as a result of processing the original image using HE. It contains only 66 gray values and has the feature loss problem. For example, the textures of the house disappear. The image is over-enhanced, and produces the excessive contrast enhancement problem that causes the grass on the road, the leaves on the trees, and the house to become too bright to see. <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>c,d, and f show the results of applying BBHE, RMSHE, and RSIHE, respectively to the original image. These exhibited the same problem whereby some regions, like the grass and leaves, appeared unnatural. <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>e shows the result of processing the original image using DSIHE, and appears to have the same problem as that encountered in HE processing, <italic>i.e</italic>., some regions, such as the grass and leaves, are too bright to be seen. <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>g,h were obtained by applying BHEPL and DQHEPL, respectively. The resulting images are extremely dark, and details such as the grass and leaves cannot be seen clearly as a consequence. However, <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>i, which is the result of applying VCEA, contains 190 gray values. Compared to other images, it has the largest number of gray values. The grass, leaves, and house can be seen clearly. The image appears more natural and has higher contrast than that obtained using the other methods. In addition, the obtained image is suitable for human visual perception.</p><fig id="sensors-15-16981-f006" position="float"><label>Figure 6</label><caption><p>Comparison results for the image &#x0201c;Landscape&#x0201d; [<xref rid="B23-sensors-15-16981" ref-type="bibr">23</xref>] (image size: 596 &#x000d7; 397 pixels). (<bold>a</bold>) Original image; (<bold>b</bold>) Histogram Equalization (HE); (<bold>c</bold>) Bi-histogram equalization (BBHE); (<bold>d</bold>) Recursive mean-separate histogram equalization (RMSHE) (<italic>r</italic> = 2); (<bold>e</bold>) Dualistic sub-image histogram equalization (DSIHE); (<bold>f</bold>) Recursive sub-image histogram equalization (RSIHE) (<italic>r</italic> = 2); (<bold>g</bold>) Bi-histogram equalization with a plateau level (BHEPL); (<bold>h</bold>) Dynamic quadrants histogram equalization plateau limit (DQHEPL); (<bold>i</bold>) Visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g006"/></fig><p><xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>a is an underexposed image as well. It contains 218 gray values. <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>b&#x02013;g represent images resulting from the application of HE, BBHE, RMSHE, DSIHE, RSIHE, and BHEPL, respectively. They exhibit the same problem of unpleasant visual artifacts in the background. <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>e,f suffer from the color distortion problem, which results in enhanced images appearing unnatural, especially the color of the face. <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>h, obtained by applying DQHEPL, yields a better result than the other methods but is a bit dark. Among all the comparison methods, <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>g,h) have 162 and 187 gray values, respectively. They have more gray values than <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>i, which contains 158 gray values. However, <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>i, the image resulting from the application of VCEA, is the clearest and contains no unpleasant visual artifacts in the background. It looks more natural than images obtained by using the other methods.</p><fig id="sensors-15-16981-f007" position="float"><label>Figure 7</label><caption><p>Comparison results for the image &#x0201c;Girl&#x0201d; [<xref rid="B24-sensors-15-16981" ref-type="bibr">24</xref>] (image size: 1200 &#x000d7; 800 pixels). (<bold>a</bold>) Original image; (<bold>b</bold>) Histogram Equalization (HE); (<bold>c</bold>) Bi-histogram equalization (BBHE); (<bold>d</bold>) Recursive mean-separate histogram equalization (RMSHE) (<italic>r</italic> = 2); (<bold>e</bold>) Dualistic sub-image histogram equalization (DSIHE); (<bold>f</bold>) Recursive sub-image histogram equalization (RSIHE) (<italic>r</italic> = 2); (<bold>g</bold>) Bi-histogram equalization with a plateau level (BHEPL); (<bold>h</bold>) Dynamic quadrants histogram equalization plateau limit (DQHEPL); (<bold>i</bold>) Visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g007"/></fig><p><xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>a shows an original underexposed image, which contains 217 gray values. <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>b, processed using HE, contains 113 gray values. It is over-enhanced, and produces excessive contrast enhancement, whereby the outdoor view is too bright to be seen. Furthermore, the number of gray values decreases and results in the feature loss problem that the textures of the things on the desk, the grass on the ground, the view, wall, and trees outside the window are difficult to be seen. <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>c, the result of applying BBHE, is better than the original image. The objects on the bookshelf in <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>c are clearer than in the original image, but are still too dark to see. <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>d&#x02013;h show the results of applying RMSHE, DSIHE, RSIHE, BHEPL, and DQHEPL, respectively. They exhibit the same problem, whereby the objects on the bookshelf are too dark to see. However, <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>i, the image obtained by applying VCEA, contains 191 gray values. It has the second largest number of gray values among all the images using other comparison methods. Compared to <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>h, which has 196 gray values, <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>i shows more clearly the objects on the bookshelf, as well as the outdoor view. In comparison with images obtained by the other methods, this one appears more natural and has a better enhancement effect.</p><fig id="sensors-15-16981-f008" position="float"><label>Figure 8</label><caption><p>Comparison results for the image &#x0201c;Window View&#x0201d; [<xref rid="B25-sensors-15-16981" ref-type="bibr">25</xref>] (image size: 752 &#x000d7; 500 pixels). (<bold>a</bold>) Original image; (<bold>b</bold>) Histogram Equalization (HE); (<bold>c</bold>) Bi-histogram equalization (BBHE); (<bold>d</bold>) Recursive mean-separate histogram equalization (RMSHE) (<italic>r</italic> = 2); (<bold>e</bold>) Dualistic sub-image histogram equalization (DSIHE); (<bold>f</bold>) Recursive sub-image histogram equalization (RSIHE) (<italic>r</italic> = 2); (<bold>g</bold>) Bi-histogram equalization with a plateau level (BHEPL); (<bold>h</bold>) Dynamic quadrants histogram equalization plateau limit (DQHEPL); (<bold>i</bold>) Visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g008"/></fig><p><xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>a shows an original overexposed image, which contains 213 gray values. <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>b shows the image processed using HE. It contains 124 gray values. The back of the chair in <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>b is too dark to be seen clearly and some features, such as the textures of the chair back and the paper tray, are lost due to the feature loss problem of HE. <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>c,e show the results after the application of BBHE and DSIHE, respectively. The feature loss problem occurs in these images as well because of which the back of the chair is not as clear as the original one. <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>d,f&#x02013;h show the results after the application of RMSHE, RSIHE, BHEPL, and DQHEPL, respectively. Here, the outdoor view and the blinds are too bright to be seen clearly. <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>i, which is processed by applying VCEA, contains 160 gray values. Although it has fewer gray values than the ones processed by RMSHE, RSIHE, BHEPL, and DQHEPL, it shows an image, where the blinds and the outdoor view are clearer than those shown in <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>a&#x02013;h. In comparison with images obtained by the other methods, the image processed by VCEA appears more natural and has superior enhancement effects.</p><p>In summary, <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>, <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>, <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>, <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref> and <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref> indicate clearly that VCEA has superior enhancement effects to the other methods that were tested. VCEA not only improves the drawbacks of HE, namely, the excessive contrast enhancement problem and the feature loss problem, but also lends better visual effects and a more natural look to the image. It can also enhance detained textures of images and render them clearer. Compared with HE and other HE-based methods, VCEA produces enhanced images that have superior visual quality and are suitable for human visual perception.</p><p>In addition to the above subjective evaluation of the enhancement effect through observation, discrete entropy [<xref rid="B26-sensors-15-16981" ref-type="bibr">26</xref>] is used in this study to quantitatively evaluate the effectiveness of the proposed algorithm. It mainly evaluates the capability of the proposed method and other comparison methods for extracting details from images. Discrete entropy <italic>E</italic>(<italic>y</italic>) is defined as:
<disp-formula id="FD8"><label>(8)</label><mml:math id="mm24"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the probability of the <italic>i-</italic>th gray level. The higher the entropy value, the more information is extracted from images. The discrete entropy values calculated for different methods are listed in <xref ref-type="table" rid="sensors-15-16981-t001">Table 1</xref>.</p><p>Both subjective and objective assessments are usually used to evaluate the effects of image enhancement. However, researchers often use objective quality assessment, producing results that may not correlate well with human visual perception. Thus, subjective assessment is regarded as the more reliable method for assessing image quality because it measures the most direct response from end users. Objective assessment provides readers quantitative information; however, quantitative information is not enough for people to evaluate the effects of image enhancement. It must be accompanied by subjective assessments. When subjective and objective assessments are not consistent, subjective assessments become more important especially in evaluating the effects of image enhancement.</p><p>As seen in <xref ref-type="table" rid="sensors-15-16981-t001">Table 1</xref>, VCEA shows the highest entropy for <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref> and <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>, indicating that VCEA extracts considerable information from the original images. <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>i has higher contrast and is not over-enhanced. The textures such as the grass on the left and right sides of <xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>i and the trees behind the door are clearer. <xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>i also has higher contrast. Textures such as the grass and trees are much clearer. The image is not over-enhanced, either. Therefore, in both objective and subjective assessments, VCEA outperforms the other comparison methods and exhibits a better enhancement effect.</p><fig id="sensors-15-16981-f009" position="float"><label>Figure 9</label><caption><p>Comparison results for the image &#x0201c;Office&#x0201d; [<xref rid="B27-sensors-15-16981" ref-type="bibr">27</xref>] (image size: 903 &#x000d7; 600 pixels). (<bold>a</bold>) Original image; (<bold>b</bold>) Histogram Equalization (HE); (<bold>c</bold>) Bi-histogram equalization (BBHE); (<bold>d</bold>) Recursive mean-separate histogram equalization (RMSHE) (<italic>r</italic> = 2); (<bold>e</bold>) Dualistic sub-image histogram equalization (DSIHE); (<bold>f</bold>) Recursive sub-image histogram equalization (RSIHE) (<italic>r</italic> = 2); (<bold>g</bold>) Bi-histogram equalization with a plateau level (BHEPL); (<bold>h</bold>) Dynamic quadrants histogram equalization plateau limit (DQHEPL); (<bold>i</bold>) Visual contrast enhancement algorithm (VCEA).</p></caption><graphic xlink:href="sensors-15-16981-g009"/></fig><table-wrap id="sensors-15-16981-t001" position="float"><object-id pub-id-type="pii">sensors-15-16981-t001_Table 1</object-id><label>Table 1</label><caption><p>Calculated discrete entropy values for the compared methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Method</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Indoor View (<xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Landscape (<xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Girl (<xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Window View (<xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Office (<xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">HE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.119617</td><td align="center" valign="middle" rowspan="1" colspan="1">5.286821</td><td align="center" valign="middle" rowspan="1" colspan="1">5.968813</td><td align="center" valign="middle" rowspan="1" colspan="1">5.829778</td><td align="center" valign="middle" rowspan="1" colspan="1">6.428654</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BBHE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.168818</td><td align="center" valign="middle" rowspan="1" colspan="1">5.425362</td><td align="center" valign="middle" rowspan="1" colspan="1">5.993828</td><td align="center" valign="middle" rowspan="1" colspan="1">5.900343</td><td align="center" valign="middle" rowspan="1" colspan="1">6.498654</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RMSHE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.103576</td><td align="center" valign="middle" rowspan="1" colspan="1">5.401507</td><td align="center" valign="middle" rowspan="1" colspan="1">6.097461</td><td align="center" valign="middle" rowspan="1" colspan="1">5.927119</td><td align="center" valign="middle" rowspan="1" colspan="1">6.512488</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSIHE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.162673</td><td align="center" valign="middle" rowspan="1" colspan="1">5.374285</td><td align="center" valign="middle" rowspan="1" colspan="1">6.022877</td><td align="center" valign="middle" rowspan="1" colspan="1">5.989769</td><td align="center" valign="middle" rowspan="1" colspan="1">6.473243</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RSIHE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.108531</td><td align="center" valign="middle" rowspan="1" colspan="1">5.379086</td><td align="center" valign="middle" rowspan="1" colspan="1">6.054375</td><td align="center" valign="middle" rowspan="1" colspan="1">5.926646</td><td align="center" valign="middle" rowspan="1" colspan="1">6.5258</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BHEPL</td><td align="center" valign="middle" rowspan="1" colspan="1">5.224879</td><td align="center" valign="middle" rowspan="1" colspan="1">5.477687</td><td align="center" valign="middle" rowspan="1" colspan="1">6.139931</td><td align="center" valign="middle" rowspan="1" colspan="1">6.038548</td><td align="center" valign="middle" rowspan="1" colspan="1">6.627142</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DQHEPL</td><td align="center" valign="middle" rowspan="1" colspan="1">5.180584</td><td align="center" valign="middle" rowspan="1" colspan="1">5.479637</td><td align="center" valign="middle" rowspan="1" colspan="1">6.186656</td><td align="center" valign="middle" rowspan="1" colspan="1">6.076967</td><td align="center" valign="middle" rowspan="1" colspan="1">6.633046</td></tr><tr><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">VCEA</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">5.231328</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">5.483696</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">6.070287</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">6.068298</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">6.506406</td></tr></tbody></table></table-wrap><p>In addition, VCEA has the fourth highest entropy in <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>, the second highest entropy in <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>, and the fifth highest entropy in <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>. Although VCEA cannot extract more details from those images through objective assessment; however, VCEA has better enhancement effects in subjective assessment. For example, in <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>i, the face and hair of the girl are much clearer. There is no artifact, such as false contours, shown in <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>b,c,e. <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>i is more natural and has better enhancement effects than the ones that have higher entropies. In <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>i, the entropy is lower than that in <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>h. However, the detail textures in the dark area of the image such as the items on the bookshelf can be seen. The enhancement effect of <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>i is much better than that of <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>h and other comparison methods. In <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>, the entropy of <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>i is also lower than the ones of <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>d,f&#x02013;h. However, compared to the contrast of these images, the contrast of <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>i is higher. The outdoor view and the small image on the screen are much clearer. <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>i has better enhancement effects.</p><p>In addition to quantitative analyses, in order to demonstrate the superiority of VCEA in subjective assessments, an experiment called &#x0201c;Subjective Image Quality Assessment Test&#x0201d; was designed and conducted by us according to the standard ITU-T P.910 (04/2008)&#x02014;Subjective video quality assessment methods for multimedia applications. The purpose of the experiment was to provide more subjective assessments for each image from unknown subjects. In this experiment, the absolute category rating (ACR), which is one of the most popular subjective measures in a quality test, was adopted and standardized for images and video in ITU-T P.910. The five-level scale&#x02014;bad (1), poor (2), fair (3), good (4), and excellent (5)&#x02014;was used to rate the overall quality of the image. Thirty volunteers without receiving any image processing training on campus were recruited to deliver their assessments. Ten subjects took the &#x0201c;Subjective Image Quality Assessment Test&#x0201d; at a time. They were given the same instructions and 10 s to look at each image. Then, they had to score each image within 10 s. The total scores of all images for different methods are listed in <xref ref-type="table" rid="sensors-15-16981-t002">Table 2</xref>. As seen in <xref ref-type="table" rid="sensors-15-16981-t002">Table 2</xref>, VCEA not only shows the highest scores for each image, but also has the highest total score among all the methods. It indicates that the image processed by VCEA has better image quality than the ones obtained by the other methods.</p><table-wrap id="sensors-15-16981-t002" position="float"><object-id pub-id-type="pii">sensors-15-16981-t002_Table 2</object-id><label>Table 2</label><caption><p>Calculated scores for each image processed by the compared methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Method</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Indoor View (<xref ref-type="fig" rid="sensors-15-16981-f005">Figure 5</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Landscape (<xref ref-type="fig" rid="sensors-15-16981-f006">Figure 6</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Girl (<xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Window View (<xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Office (<xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref>)</th><th style="border-top:solid thin;border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">Sum</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">HE</td><td align="center" valign="middle" rowspan="1" colspan="1">116</td><td align="center" valign="middle" rowspan="1" colspan="1">97</td><td align="center" valign="middle" rowspan="1" colspan="1">108</td><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">129</td><td align="center" valign="middle" rowspan="1" colspan="1">575</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BBHE</td><td align="center" valign="middle" rowspan="1" colspan="1">70</td><td align="center" valign="middle" rowspan="1" colspan="1">90</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">79</td><td align="center" valign="middle" rowspan="1" colspan="1">70</td><td align="center" valign="middle" rowspan="1" colspan="1">402</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RMSHE</td><td align="center" valign="middle" rowspan="1" colspan="1">41</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">71</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">76</td><td align="center" valign="middle" rowspan="1" colspan="1">362</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSIHE</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">90</td><td align="center" valign="middle" rowspan="1" colspan="1">54</td><td align="center" valign="middle" rowspan="1" colspan="1">77</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">402</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RSIHE</td><td align="center" valign="middle" rowspan="1" colspan="1">61</td><td align="center" valign="middle" rowspan="1" colspan="1">96</td><td align="center" valign="middle" rowspan="1" colspan="1">38</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">347</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BHEPL</td><td align="center" valign="middle" rowspan="1" colspan="1">103</td><td align="center" valign="middle" rowspan="1" colspan="1">76</td><td align="center" valign="middle" rowspan="1" colspan="1">89</td><td align="center" valign="middle" rowspan="1" colspan="1">77</td><td align="center" valign="middle" rowspan="1" colspan="1">82</td><td align="center" valign="middle" rowspan="1" colspan="1">427</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DQHEPL</td><td align="center" valign="middle" rowspan="1" colspan="1">106</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">76</td><td align="center" valign="middle" rowspan="1" colspan="1">84</td><td align="center" valign="middle" rowspan="1" colspan="1">96</td><td align="center" valign="middle" rowspan="1" colspan="1">461</td></tr><tr><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">VCEA</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">133</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">140</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">132</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">129</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">134</td><td style="border-bottom:solid thin" align="center" valign="middle" rowspan="1" colspan="1">668</td></tr></tbody></table></table-wrap><p>To sum up, although VCEA extracts fewer details from <xref ref-type="fig" rid="sensors-15-16981-f007">Figure 7</xref>, <xref ref-type="fig" rid="sensors-15-16981-f008">Figure 8</xref> and <xref ref-type="fig" rid="sensors-15-16981-f009">Figure 9</xref> compared to those extracted by other methods, the VCEA image is clearer and has higher contrast; the detailed textures are much clearer as well. Through subjective assessments conducted using 30 unknown subjects, it was shown that VCEA has the highest score for each image. This implies that the image processed by VCEA has better image quality than the ones obtained by the other methods. Overall, the subjective and objective analyses indicate that VCEA outperforms other methods and has a better contrast enhancement effect.</p></sec><sec id="sec4-sensors-15-16981"><title>4. Conclusions</title><p>In this paper, a contrast enhancement algorithm called VCEA, which improves image quality in consideration of the requirements of human visual perception, is proposed. VCEA uses the concept of JND to devise a space adjustment function as an adjustable reference to adjust the spaces between two adjacent gray values, which are overstretched by HE, and hence solves the excessive contrast enhancement problem. It is worth noting that VCEA mitigates the feature loss problem, caused by HE or HE-based methods because many gray values are compressed to the same gray value. Further, VCEA aims at representing the detailed textures of an image through further enhancement. Hence, images processed by VCEA have superior visual quality to those obtained using HE and other HE-based methods, and are adequate for human visual perception.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank the reviewers for their comments that help improve the manuscript.</p></ack><notes><title>Author Contributions</title><p>C.-C.T. wrote the paper and devised the VCEA algorithm. B.-F.W. provided many comments to help improve the paper. M.-L.C. and Y.-C.W. performed HE-based methods and analyzed the experimental results. C.-C.C. is responsible for coming up with the initial concept of the algorithm and for further improvements to the algorithm.</p></notes><notes><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-15-16981"><label>1.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>R.</given-names></name><name><surname>Woods</surname><given-names>R.</given-names></name></person-group>
<source>Digital Image Processing</source>
<edition>3rd ed.</edition>
<publisher-name>Prentice Hall</publisher-name>
<publisher-loc>Upper Saddle River, NJ, USA</publisher-loc>
<year>2007</year>
<fpage>144</fpage>
<lpage>166</lpage>
</element-citation></ref><ref id="B2-sensors-15-16981"><label>2.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y.</given-names></name></person-group>
<article-title>Contrast enhancement using brightness preserving bi-histogram equalization</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>1997</year>
<volume>43</volume>
<fpage>1</fpage>
<lpage>8</lpage>
</element-citation></ref><ref id="B3-sensors-15-16981"><label>3.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Abdullah-Al-Wadud</surname><given-names>M.</given-names></name><name><surname>Kabir</surname><given-names>M.</given-names></name><name><surname>Dewan</surname><given-names>M.</given-names></name><name><surname>Chae</surname><given-names>O.</given-names></name></person-group>
<article-title>A dynamic histogram equalization for image contrast enhancement</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2007</year>
<volume>53</volume>
<fpage>593</fpage>
<lpage>600</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2007.381734</pub-id>
</element-citation></ref><ref id="B4-sensors-15-16981"><label>4.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Abdullah-Al-Wadud</surname><given-names>M.</given-names></name><name><surname>Kabir</surname><given-names>M.</given-names></name><name><surname>Chae</surname><given-names>O.</given-names></name></person-group>
<article-title>A spatially controlled histogram equalization for image enhancement</article-title>
<source>Proceedings of the 23rd International Symposium on Computer and Information Sciences</source>
<conf-loc>Istanbul, Turkey</conf-loc>
<conf-date>27&#x02013;29 October 2008</conf-date>
<fpage>1</fpage>
<lpage>6</lpage>
</element-citation></ref><ref id="B5-sensors-15-16981"><label>5.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Park</surname><given-names>G.</given-names></name><name><surname>Cho</surname><given-names>H.</given-names></name><name><surname>Choi</surname><given-names>M.</given-names></name></person-group>
<article-title>A contrast enhancement method using dynamic range separate histogram equalization</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2008</year>
<volume>54</volume>
<fpage>1981</fpage>
<lpage>1987</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2008.4711262</pub-id>
</element-citation></ref><ref id="B6-sensors-15-16981"><label>6.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Lin</surname><given-names>P.</given-names></name><name><surname>Lin</surname><given-names>C.</given-names></name><name><surname>Yen</surname><given-names>H.</given-names></name></person-group>
<article-title>Tri-histogram equalization based on first order statistics</article-title>
<source>Proceedings of 13th IEEE International Symposium on Consumer Electronics</source>
<conf-loc>Kyoto, Japan</conf-loc>
<conf-date>25&#x02013;28 May 2009</conf-date>
<fpage>387</fpage>
<lpage>391</lpage>
</element-citation></ref><ref id="B7-sensors-15-16981"><label>7.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ooi</surname><given-names>C.</given-names></name><name><surname>Kong</surname><given-names>N.</given-names></name><name><surname>Ibrahim</surname><given-names>H.</given-names></name></person-group>
<article-title>Bi-histogram equalization with a plateau limit for digital image enhancement</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2009</year>
<volume>55</volume>
<fpage>2072</fpage>
<lpage>2080</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2009.5373771</pub-id>
</element-citation></ref><ref id="B8-sensors-15-16981"><label>8.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wu</surname><given-names>P.</given-names></name><name><surname>Cheng</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name></person-group>
<article-title>A weighting mean-separated sub-histogram equalization for contrast enhancement</article-title>
<source>Proceedings of the 2010 International Conference on. Biomedical Engineering and Computer Sciences</source>
<conf-loc>Wuhan, China</conf-loc>
<conf-date>23&#x02013;25 April 2010</conf-date>
<fpage>1</fpage>
<lpage>4</lpage>
</element-citation></ref><ref id="B9-sensors-15-16981"><label>9.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wongsritong</surname><given-names>K.</given-names></name><name><surname>Kittayaruasiriwat</surname><given-names>K.</given-names></name><name><surname>Cheevasuvit</surname><given-names>F.</given-names></name><name><surname>Dejhan</surname><given-names>K.</given-names></name><name><surname>Somboonkaew</surname><given-names>A.</given-names></name></person-group>
<article-title>Contrast enhancement using multipeak histogram equalization with brightness preserving</article-title>
<source>Proceedings of the 1998 IEEE Asia-Pacific Conference Circuits and Systems</source>
<conf-loc>Chiangmai, Thailand</conf-loc>
<conf-date>24&#x02013;27 November 1998</conf-date>
<fpage>455</fpage>
<lpage>458</lpage>
</element-citation></ref><ref id="B10-sensors-15-16981"><label>10.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name></person-group>
<article-title>Image enhancement based on equal area dualistic subimage histogram equalization method</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>1999</year>
<volume>45</volume>
<fpage>68</fpage>
<lpage>75</lpage>
<pub-id pub-id-type="doi">10.1109/30.754419</pub-id>
</element-citation></ref><ref id="B11-sensors-15-16981"><label>11.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Ramli</surname><given-names>A.</given-names></name></person-group>
<article-title>Contrast enhancement using recursive mean-separate histogram equalization for scalable brightness preservation</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2003</year>
<volume>49</volume>
<fpage>1301</fpage>
<lpage>1309</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2003.1261233</pub-id>
</element-citation></ref><ref id="B12-sensors-15-16981"><label>12.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Ramli</surname><given-names>A.</given-names></name></person-group>
<article-title>Minimum mean brightness error bi-histogram equalization in contrast enhancement</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2003</year>
<volume>49</volume>
<fpage>1310</fpage>
<lpage>1319</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2003.1261234</pub-id>
</element-citation></ref><ref id="B13-sensors-15-16981"><label>13.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Ye</surname><given-names>Z.</given-names></name></person-group>
<article-title>Brightness preserving histogram equalization with maximum entropy: A variational perspective</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2005</year>
<volume>51</volume>
<fpage>1326</fpage>
<lpage>1334</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2005.1561863</pub-id>
</element-citation></ref><ref id="B14-sensors-15-16981"><label>14.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sim</surname><given-names>K.</given-names></name><name><surname>Tso</surname><given-names>C.</given-names></name><name><surname>Tan</surname><given-names>Y.</given-names></name></person-group>
<article-title>Recursive sub-image histogram equalization applied to gray scale images</article-title>
<source>Pattern Recogn. Lett.</source>
<year>2007</year>
<volume>28</volume>
<fpage>1209</fpage>
<lpage>1221</lpage>
<pub-id pub-id-type="doi">10.1016/j.patrec.2007.02.003</pub-id>
</element-citation></ref><ref id="B15-sensors-15-16981"><label>15.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>H.</given-names></name><name><surname>Kong</surname><given-names>N.</given-names></name></person-group>
<article-title>Brightness preserving dynamic histogram equalization for image contrast enhancement</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2007</year>
<volume>53</volume>
<fpage>1752</fpage>
<lpage>1758</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2007.4429280</pub-id>
</element-citation></ref><ref id="B16-sensors-15-16981"><label>16.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Peng</surname><given-names>J.</given-names></name><name><surname>Ye</surname><given-names>Z.</given-names></name></person-group>
<article-title>Flattest histogram specification with accurate brightness preservation</article-title>
<source>IET Image Process.</source>
<year>2008</year>
<volume>2</volume>
<fpage>249</fpage>
<lpage>262</lpage>
<pub-id pub-id-type="doi">10.1049/iet-ipr:20070198</pub-id>
</element-citation></ref><ref id="B17-sensors-15-16981"><label>17.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ooi</surname><given-names>C.</given-names></name><name><surname>Isa</surname><given-names>N.</given-names></name></person-group>
<article-title>Adaptive contrast enhancement methods with brightness preserving</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2010</year>
<volume>56</volume>
<fpage>2543</fpage>
<lpage>2551</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2010.5681139</pub-id>
</element-citation></ref><ref id="B18-sensors-15-16981"><label>18.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Thomas</surname><given-names>G.</given-names></name><name><surname>Flores-Tapia</surname><given-names>D.</given-names></name><name><surname>Pistorius</surname><given-names>S.</given-names></name></person-group>
<article-title>Histogram specification: a fast and flexible method to process digital images</article-title>
<source>IEEE Trans. Instrum. Meas.</source>
<year>2011</year>
<volume>60</volume>
<fpage>1565</fpage>
<lpage>1578</lpage>
<pub-id pub-id-type="doi">10.1109/TIM.2010.2089110</pub-id>
</element-citation></ref><ref id="B19-sensors-15-16981"><label>19.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tsai</surname><given-names>C.</given-names></name><name><surname>Yeh</surname><given-names>Z.</given-names></name></person-group>
<article-title>Contrast enhancement by automatic and parameter-free piecewise linear transformation for color images</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2008</year>
<volume>54</volume>
<fpage>213</fpage>
<lpage>219</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2008.4560077</pub-id>
</element-citation></ref><ref id="B20-sensors-15-16981"><label>20.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Buchsbaum</surname><given-names>G.</given-names></name></person-group>
<article-title>An analytical derivation of visual nonlinearity</article-title>
<source>IEEE Trans. Biomed. Eng.</source>
<year>1980</year>
<volume>BME-27</volume>
<fpage>237</fpage>
<lpage>242</lpage>
<pub-id pub-id-type="doi">10.1109/TBME.1980.326628</pub-id>
<pub-id pub-id-type="pmid">7380439</pub-id></element-citation></ref><ref id="B21-sensors-15-16981"><label>21.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Lie</surname><given-names>W.</given-names></name><name><surname>Chang</surname><given-names>L.</given-names></name></person-group>
<article-title>Data hiding in images with adaptive numbers of least significant bits based on the human visual system</article-title>
<source>Proceedings of the International Conference on Image Processing</source>
<conf-loc>Kobe, Japan</conf-loc>
<conf-date>24&#x02013;28 October 1999</conf-date>
<fpage>286</fpage>
<lpage>290</lpage>
</element-citation></ref><ref id="B22-sensors-15-16981"><label>22.</label><element-citation publication-type="webpage">
<article-title>Rhian Davis Photography</article-title>
<comment>Available online:<ext-link ext-link-type="uri" xlink:href="http://www.rhian.me/2013_06_01_archive.html">http://www.rhian.me/2013_06_01_archive.html</ext-link></comment>
<date-in-citation>(accessed on 3 March 2015)</date-in-citation>
</element-citation></ref><ref id="B23-sensors-15-16981"><label>23.</label><element-citation publication-type="webpage">
<article-title>Galleries for Under Exposure</article-title>
<comment>Available online:<ext-link ext-link-type="uri" xlink:href="http://galleryhip.com/under-exposure.html">http://galleryhip.com/under-exposure.html</ext-link></comment>
<date-in-citation>(accessed on 5 March 2015)</date-in-citation>
</element-citation></ref><ref id="B24-sensors-15-16981"><label>24.</label><element-citation publication-type="webpage">
<article-title>Backlighting Photography Portrait</article-title>
<comment>Available online:<ext-link ext-link-type="uri" xlink:href="http://imgkid.com/backlighting-photography-portrait.shtml">http://imgkid.com/backlighting-photography-portrait.shtml</ext-link></comment>
<date-in-citation>(accessed on 5 March 2015)</date-in-citation>
</element-citation></ref><ref id="B25-sensors-15-16981"><label>25.</label><element-citation publication-type="webpage">
<article-title>HDR on N900&#x02014;Mike Comella, Vazheh Mosussavi, &#x00026; adam zethraeus</article-title>
<comment>Available online:<ext-link ext-link-type="uri" xlink:href="http://cs.brown.edu/courses/csci1290/2011/results/proj5/mcomella/img/fu4/exposure_1.jpg">http://cs.brown.edu/courses/csci1290/2011/results/proj5/mcomella/img/fu4/exposure_1.jpg</ext-link></comment>
<date-in-citation>(accessed on 7 March 2015)</date-in-citation>
</element-citation></ref><ref id="B26-sensors-15-16981"><label>26.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ooi</surname><given-names>C.H.</given-names></name><name><surname>Isa</surname><given-names>N.A.M.</given-names></name></person-group>
<article-title>Quadrants dynamic histogram equalization for contrast enhancement</article-title>
<source>IEEE Trans. Consum. Electron.</source>
<year>2010</year>
<volume>56</volume>
<fpage>2552</fpage>
<lpage>2559</lpage>
<pub-id pub-id-type="doi">10.1109/TCE.2010.5681140</pub-id>
</element-citation></ref><ref id="B27-sensors-15-16981"><label>27.</label><element-citation publication-type="webpage">
<article-title>Index of /Matlab/images/MATLAB_DEMO_IMAGES</article-title>
<comment>Available online:<ext-link ext-link-type="uri" xlink:href="http://www.bogotobogo.com/Matlab/images/MATLAB_DEMO_IMAGES/office_6.jpg">http://www.bogotobogo.com/Matlab/images/MATLAB_DEMO_IMAGES/office_6.jpg</ext-link></comment>
<date-in-citation>(accessed on 7 March 2015)</date-in-citation>
</element-citation></ref></ref-list></back></article>